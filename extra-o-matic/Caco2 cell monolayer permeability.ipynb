{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from sklearn.tree import *\n",
    "from sklearn.metrics import r2_score, matthews_corrcoef\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3796, 997)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_atom=pd.read_csv('caco_AtomPair.tsv', sep='\\t', index_col=False)\n",
    "df_atom.shape\n",
    "#df_atom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3796, 53)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dragon=pd.read_csv('caco_Dragon.tsv', sep='\\t', index_col=False)\n",
    "df_dragon.shape\n",
    "#pd.DataFrame.to_csv(df_dragon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3796, 52)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quick=pd.read_csv('caco_QuickProp.tsv', sep='\\t', index_col=False)\n",
    "df_quick.shape\n",
    "#list(df_quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3796, 5402)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pipe_FP=pd.read_csv('caco_PipelinePilot_FP.tsv', sep='\\t', index_col=False)\n",
    "df_pipe_FP.shape\n",
    "#list(df_pipe_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Molecule', 'Class']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out=pd.read_csv('caco_Outcome.tsv', sep='\\t', index_col=False)\n",
    "list(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Molecule</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compound0001</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compound0002</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compound0003</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compound0004</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compound0005</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Molecule Class\n",
       "0  Compound0001     M\n",
       "1  Compound0002     L\n",
       "2  Compound0003     M\n",
       "3  Compound0004     M\n",
       "4  Compound0005     M"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_df=df_dragon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3796, 52)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_df.drop(x_df.columns[[1]], axis=1)\n",
    "#x_df\n",
    "x_df.set_index('Molecule', inplace=True)\n",
    "x_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxXAiSZI6xXAiSZI65f8HdoPDD67T+RcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e214539c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA().fit(x_df)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel(\"variance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca=PCA(n_components=5)\n",
    "x_pca=pca.fit_transform(x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Molecule</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compound0001</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compound0002</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compound0003</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compound0004</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compound0005</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Molecule Class\n",
       "0  Compound0001     M\n",
       "1  Compound0002     L\n",
       "2  Compound0003     M\n",
       "3  Compound0004     M\n",
       "4  Compound0005     M"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df=df_out\n",
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class\n",
       "0     M\n",
       "1     L\n",
       "2     M\n",
       "3     M\n",
       "4     M"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_final=y_df.drop('Molecule', axis=1)\n",
    "y_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_df, x_test_df, y_train_df, y_test_df=train_test_split(x_pca, y_final, test_size=0.2)\n",
    "#x_train_int=table.Columns.RemoveAt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3036, 5), (760, 5))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler=StandardScaler()\n",
    "clf_model=x_scaler.fit(x_train_df)\n",
    "x_train_norm=clf_model.transform(x_train_df)\n",
    "x_test_norm=clf_model.transform(x_test_df)\n",
    "x_train_norm.shape, x_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:129: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3036, 3), (760, 3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoder=LabelEncoder()\n",
    "y_encoded_tr=y_encoder.fit_transform(y_train_df)\n",
    "y_enc_tr=np_utils.to_categorical(y_encoded_tr)\n",
    "y_enc_tr.shape\n",
    "y_encoded_test=y_encoder.fit_transform(y_test_df)\n",
    "y_enc_test=np_utils.to_categorical(y_encoded_test)\n",
    "y_enc_tr.shape,y_enc_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3036, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_enc_tr.shape[0], y_enc_tr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3036, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler=StandardScaler()\n",
    "y_scaler.fit(y_enc_tr)\n",
    "y_train_norm=y_scaler.transform(y_enc_tr)\n",
    "y_test_norm=y_scaler.transform(y_enc_test)\n",
    "y_train_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEURAL NETWORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(5, input_dim=5,init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(4, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Dense(75, init='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(Dense(25, init='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.2))   \n",
    "    #model.add(Dense(12, init='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, init='normal', activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"normal\", activation=\"relu\", input_dim=5)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 39.5170009756\n"
     ]
    }
   ],
   "source": [
    "#fit and evaluate the model\n",
    "estimators=[]\n",
    "#estimators.append(('standardise',StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, nb_epoch=100, batch_size=200, verbose=0)))\n",
    "pipeline=Pipeline(estimators)\n",
    "kfold=KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results=cross_val_score(pipeline, x_train_norm, y_train_norm, cv=kfold)\n",
    "print('accuracy:', results.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3036, 5), (760, 5), (3036, 3), (760, 3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_norm.shape, x_test_norm.shape, y_train_norm.shape, y_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3036, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final, y_test_final=train_test_split(y_final, test_size=0.2)\n",
    "y_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.518421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf_linear=linear_model.LogisticRegression()\n",
    "model_linear=clf_linear.fit(x_train_norm, y_train_final)\n",
    "acc_logit=model_linear.score(x_test_norm, y_test_final)\n",
    "print('accuracy:',acc_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculation of the confusion matrix for Logistic\n",
    "clf_logit_predict=clf_linear.predict(x_test_norm)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test_final, clf_logit_predict)\n",
    "\n",
    "cmatrix_logit=confusion_matrix(y_test_final, clf_logit_predict)\n",
    "cmatrix_logit.diagonal()/cmatrix_logit.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.430263157895\n"
     ]
    }
   ],
   "source": [
    "tree_c=DecisionTreeClassifier(random_state=seed)#check this\n",
    "model_c=tree_c.fit(x_train_norm, y_train_final)\n",
    "acc_clf=model_c.score(x_test_norm, y_test_final)\n",
    "print('accuracy:', acc_clf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_graphviz(tree_c, out_file='tree_clf.dot', rounded=True)#visualizing the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of SGD: 0.402631578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf_SGD=linear_model.SGDClassifier(loss='epsilon_insensitive', penalty='none', alpha=0.0001)\n",
    "clf_SGD.fit(x_train_norm, y_train_final)\n",
    "y_predict_SGD=clf_SGD.predict(x_test_norm)\n",
    "acc_SGD=clf_SGD.score(x_test_norm, y_test_final)\n",
    "print('accuracy of SGD:', acc_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of svc: 0.510526315789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svc=SVC(kernel='rbf', C=1e3)\n",
    "clf_svc.fit(x_train_norm, y_train_final)\n",
    "y_predict_svc=clf_svc.predict(x_test_norm)\n",
    "acc_svc=clf_svc.score(x_test_norm, y_test_final)\n",
    "print(\"accuracy of svc:\", acc_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of randomforest: 0.468421052632\n",
      "order of classes ['H' 'L' 'M']\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=70)\n",
    "rf.fit(x_train_norm, y_train_final)\n",
    "rf_predict_norm=rf.predict(x_test_norm)\n",
    "acc_rf=rf.score(x_test_norm, y_test_final)\n",
    "print('accuracy of randomforest:', acc_rf)\n",
    "print('order of classes', rf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70,   2, 217],\n",
       "       [ 16,   0,  61],\n",
       "       [104,   4, 286]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the confusion matrix\n",
    "confusion_matrix(y_test_final, rf_predict_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24221453,  0.        ,  0.72588832])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmatrix_rf=confusion_matrix(y_test_final, rf_predict_norm)\n",
    "cmatrix_rf.diagonal()/cmatrix_rf.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary class modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(df_dragon), list(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dragon_mrg=pd.merge(df_quick, df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_dragon_mrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HM model\n",
    "a=df_dragon_mrg[df_out['Class']=='H']\n",
    "b=df_dragon_mrg[df_out['Class']=='M']\n",
    "frames_hm=[a, b]\n",
    "HM=pd.concat(frames_hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(HM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ML model\n",
    "c=df_dragon_mrg[df_out['Class']=='M']\n",
    "d=df_dragon_mrg[df_out['Class']=='L']\n",
    "frames_ml=[c,d]\n",
    "ML=pd.concat(frames_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2406, 53)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 0, 0, ..., 0, 9, 0],\n",
       "       [0, 2, 0, ..., 7, 38, 1],\n",
       "       [0, 2, 0, ..., 8, 39, 1],\n",
       "       ..., \n",
       "       [19, 1, 1, ..., 4, 72, 2],\n",
       "       [0, 0, 0, ..., 4, 19, 0],\n",
       "       [0, 1, 0, ..., 4, 24, 0]], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ML=ML.values[:, 1:51]\n",
    "x_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'M', 'M', ..., 'L', 'L', 'L'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ML=ML.values[:, 52]\n",
    "y_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ML_x_train, ML_x_test, ML_y_train, ML_y_test = train_test_split(x_ML, y_ML, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HL model\n",
    "e=df_dragon_mrg[df_out['Class']=='H']\n",
    "f=df_dragon_mrg[df_out['Class']=='L']\n",
    "frames_hl=[e,f]\n",
    "HL=pd.concat(frames_hl)\n",
    "#HL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1767, 53)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_HL=HL.values[:,1:51]\n",
    "y_HL=HL.values[:, 52]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HL_x_train, HL_x_test, HL_y_train, HL_y_test=train_test_split(x_HL, y_HL, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.   0.   0. ...,   0.  11.   0.]\n",
      " [  0.   0.   0. ...,   0.  11.   0.]\n",
      " [  1.   1.   0. ...,   9.  41.   0.]\n",
      " ..., \n",
      " [  0.   0.   0. ...,   7.  23.   0.]\n",
      " [  0.   1.   0. ...,   5.  28.   0.]\n",
      " [  0.   1.   0. ...,   2.  21.   0.]]\n"
     ]
    }
   ],
   "source": [
    "x_HM=HM.values[:, 1:51].astype('float32')\n",
    "print(x_HM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['H', 'H', 'H', ..., 'M', 'M', 'M'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_HM=HM.values[:, 52]\n",
    "y_HM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HM_x_train, HM_x_test, HM_y_train, HM_y_test=train_test_split(x_HM,y_HM, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM_y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Decision tree model for HM class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H' 'M']\n",
      "[[ 0.0848329   0.9151671 ]\n",
      " [ 0.0848329   0.9151671 ]\n",
      " [ 0.88282504  0.11717496]\n",
      " ..., \n",
      " [ 0.0848329   0.9151671 ]\n",
      " [ 0.34624697  0.65375303]\n",
      " [ 0.88282504  0.11717496]]\n",
      "accuracy for HM model: 0.880116959064\n",
      "mcc for the CART for HM: 0.743578882513\n"
     ]
    }
   ],
   "source": [
    "tree_hm=DecisionTreeClassifier(min_weight_fraction_leaf=0.13, max_leaf_nodes=12)\n",
    "tree_hm.fit(HM_x_train, HM_y_train)\n",
    "acc_hm=tree_hm.score(HM_x_test, HM_y_test)\n",
    "tree_hm_predict=tree_hm.predict(HM_x_test)\n",
    "\n",
    "#matthews correlation coefficient\n",
    "matt_tree_hm=matthews_corrcoef(HM_y_test, tree_hm_predict)\n",
    "\n",
    "print(tree_hm.classes_)\n",
    "print(tree_hm.predict_proba(HM_x_test))\n",
    "print('accuracy for HM model:',acc_hm)\n",
    "print('mcc for the CART for HM:', matt_tree_hm)\n",
    "#mcc of the literature value for only the CART model is 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37390871055829272, 0.62609128944170733)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=tree_hm.predict_proba(HM_x_test)\n",
    "z[:,0].mean(), z[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[206,  55],\n",
       "       [ 27, 396]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing the confusion matrix for tree in HM\n",
    "confusion_matrix(HM_y_test, tree_hm_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78927203,  0.93617021])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmatrix_tree_hm=confusion_matrix(HM_y_test, tree_hm_predict)\n",
    "cmatrix_tree_hm.diagonal()/cmatrix_tree_hm.sum(axis=1)\n",
    "#LITERATURE VALUE OF THE CONSENSUS MODEL\n",
    "#H:0.78\n",
    "#M:0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chiad tree model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from CHAID import Tree\n",
    "\n",
    "#CHAID input parameters\n",
    "indep_variable_cols=['Molecule','QikProp_.stars','QikProp_.amine','QikProp_.amidine','QikProp_.acid',\n",
    " 'QikProp_.amide','QikProp_.rotor','QikProp_.rtvFG','QikProp_CNS','QikProp_mol_MW','QikProp_dipole','QikProp_SASA','QikProp_FOSA',\n",
    " 'QikProp_FISA','QikProp_PISA','QikProp_WPSA', 'QikProp_volume','QikProp_donorHB','QikProp_accptHB','QikProp_dip.2.V','QikProp_ACxDN..5.SA',\n",
    " 'QikProp_glob','QikProp_QPpolrz','QikProp_QPlogPC16','QikProp_QPlogPoct','QikProp_QPlogPw','QikProp_QPlogPo.w',\n",
    "'QikProp_QPlogS','QikProp_CIQPlogS','QikProp_QPlogHERG','QikProp_QPPCaco','QikProp_QPlogBB','QikProp_QPPMDCK','QikProp_QPlogKp',\n",
    " 'QikProp_IP.eV.','QikProp_EA.eV.','QikProp_.metab','QikProp_QPlogKhsa','QikProp_HumanOralAbsorption','QikProp_PercentHumanOralAbsorption','QikProp_SAfluorine',\n",
    " 'QikProp_SAamideO','QikProp_PSA', 'QikProp_.NandO','QikProp_RuleOfFive','QikProp_.ringatoms','QikProp_.in34','QikProp_.in56','QikProp_.noncon',\n",
    " 'QikProp_.nonHatm','QikProp_RuleOfThree','QikProp_ACxDN..5.SAxSASA.MW']\n",
    "dep_variable=[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zip(indep_variable_cols,['nominal']*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tree from pandas\n",
    "tree_chaid= Tree.from_pandas_df(HM, dict(zip(indep_variable_cols, ['nominal']*3)), dep_variable, \n",
    "                          max_depth=4, min_parent_node_size=80, min_child_node_size=35)\n",
    "#tree.to_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest classifier for HM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H' 'M']\n",
      "[[ 0.          1.        ]\n",
      " [ 0.10526316  0.89473684]\n",
      " [ 1.          0.        ]\n",
      " ..., \n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]]\n",
      "accuracy of Random forest for HM model is: 0.941520467836\n",
      "mcc: 0.875831383053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=95, n_jobs=1, oob_score=False, random_state=1,\n",
       "            verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_hm=RandomForestClassifier(n_estimators=95, random_state=1)\n",
    "RF_hm.fit(HM_x_train, HM_y_train)\n",
    "RF_hm_predict= RF_hm.predict(HM_x_test)\n",
    "acc_RF_HM=RF_hm.score(HM_x_test, HM_y_test)\n",
    "\n",
    "print(RF_hm.classes_)\n",
    "print(RF_hm.predict_proba(HM_x_test))\n",
    "print('accuracy of Random forest for HM model is:', acc_RF_HM)\n",
    "\n",
    "\n",
    "#matthews correlation coefficient\n",
    "matt_corr_HM=matthews_corrcoef(HM_y_test, RF_hm_predict)\n",
    "print('mcc:',matt_corr_HM)\n",
    "\n",
    "RF_hm.get_params\n",
    "\n",
    "#MCC OF THE LITERATURE VALUE FOR THE HM RF IS 0.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[232,  29],\n",
       "       [ 11, 412]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(HM_y_test, RF_hm_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.88888889,  0.97399527])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmatrix_rf_hm=confusion_matrix(HM_y_test, RF_hm_predict)\n",
    "cmatrix_rf_hm.diagonal()/cmatrix_rf_hm.sum(axis=1)\n",
    "#LITERATURE VALUE FOR consensus model\n",
    "#H:0.78\n",
    "#M:0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['M', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "        'H', 'H', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'H',\n",
       "        'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'H', 'M', 'M', 'H', 'M',\n",
       "        'H', 'M', 'H', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'M',\n",
       "        'H', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'M',\n",
       "        'H', 'M', 'H', 'H', 'H', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H',\n",
       "        'H', 'M', 'M', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "        'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'H', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'M', 'H', 'H', 'M', 'H',\n",
       "        'H', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H',\n",
       "        'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'H', 'H',\n",
       "        'H', 'H', 'H', 'M', 'M', 'H', 'H', 'M', 'H', 'M', 'H', 'M', 'M',\n",
       "        'M', 'M', 'H', 'H', 'M', 'H', 'H', 'M', 'H', 'M', 'M', 'H', 'H',\n",
       "        'H', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'H', 'M', 'H', 'H', 'M',\n",
       "        'H', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'H',\n",
       "        'M', 'M', 'H', 'M', 'M', 'H', 'M', 'H', 'H', 'M', 'M', 'M', 'M',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'M', 'H',\n",
       "        'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'M', 'H', 'M', 'M',\n",
       "        'M', 'H', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'H', 'M', 'M',\n",
       "        'M', 'M', 'M', 'H', 'H', 'M', 'H', 'M', 'H', 'H', 'H', 'H', 'M',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M',\n",
       "        'M', 'H', 'H', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'H', 'H',\n",
       "        'H', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'H',\n",
       "        'H', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'M', 'H',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'H', 'M',\n",
       "        'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'M', 'M', 'H',\n",
       "        'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'M',\n",
       "        'H', 'H', 'H', 'M', 'H', 'M', 'H', 'H', 'M', 'H', 'H', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'H', 'H', 'M', 'H',\n",
       "        'M', 'M', 'H', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M',\n",
       "        'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'M', 'M',\n",
       "        'H', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'H', 'M', 'H',\n",
       "        'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'H',\n",
       "        'M', 'M', 'M', 'H', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M',\n",
       "        'M', 'H', 'H', 'H', 'M', 'H', 'H', 'H', 'M', 'M', 'M', 'H', 'M',\n",
       "        'H', 'H', 'H', 'H', 'M', 'M', 'M', 'M', 'H', 'M', 'M', 'H', 'M',\n",
       "        'H', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'H', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'H', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H',\n",
       "        'H', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'H',\n",
       "        'H', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'M', 'H', 'M', 'H',\n",
       "        'M', 'H', 'M', 'H', 'H', 'M', 'M', 'H', 'H', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H',\n",
       "        'H', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'M',\n",
       "        'M', 'M', 'H', 'M', 'M', 'M', 'M', 'H'], dtype=object),\n",
       " array(['M', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "        'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'H',\n",
       "        'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'H', 'M', 'M', 'H', 'M',\n",
       "        'H', 'M', 'H', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'M',\n",
       "        'H', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'M', 'H', 'H', 'M', 'M',\n",
       "        'H', 'M', 'H', 'H', 'H', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H',\n",
       "        'H', 'M', 'M', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "        'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'M', 'M', 'H', 'M', 'H',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'H',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H',\n",
       "        'M', 'M', 'M', 'H', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'H', 'H',\n",
       "        'H', 'H', 'H', 'H', 'M', 'H', 'H', 'M', 'H', 'M', 'H', 'M', 'M',\n",
       "        'M', 'M', 'H', 'H', 'M', 'H', 'H', 'H', 'H', 'M', 'M', 'H', 'H',\n",
       "        'H', 'M', 'M', 'H', 'H', 'H', 'M', 'M', 'H', 'H', 'H', 'H', 'M',\n",
       "        'H', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'M', 'H', 'M', 'H',\n",
       "        'M', 'M', 'H', 'M', 'M', 'H', 'M', 'H', 'H', 'M', 'M', 'M', 'M',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'M', 'H',\n",
       "        'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'M', 'H', 'M', 'M',\n",
       "        'M', 'H', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'H', 'M',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'H', 'M', 'M',\n",
       "        'H', 'M', 'M', 'H', 'H', 'H', 'H', 'M', 'H', 'H', 'H', 'H', 'M',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M',\n",
       "        'M', 'H', 'H', 'M', 'H', 'M', 'M', 'M', 'H', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'H', 'H',\n",
       "        'H', 'H', 'M', 'M', 'H', 'H', 'M', 'M', 'H', 'M', 'H', 'M', 'H',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'M', 'H',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'H', 'H',\n",
       "        'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'M', 'M', 'H',\n",
       "        'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'M',\n",
       "        'H', 'H', 'H', 'M', 'H', 'M', 'H', 'H', 'M', 'H', 'H', 'H', 'H',\n",
       "        'M', 'H', 'M', 'M', 'H', 'H', 'H', 'H', 'M', 'H', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M',\n",
       "        'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'H', 'M', 'M', 'M',\n",
       "        'H', 'M', 'H', 'M', 'M', 'M', 'H', 'H', 'M', 'M', 'M', 'M', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'H', 'M', 'H',\n",
       "        'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'H',\n",
       "        'M', 'M', 'M', 'H', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'M', 'H',\n",
       "        'M', 'H', 'H', 'H', 'M', 'H', 'H', 'H', 'M', 'M', 'M', 'H', 'M',\n",
       "        'H', 'H', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M',\n",
       "        'M', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'M', 'H', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'H', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H',\n",
       "        'H', 'M', 'M', 'H', 'M', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'H',\n",
       "        'H', 'M', 'M', 'M', 'H', 'H', 'H', 'M', 'H', 'H', 'M', 'H', 'M',\n",
       "        'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'M', 'M', 'H', 'M', 'H',\n",
       "        'M', 'H', 'M', 'H', 'H', 'M', 'M', 'H', 'H', 'M', 'M', 'M', 'M',\n",
       "        'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'H', 'H', 'H',\n",
       "        'H', 'H', 'M', 'M', 'H', 'M', 'M', 'M', 'H', 'M', 'H', 'H', 'M',\n",
       "        'M', 'M', 'H', 'M', 'M', 'M', 'M', 'H'], dtype=object))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_hm_predict, HM_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-071514e267cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mHM_y_test_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHM_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mRF_hm_predict_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRF_hm_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "y=label_binarize(y_HM, classes=['H', 'M'])\n",
    "n_classes=y.shape[1]\n",
    "#roc\n",
    "fpr=dict()\n",
    "tpr=dict()\n",
    "roc_auc=dict()\n",
    "HM_y_test_col=float(HM_y_test)\n",
    "RF_hm_predict_col=float(RF_hm_predict[:,None])\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _= roc_curve(HM_y_test_col[:,i], RF_hm_predict_col[:,i], pos_label='H')\n",
    "    roc_auc[:, i]= auc(fpr[i], tpr[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NN FOR CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((684, 2), (2735, 2))"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding\n",
    "y_encoder=LabelEncoder()\n",
    "y_encoded_tr_HM=y_encoder.fit_transform(HM_y_train)\n",
    "y_en_tr_HM=np_utils.to_categorical(y_encoded_tr_HM)\n",
    "y_en_tr_HM.shape\n",
    "y_encoded_test_HM=y_encoder.fit_transform(HM_y_test)\n",
    "y_en_test_HM=np_utils.to_categorical(y_encoded_test_HM)\n",
    "y_en_test_HM.shape, y_en_tr_HM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2735, 2), (684, 2))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler=StandardScaler()\n",
    "y_scaler.fit(y_en_tr_HM)\n",
    "HM_y_train_norm=y_scaler.transform(y_en_tr_HM)\n",
    "HM_y_test_norm=y_scaler.transform(y_en_test_HM)\n",
    "HM_y_train_norm.shape, HM_y_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2735, 50), (684, 50))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM_x_train.shape, HM_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2735, 50)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler=StandardScaler()\n",
    "x_scaler.fit(HM_x_train)\n",
    "HM_x_train_norm=x_scaler.transform(HM_x_train)\n",
    "HM_x_test_norm=x_scaler.transform(HM_x_test)\n",
    "HM_x_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(51, input_dim=51, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(26, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(13, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(6, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(51, input_dim=51, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(26, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(13, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, kernel_initializer=\"normal\", activation=\"relu\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\")`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_165_input to have shape (None, 51) but got array with shape (2461, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-304-77343321d312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHM_x_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHM_y_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid shape for y: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1379\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    142\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_165_input to have shape (None, 51) but got array with shape (2461, 50)"
     ]
    }
   ],
   "source": [
    "#fit and evaluate the model\n",
    "estimators=[]\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, nb_epoch=100, batch_size=20, verbose=0)))\n",
    "pipeline=Pipeline(estimators)\n",
    "kfold=KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results=cross_val_score(pipeline, HM_x_train_norm, HM_y_train_norm, cv=kfold)\n",
    "print('accuracy:', results.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision tree classifier for ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of classes: ['L' 'M']\n",
      "<bound method BaseEstimator.get_params of DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=10, min_impurity_split=1e-07,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.05, presort=False,\n",
      "            random_state=None, splitter='best')>\n",
      "accuracy for the CT for ML model: 0.906639004149\n",
      "probability predictions for ML model: [[ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09278351  0.90721649]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04123711  0.95876289]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15714286  0.84285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.28971963  0.71028037]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]\n",
      " [ 0.          1.        ]\n",
      " [ 0.94957983  0.05042017]\n",
      " [ 0.          1.        ]\n",
      " [ 0.048       0.952     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60732984  0.39267016]]\n",
      "matthews correlation coeff: 0.687209019189\n"
     ]
    }
   ],
   "source": [
    "tree_ml=DecisionTreeClassifier(min_weight_fraction_leaf=0.05, max_leaf_nodes=10)\n",
    "tree_ml.fit(ML_x_train, ML_y_train)\n",
    "acc_tree_ml=tree_ml.score(ML_x_test, ML_y_test)\n",
    "prob_tree_ml=tree_ml.predict_proba(ML_x_test)\n",
    "predict_tree_ml=tree_ml.predict(ML_x_test)\n",
    "\n",
    "print('order of classes:',tree_ml.classes_)\n",
    "print(tree_ml.get_params)\n",
    "print('accuracy for the CT for ML model:', acc_tree_ml)\n",
    "print('probability predictions for ML model:', prob_tree_ml)\n",
    "\n",
    "#matthews correlation coefficient\n",
    "mat_corr_ML=matthews_corrcoef(ML_y_test, predict_tree_ml)\n",
    "print('matthews correlation coeff:',mat_corr_ML)\n",
    "\n",
    "#len(acc_tree_ml),len(ML_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 63,  13],\n",
       "       [ 32, 374]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing the confusion matrix\n",
    "confusion_matrix(ML_y_test, predict_tree_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82894737,  0.92118227])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing the accuracies of the predictions of the individual classes\n",
    "cmatrix_ML_tree=confusion_matrix(ML_y_test, predict_tree_ml)\n",
    "cmatrix_ML_tree.diagonal()/cmatrix_ML_tree.sum(axis=1)\n",
    "#literature values of L and M class respectively, ONLY FOR CART MODELS\n",
    "#L:0.83\n",
    "#M:0.70\n",
    "#literature values of L and M on the consensus system\n",
    "#L:0.79\n",
    "#M:0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier for ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of classes: ['L' 'M']\n",
      "accuracy of model: 0.929460580913\n",
      "probabilities of the respective classes: [[ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.2173913   0.7826087 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.65217391  0.34782609]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.26086957  0.73913043]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.39130435  0.60869565]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.34782609  0.65217391]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.39130435  0.60869565]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.17391304  0.82608696]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52173913  0.47826087]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.26086957  0.73913043]\n",
      " [ 0.          1.        ]\n",
      " [ 0.17391304  0.82608696]\n",
      " [ 0.69565217  0.30434783]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.82608696  0.17391304]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52173913  0.47826087]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.17391304  0.82608696]\n",
      " [ 0.          1.        ]\n",
      " [ 0.30434783  0.69565217]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52173913  0.47826087]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.2173913   0.7826087 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52173913  0.47826087]\n",
      " [ 0.          1.        ]\n",
      " [ 0.56521739  0.43478261]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.17391304  0.82608696]\n",
      " [ 0.          1.        ]\n",
      " [ 0.2173913   0.7826087 ]\n",
      " [ 0.56521739  0.43478261]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.2173913   0.7826087 ]\n",
      " [ 0.08695652  0.91304348]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.69565217  0.30434783]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.2173913   0.7826087 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.73913043  0.26086957]\n",
      " [ 0.2173913   0.7826087 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.26086957  0.73913043]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52173913  0.47826087]\n",
      " [ 0.          1.        ]\n",
      " [ 0.91304348  0.08695652]\n",
      " [ 0.95652174  0.04347826]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.08695652  0.91304348]\n",
      " [ 0.          1.        ]\n",
      " [ 0.30434783  0.69565217]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.91304348  0.08695652]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.56521739  0.43478261]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.91304348  0.08695652]\n",
      " [ 0.          1.        ]\n",
      " [ 0.26086957  0.73913043]\n",
      " [ 0.65217391  0.34782609]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.39130435  0.60869565]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.34782609  0.65217391]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.47826087  0.52173913]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.08695652  0.91304348]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52173913  0.47826087]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.47826087  0.52173913]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52173913  0.47826087]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.08695652  0.91304348]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.08695652  0.91304348]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.30434783  0.69565217]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.34782609  0.65217391]\n",
      " [ 0.          1.        ]\n",
      " [ 0.26086957  0.73913043]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52173913  0.47826087]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.82608696  0.17391304]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.30434783  0.69565217]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.34782609  0.65217391]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.43478261  0.56521739]\n",
      " [ 0.          1.        ]\n",
      " [ 0.47826087  0.52173913]\n",
      " [ 0.          1.        ]\n",
      " [ 0.39130435  0.60869565]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.2173913   0.7826087 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.39130435  0.60869565]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.69565217  0.30434783]\n",
      " [ 0.17391304  0.82608696]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60869565  0.39130435]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.2173913   0.7826087 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.26086957  0.73913043]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.34782609  0.65217391]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.91304348  0.08695652]\n",
      " [ 0.39130435  0.60869565]\n",
      " [ 0.34782609  0.65217391]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.17391304  0.82608696]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.30434783  0.69565217]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.91304348  0.08695652]\n",
      " [ 0.47826087  0.52173913]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.30434783  0.69565217]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.04347826  0.95652174]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.08695652  0.91304348]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.30434783  0.69565217]\n",
      " [ 0.65217391  0.34782609]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.17391304  0.82608696]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.34782609  0.65217391]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.56521739  0.43478261]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.60869565  0.39130435]\n",
      " [ 0.30434783  0.69565217]\n",
      " [ 1.          0.        ]\n",
      " [ 0.34782609  0.65217391]\n",
      " [ 0.13043478  0.86956522]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]]\n",
      "matthews correlation coefficient: 0.731664299599\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=23, n_jobs=1, oob_score=False, random_state=2,\n",
      "            verbose=0, warm_start=False)>\n"
     ]
    }
   ],
   "source": [
    "RF_ml=RandomForestClassifier(n_estimators=23, random_state=2)\n",
    "RF_ml.fit(ML_x_train, ML_y_train)\n",
    "accu_RF_ml=RF_ml.score(ML_x_test, ML_y_test)\n",
    "prob_RF_ml=RF_ml.predict_proba(ML_x_test)\n",
    "predict_RF_ml=RF_ml.predict(ML_x_test)\n",
    "#matthews correlation coefficients\n",
    "matt_coeff_ml=matthews_corrcoef(ML_y_test, predict_RF_ml)\n",
    "\n",
    "print('order of classes:',RF_ml.classes_)\n",
    "print('accuracy of model:', accu_RF_ml)\n",
    "print('probabilities of the respective classes:', prob_RF_ml)\n",
    "print('matthews correlation coefficient:', matt_coeff_ml )\n",
    "print(RF_ml.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 58,  18],\n",
       "       [ 16, 390]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing the confusion matrix for random forest\n",
    "confusion_matrix(ML_y_test, predict_RF_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76315789,  0.96059113])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmatrix_rf_ml=confusion_matrix(ML_y_test, predict_RF_ml)\n",
    "cmatrix_rf_ml.diagonal()/cmatrix_rf_ml.sum(axis=1)\n",
    "\n",
    "#literature values of the consensus system for L and M classes respectively\n",
    "#L:0.79\n",
    "#M:0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree for the LH model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of classes: ['H' 'L']\n",
      "accuracy of the model 0.923728813559\n",
      "probabilities of the classes: [[ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.          1.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.          1.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.          1.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.01408451  0.98591549]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.86956522  0.13043478]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 1.          0.        ]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 0.62025316  0.37974684]\n",
      " [ 0.09473684  0.90526316]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.95833333  0.04166667]]\n",
      "prediction of classes: ['H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'L' 'H' 'H' 'H' 'H' 'L' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H'\n",
      " 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'H'\n",
      " 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'L' 'H'\n",
      " 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'H' 'H' 'H' 'L' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'L' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'L' 'H' 'H' 'L' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'L' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'L'\n",
      " 'L' 'H' 'H' 'H' 'H' 'L' 'H' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'L' 'L'\n",
      " 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L'\n",
      " 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'L' 'L' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'L' 'H' 'H' 'H' 'H' 'L' 'L' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'L' 'H' 'H' 'H'\n",
      " 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'L' 'L' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'L' 'H' 'L' 'H' 'L' 'H' 'H' 'H' 'H' 'H']\n",
      "matthews correlation coeff: 0.779530302569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=10, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.05, presort=False,\n",
       "            random_state=None, splitter='best')>"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_LH=DecisionTreeClassifier(min_weight_fraction_leaf=0.05, max_leaf_nodes=10)\n",
    "tree_LH.fit(HL_x_train, HL_y_train)\n",
    "acc_tree_LH=tree_LH.score(HL_x_test, HL_y_test)\n",
    "predict_tree_LH=tree_LH.predict(HL_x_test)\n",
    "prob_tree_LH=tree_LH.predict_proba(HL_x_test)\n",
    "#matthews correlation coeff\n",
    "matt_corrcoef= matthews_corrcoef(HL_y_test, predict_tree_LH)\n",
    "print('order of classes:', tree_LH.classes_)\n",
    "print(\"accuracy of the model\", acc_tree_LH)\n",
    "print('probabilities of the classes:', prob_tree_LH)\n",
    "print('prediction of classes:', predict_tree_LH)\n",
    "print('matthews correlation coeff:', matt_corrcoef)\n",
    "\n",
    "tree_LH.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[265,   6],\n",
       "       [ 21,  62]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing the confusion matrix\n",
    "confusion_matrix(HL_y_test, predict_tree_LH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97785978,  0.74698795])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmatrix_HL_tree=confusion_matrix(HL_y_test, predict_tree_LH)\n",
    "cmatrix_HL_tree.diagonal()/cmatrix_HL_tree.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest for LH model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of classes: ['H' 'L']\n",
      "accuracy of model: 0.954802259887\n",
      "probabilities of the respective classes: [[ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.87234043  0.12765957]\n",
      " [ 1.          0.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.5106383   0.4893617 ]\n",
      " [ 0.95744681  0.04255319]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 0.63829787  0.36170213]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.63829787  0.36170213]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.85106383  0.14893617]\n",
      " [ 1.          0.        ]\n",
      " [ 0.68085106  0.31914894]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.44680851  0.55319149]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.93617021  0.06382979]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.87234043  0.12765957]\n",
      " [ 0.          1.        ]\n",
      " [ 0.06382979  0.93617021]\n",
      " [ 0.44680851  0.55319149]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.08510638  0.91489362]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.0212766   0.9787234 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.91489362  0.08510638]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.40425532  0.59574468]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.80851064  0.19148936]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.04255319  0.95744681]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.63829787  0.36170213]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.38297872  0.61702128]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95744681  0.04255319]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.76595745  0.23404255]\n",
      " [ 0.65957447  0.34042553]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.31914894  0.68085106]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.72340426  0.27659574]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.10638298  0.89361702]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.74468085  0.25531915]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.53191489  0.46808511]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.85106383  0.14893617]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.28510638  0.71489362]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.72340426  0.27659574]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.4893617   0.5106383 ]\n",
      " [ 0.68085106  0.31914894]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.10638298  0.89361702]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.59574468  0.40425532]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.29787234  0.70212766]\n",
      " [ 0.          1.        ]\n",
      " [ 0.57446809  0.42553191]\n",
      " [ 0.10638298  0.89361702]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.46808511  0.53191489]\n",
      " [ 0.          1.        ]\n",
      " [ 0.42553191  0.57446809]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.44680851  0.55319149]\n",
      " [ 0.19148936  0.80851064]\n",
      " [ 0.          1.        ]\n",
      " [ 0.70212766  0.29787234]\n",
      " [ 0.57446809  0.42553191]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.74468085  0.25531915]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.63829787  0.36170213]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.0212766   0.9787234 ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.91489362  0.08510638]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.93617021  0.06382979]\n",
      " [ 0.93617021  0.06382979]\n",
      " [ 1.          0.        ]\n",
      " [ 0.31914894  0.68085106]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.87234043  0.12765957]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.89361702  0.10638298]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.89361702  0.10638298]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.72340426  0.27659574]\n",
      " [ 0.          1.        ]\n",
      " [ 0.0212766   0.9787234 ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.57446809  0.42553191]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.0212766   0.9787234 ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]]\n",
      "predictions of the model: ['H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'L' 'H' 'H' 'H' 'H' 'L' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'L'\n",
      " 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'L'\n",
      " 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'L' 'H'\n",
      " 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'H' 'H' 'H' 'L' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'L' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'L' 'H' 'H' 'L' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'L' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'L'\n",
      " 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'H' 'H' 'L' 'H' 'H'\n",
      " 'H' 'H' 'L' 'H' 'H' 'L' 'L' 'L' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'L'\n",
      " 'L' 'H' 'H' 'H' 'H' 'L' 'H' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'L' 'L'\n",
      " 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L'\n",
      " 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'L' 'L' 'H' 'L' 'H'\n",
      " 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'L' 'L'\n",
      " 'L' 'H' 'H' 'L' 'H' 'H' 'H' 'L' 'L' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'L' 'H' 'H' 'H' 'H' 'L' 'L' 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'L' 'H' 'H' 'H'\n",
      " 'L' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'L' 'H' 'H' 'H' 'H' 'H' 'L' 'L' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'L' 'H' 'L' 'H' 'L' 'H' 'H' 'H' 'H' 'H']\n",
      "matthews correlation coefficient: 0.873142924921\n"
     ]
    }
   ],
   "source": [
    "RF_LH=RandomForestClassifier(n_estimators=47, random_state=3)\n",
    "RF_LH.fit(HL_x_train, HL_y_train)\n",
    "acc_RF_HL=RF_LH.score(HL_x_test, HL_y_test)\n",
    "predict_RF_HL=RF_LH.predict(HL_x_test)\n",
    "prob_RF_HL=RF_LH.predict_proba(HL_x_test)\n",
    "#matthews correlation coefficient\n",
    "matt_corrcoeff_lh=matthews_corrcoef(HL_y_test, predict_RF_HL)\n",
    "\n",
    "print('order of classes:', RF_LH.classes_)\n",
    "print('accuracy of model:', acc_RF_HL)\n",
    "print('probabilities of the respective classes:', prob_RF_HL)\n",
    "print('predictions of the model:', predict_RF_HL )\n",
    "print('matthews correlation coefficient:', matt_corrcoeff_lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[264,   7],\n",
       "       [  9,  74]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing the confusion matrix for HL rf\n",
    "confusion_matrix(HL_y_test, predict_RF_HL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97416974,  0.89156627])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmatrix_rf_HL=confusion_matrix(HL_y_test, predict_RF_HL)\n",
    "cmatrix_rf_HL.diagonal()/cmatrix_rf_HL.sum(axis=1)\n",
    "\n",
    "#LITERATURE VALUE FOR THE LH MODEL FOR ONLY RF\n",
    "#H:0.95\n",
    "#L:0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**BUILDING A CLASSIFICATION MODEL IN TENSORFLOW FOR LH model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1413, 53), (354, 53))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set, test_set=train_test_split(HL, test_size=0.2)\n",
    "training_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COLUMNS=['Molecule','QikProp_.stars','QikProp_.amine','QikProp_.amidine','QikProp_.acid',\n",
    " 'QikProp_.amide','QikProp_.rotor','QikProp_.rtvFG','QikProp_CNS','QikProp_mol_MW','QikProp_dipole','QikProp_SASA','QikProp_FOSA',\n",
    " 'QikProp_FISA','QikProp_PISA','QikProp_WPSA', 'QikProp_volume','QikProp_donorHB','QikProp_accptHB','QikProp_dip.2.V','QikProp_ACxDN..5.SA',\n",
    " 'QikProp_glob','QikProp_QPpolrz','QikProp_QPlogPC16','QikProp_QPlogPoct','QikProp_QPlogPw','QikProp_QPlogPo.w',\n",
    "'QikProp_QPlogS','QikProp_CIQPlogS','QikProp_QPlogHERG','QikProp_QPPCaco','QikProp_QPlogBB','QikProp_QPPMDCK','QikProp_QPlogKp',\n",
    " 'QikProp_IP.eV.','QikProp_EA.eV.','QikProp_.metab','QikProp_QPlogKhsa','QikProp_HumanOralAbsorption','QikProp_PercentHumanOralAbsorption','QikProp_SAfluorine',\n",
    " 'QikProp_SAamideO','QikProp_PSA', 'QikProp_.NandO','QikProp_RuleOfFive','QikProp_.ringatoms','QikProp_.in34','QikProp_.in56','QikProp_.noncon',\n",
    " 'QikProp_.nonHatm','QikProp_RuleOfThree','QikProp_ACxDN..5.SAxSASA.MW','Class']\n",
    "FEATURES=['Molecule','QikProp_.stars','QikProp_.amine','QikProp_.amidine','QikProp_.acid',\n",
    " 'QikProp_.amide','QikProp_.rotor','QikProp_.rtvFG','QikProp_CNS','QikProp_mol_MW','QikProp_dipole','QikProp_SASA','QikProp_FOSA',\n",
    " 'QikProp_FISA','QikProp_PISA','QikProp_WPSA', 'QikProp_volume','QikProp_donorHB','QikProp_accptHB','QikProp_dip.2.V','QikProp_ACxDN..5.SA',\n",
    " 'QikProp_glob','QikProp_QPpolrz','QikProp_QPlogPC16','QikProp_QPlogPoct','QikProp_QPlogPw','QikProp_QPlogPo.w',\n",
    "'QikProp_QPlogS','QikProp_CIQPlogS','QikProp_QPlogHERG','QikProp_QPPCaco','QikProp_QPlogBB','QikProp_QPPMDCK','QikProp_QPlogKp',\n",
    " 'QikProp_IP.eV.','QikProp_EA.eV.','QikProp_.metab','QikProp_QPlogKhsa','QikProp_HumanOralAbsorption','QikProp_PercentHumanOralAbsorption','QikProp_SAfluorine',\n",
    " 'QikProp_SAamideO','QikProp_PSA', 'QikProp_.NandO','QikProp_RuleOfFive','QikProp_.ringatoms','QikProp_.in34','QikProp_.in56','QikProp_.noncon',\n",
    " 'QikProp_.nonHatm','QikProp_RuleOfThree','QikProp_ACxDN..5.SAxSASA.MW']\n",
    "LABEL=['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='Molecule', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.stars', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.amine', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.amidine', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.acid', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.amide', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.rotor', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.rtvFG', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_CNS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_mol_MW', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_dipole', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_SASA', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_FOSA', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_FISA', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_PISA', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_WPSA', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_volume', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_donorHB', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_accptHB', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_dip.2.V', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_ACxDN..5.SA', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_glob', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPpolrz', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogPC16', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogPoct', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogPw', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogPo.w', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_CIQPlogS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogHERG', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPPCaco', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogBB', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPPMDCK', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogKp', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_IP.eV.', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_EA.eV.', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.metab', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_QPlogKhsa', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_HumanOralAbsorption', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_PercentHumanOralAbsorption', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_SAfluorine', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_SAamideO', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_PSA', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.NandO', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_RuleOfFive', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.ringatoms', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.in34', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.in56', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.noncon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_.nonHatm', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_RuleOfThree', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='QikProp_ACxDN..5.SAxSASA.MW', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols=[tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_fn(data_set, num_epochs=None, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(x=pd.DataFrame({j:data_set[j] for j in FEATURES}),\n",
    "                                              y=pd.Series(data_set[LABEL].values),\n",
    "                                              num_epochs=num_epochs, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\saiva\\AppData\\Local\\Temp\\tmpysr6n_5p\n",
      "INFO:tensorflow:Using config: {'_session_config': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_max': 5, '_model_dir': 'C:\\\\Users\\\\saiva\\\\AppData\\\\Local\\\\Temp\\\\tmpysr6n_5p', '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': 1, '_save_summary_steps': 100, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "Classifier=tf.estimator.DNNClassifier(feature_columns=feature_cols, hidden_units=[51,51,30,20,10], n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Classifier.train(input_fn=get_input_fn(ML), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
