{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import SGDRegressor, LassoCV, LassoLarsCV, LassoLarsIC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from math import *\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the tsv files of the features and respective responses\n",
    "df=pd.read_csv('MP_Data.tsv', sep='\\t', index_col=False)\n",
    "tt_df=df.rename(columns={'x':'test_train'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diameter</th>\n",
       "      <th>petitjean</th>\n",
       "      <th>petitjeanSC</th>\n",
       "      <th>radius</th>\n",
       "      <th>VDistEq</th>\n",
       "      <th>VDistMa</th>\n",
       "      <th>weinerPath</th>\n",
       "      <th>weinerPol</th>\n",
       "      <th>a_aro</th>\n",
       "      <th>a_count</th>\n",
       "      <th>...</th>\n",
       "      <th>FASA_P</th>\n",
       "      <th>FCASA.</th>\n",
       "      <th>FCASA..1</th>\n",
       "      <th>VSA</th>\n",
       "      <th>dens</th>\n",
       "      <th>glob</th>\n",
       "      <th>std_dim1</th>\n",
       "      <th>std_dim2</th>\n",
       "      <th>std_dim3</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.037476</td>\n",
       "      <td>6.011166</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129128</td>\n",
       "      <td>0.326050</td>\n",
       "      <td>0.224950</td>\n",
       "      <td>138.13699</td>\n",
       "      <td>0.958985</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>1.853123</td>\n",
       "      <td>1.382682</td>\n",
       "      <td>0.326444</td>\n",
       "      <td>123.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.954872</td>\n",
       "      <td>8.805204</td>\n",
       "      <td>1046</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059026</td>\n",
       "      <td>0.847237</td>\n",
       "      <td>1.136764</td>\n",
       "      <td>296.30197</td>\n",
       "      <td>1.296254</td>\n",
       "      <td>0.258447</td>\n",
       "      <td>2.394159</td>\n",
       "      <td>1.966377</td>\n",
       "      <td>1.217135</td>\n",
       "      <td>262.82812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.083532</td>\n",
       "      <td>8.211762</td>\n",
       "      <td>742</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046707</td>\n",
       "      <td>0.817581</td>\n",
       "      <td>0.464419</td>\n",
       "      <td>296.54431</td>\n",
       "      <td>0.946322</td>\n",
       "      <td>0.162139</td>\n",
       "      <td>3.164745</td>\n",
       "      <td>1.552043</td>\n",
       "      <td>1.274330</td>\n",
       "      <td>266.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.616827</td>\n",
       "      <td>7.313269</td>\n",
       "      <td>288</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044688</td>\n",
       "      <td>0.417896</td>\n",
       "      <td>0.574827</td>\n",
       "      <td>187.43799</td>\n",
       "      <td>1.195242</td>\n",
       "      <td>0.018573</td>\n",
       "      <td>2.524826</td>\n",
       "      <td>1.446899</td>\n",
       "      <td>0.344087</td>\n",
       "      <td>164.95312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.609669</td>\n",
       "      <td>6.833154</td>\n",
       "      <td>203</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103925</td>\n",
       "      <td>0.595412</td>\n",
       "      <td>0.267278</td>\n",
       "      <td>193.42802</td>\n",
       "      <td>0.978233</td>\n",
       "      <td>0.088156</td>\n",
       "      <td>2.550456</td>\n",
       "      <td>1.430062</td>\n",
       "      <td>0.757260</td>\n",
       "      <td>165.79688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.575820</td>\n",
       "      <td>7.324709</td>\n",
       "      <td>284</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088718</td>\n",
       "      <td>0.641968</td>\n",
       "      <td>0.320366</td>\n",
       "      <td>218.67284</td>\n",
       "      <td>0.974160</td>\n",
       "      <td>0.042736</td>\n",
       "      <td>2.843004</td>\n",
       "      <td>1.581263</td>\n",
       "      <td>0.587728</td>\n",
       "      <td>193.21875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.473536</td>\n",
       "      <td>6.864490</td>\n",
       "      <td>197</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163559</td>\n",
       "      <td>0.432359</td>\n",
       "      <td>0.357734</td>\n",
       "      <td>188.12050</td>\n",
       "      <td>0.968377</td>\n",
       "      <td>0.019074</td>\n",
       "      <td>2.784485</td>\n",
       "      <td>1.359217</td>\n",
       "      <td>0.384559</td>\n",
       "      <td>167.48438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.331947</td>\n",
       "      <td>6.311081</td>\n",
       "      <td>120</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150887</td>\n",
       "      <td>0.674236</td>\n",
       "      <td>0.253037</td>\n",
       "      <td>166.62454</td>\n",
       "      <td>1.025273</td>\n",
       "      <td>0.025261</td>\n",
       "      <td>2.633108</td>\n",
       "      <td>1.334763</td>\n",
       "      <td>0.418498</td>\n",
       "      <td>136.68750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.964607</td>\n",
       "      <td>7.678609</td>\n",
       "      <td>472</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.724254</td>\n",
       "      <td>0.428256</td>\n",
       "      <td>261.96707</td>\n",
       "      <td>0.913285</td>\n",
       "      <td>0.031045</td>\n",
       "      <td>3.156831</td>\n",
       "      <td>1.637290</td>\n",
       "      <td>0.556221</td>\n",
       "      <td>232.45312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.492033</td>\n",
       "      <td>7.331616</td>\n",
       "      <td>271</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353197</td>\n",
       "      <td>0.269067</td>\n",
       "      <td>211.11229</td>\n",
       "      <td>0.909062</td>\n",
       "      <td>0.035267</td>\n",
       "      <td>2.516839</td>\n",
       "      <td>1.749148</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>198.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.215084</td>\n",
       "      <td>6.621265</td>\n",
       "      <td>144</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129647</td>\n",
       "      <td>0.568070</td>\n",
       "      <td>0.244231</td>\n",
       "      <td>169.23441</td>\n",
       "      <td>0.978263</td>\n",
       "      <td>0.034130</td>\n",
       "      <td>2.232579</td>\n",
       "      <td>1.548814</td>\n",
       "      <td>0.412455</td>\n",
       "      <td>151.45312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.103526</td>\n",
       "      <td>8.651907</td>\n",
       "      <td>1016</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100093</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>333.74268</td>\n",
       "      <td>1.014593</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>3.667873</td>\n",
       "      <td>2.044620</td>\n",
       "      <td>0.661501</td>\n",
       "      <td>294.04688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.172353</td>\n",
       "      <td>8.021215</td>\n",
       "      <td>657</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083367</td>\n",
       "      <td>0.891294</td>\n",
       "      <td>0.501066</td>\n",
       "      <td>290.32471</td>\n",
       "      <td>0.979809</td>\n",
       "      <td>0.079126</td>\n",
       "      <td>3.234525</td>\n",
       "      <td>2.066361</td>\n",
       "      <td>0.909850</td>\n",
       "      <td>249.32812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.645954</td>\n",
       "      <td>6.838005</td>\n",
       "      <td>212</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206543</td>\n",
       "      <td>0.588854</td>\n",
       "      <td>0.426566</td>\n",
       "      <td>199.01100</td>\n",
       "      <td>0.944719</td>\n",
       "      <td>0.143789</td>\n",
       "      <td>2.386840</td>\n",
       "      <td>1.241180</td>\n",
       "      <td>0.905079</td>\n",
       "      <td>173.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.116835</td>\n",
       "      <td>6.635355</td>\n",
       "      <td>138</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295517</td>\n",
       "      <td>0.144826</td>\n",
       "      <td>192.30621</td>\n",
       "      <td>0.831238</td>\n",
       "      <td>0.065099</td>\n",
       "      <td>2.208827</td>\n",
       "      <td>1.801646</td>\n",
       "      <td>0.563573</td>\n",
       "      <td>175.92188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.440839</td>\n",
       "      <td>6.583960</td>\n",
       "      <td>152</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168579</td>\n",
       "      <td>0.657007</td>\n",
       "      <td>0.241993</td>\n",
       "      <td>178.11615</td>\n",
       "      <td>0.972610</td>\n",
       "      <td>0.043325</td>\n",
       "      <td>2.602670</td>\n",
       "      <td>1.293131</td>\n",
       "      <td>0.541739</td>\n",
       "      <td>154.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>1.864984</td>\n",
       "      <td>6.930515</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094981</td>\n",
       "      <td>0.458418</td>\n",
       "      <td>0.199364</td>\n",
       "      <td>208.65715</td>\n",
       "      <td>0.879704</td>\n",
       "      <td>0.361984</td>\n",
       "      <td>1.945283</td>\n",
       "      <td>1.427529</td>\n",
       "      <td>1.170381</td>\n",
       "      <td>189.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "      <td>3.897747</td>\n",
       "      <td>9.399097</td>\n",
       "      <td>2782</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109526</td>\n",
       "      <td>1.993336</td>\n",
       "      <td>0.759835</td>\n",
       "      <td>482.21530</td>\n",
       "      <td>1.011069</td>\n",
       "      <td>0.075144</td>\n",
       "      <td>5.715179</td>\n",
       "      <td>1.642226</td>\n",
       "      <td>1.566671</td>\n",
       "      <td>405.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.111014</td>\n",
       "      <td>6.343670</td>\n",
       "      <td>111</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225422</td>\n",
       "      <td>0.621861</td>\n",
       "      <td>0.261949</td>\n",
       "      <td>173.44606</td>\n",
       "      <td>0.954850</td>\n",
       "      <td>0.382645</td>\n",
       "      <td>1.689691</td>\n",
       "      <td>1.297074</td>\n",
       "      <td>1.045214</td>\n",
       "      <td>148.92188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.841363</td>\n",
       "      <td>7.057745</td>\n",
       "      <td>270</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155281</td>\n",
       "      <td>0.929101</td>\n",
       "      <td>0.328421</td>\n",
       "      <td>209.91570</td>\n",
       "      <td>1.005831</td>\n",
       "      <td>0.036820</td>\n",
       "      <td>3.373866</td>\n",
       "      <td>1.161317</td>\n",
       "      <td>0.647394</td>\n",
       "      <td>180.14062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.342719</td>\n",
       "      <td>6.599185</td>\n",
       "      <td>147</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286153</td>\n",
       "      <td>0.235847</td>\n",
       "      <td>168.94328</td>\n",
       "      <td>0.919319</td>\n",
       "      <td>0.017801</td>\n",
       "      <td>2.249843</td>\n",
       "      <td>1.410680</td>\n",
       "      <td>0.300171</td>\n",
       "      <td>153.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.763862</td>\n",
       "      <td>7.058914</td>\n",
       "      <td>261</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114256</td>\n",
       "      <td>0.454641</td>\n",
       "      <td>0.434381</td>\n",
       "      <td>192.14456</td>\n",
       "      <td>1.116867</td>\n",
       "      <td>0.009320</td>\n",
       "      <td>2.907551</td>\n",
       "      <td>1.258343</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>158.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.069014</td>\n",
       "      <td>6.341292</td>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>0.358955</td>\n",
       "      <td>0.305420</td>\n",
       "      <td>143.44342</td>\n",
       "      <td>1.071982</td>\n",
       "      <td>0.030909</td>\n",
       "      <td>1.855768</td>\n",
       "      <td>1.410430</td>\n",
       "      <td>0.326260</td>\n",
       "      <td>126.98438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.451120</td>\n",
       "      <td>6.860934</td>\n",
       "      <td>188</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007464</td>\n",
       "      <td>0.416032</td>\n",
       "      <td>0.256968</td>\n",
       "      <td>206.14227</td>\n",
       "      <td>1.002195</td>\n",
       "      <td>0.058401</td>\n",
       "      <td>2.359899</td>\n",
       "      <td>1.745628</td>\n",
       "      <td>0.570301</td>\n",
       "      <td>182.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.011497</td>\n",
       "      <td>8.190375</td>\n",
       "      <td>688</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067148</td>\n",
       "      <td>0.753454</td>\n",
       "      <td>0.416760</td>\n",
       "      <td>296.70996</td>\n",
       "      <td>0.958390</td>\n",
       "      <td>0.120846</td>\n",
       "      <td>3.455375</td>\n",
       "      <td>1.354939</td>\n",
       "      <td>1.201189</td>\n",
       "      <td>269.57812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.815238</td>\n",
       "      <td>8.074170</td>\n",
       "      <td>567</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.619382</td>\n",
       "      <td>0.310618</td>\n",
       "      <td>342.15201</td>\n",
       "      <td>0.854597</td>\n",
       "      <td>0.126480</td>\n",
       "      <td>3.275252</td>\n",
       "      <td>1.926175</td>\n",
       "      <td>1.164813</td>\n",
       "      <td>307.12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.582369</td>\n",
       "      <td>6.846662</td>\n",
       "      <td>202</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232878</td>\n",
       "      <td>0.589891</td>\n",
       "      <td>0.413521</td>\n",
       "      <td>202.72173</td>\n",
       "      <td>0.956325</td>\n",
       "      <td>0.167625</td>\n",
       "      <td>2.534937</td>\n",
       "      <td>1.143422</td>\n",
       "      <td>1.037853</td>\n",
       "      <td>171.70312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.112197</td>\n",
       "      <td>6.007506</td>\n",
       "      <td>86</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156427</td>\n",
       "      <td>0.662649</td>\n",
       "      <td>0.208066</td>\n",
       "      <td>162.03949</td>\n",
       "      <td>0.952368</td>\n",
       "      <td>0.089620</td>\n",
       "      <td>2.179934</td>\n",
       "      <td>1.370603</td>\n",
       "      <td>0.652598</td>\n",
       "      <td>132.46875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.944840</td>\n",
       "      <td>7.274346</td>\n",
       "      <td>339</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194714</td>\n",
       "      <td>0.696591</td>\n",
       "      <td>0.384625</td>\n",
       "      <td>238.42363</td>\n",
       "      <td>0.928152</td>\n",
       "      <td>0.202991</td>\n",
       "      <td>2.515293</td>\n",
       "      <td>1.463880</td>\n",
       "      <td>1.133252</td>\n",
       "      <td>207.14062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>3.917071</td>\n",
       "      <td>8.798213</td>\n",
       "      <td>1812</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007766</td>\n",
       "      <td>0.947079</td>\n",
       "      <td>0.445880</td>\n",
       "      <td>411.07907</td>\n",
       "      <td>0.897444</td>\n",
       "      <td>0.008328</td>\n",
       "      <td>6.939130</td>\n",
       "      <td>1.223135</td>\n",
       "      <td>0.633240</td>\n",
       "      <td>361.54688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.432720</td>\n",
       "      <td>9.749645</td>\n",
       "      <td>2739</td>\n",
       "      <td>54</td>\n",
       "      <td>24</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059120</td>\n",
       "      <td>1.331589</td>\n",
       "      <td>0.789783</td>\n",
       "      <td>444.69284</td>\n",
       "      <td>0.998202</td>\n",
       "      <td>0.241440</td>\n",
       "      <td>3.745843</td>\n",
       "      <td>2.286851</td>\n",
       "      <td>1.840579</td>\n",
       "      <td>417.23438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.630577</td>\n",
       "      <td>7.094596</td>\n",
       "      <td>252</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307029</td>\n",
       "      <td>0.567474</td>\n",
       "      <td>0.922751</td>\n",
       "      <td>191.10207</td>\n",
       "      <td>1.074610</td>\n",
       "      <td>0.009548</td>\n",
       "      <td>2.822690</td>\n",
       "      <td>1.355110</td>\n",
       "      <td>0.275819</td>\n",
       "      <td>165.79688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>23</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>12</td>\n",
       "      <td>4.220913</td>\n",
       "      <td>9.998367</td>\n",
       "      <td>5174</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104864</td>\n",
       "      <td>2.405340</td>\n",
       "      <td>0.769689</td>\n",
       "      <td>571.06494</td>\n",
       "      <td>0.996555</td>\n",
       "      <td>0.061811</td>\n",
       "      <td>6.141288</td>\n",
       "      <td>2.657702</td>\n",
       "      <td>1.526831</td>\n",
       "      <td>500.34375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>13</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>7</td>\n",
       "      <td>3.402777</td>\n",
       "      <td>9.440133</td>\n",
       "      <td>2061</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124887</td>\n",
       "      <td>1.510654</td>\n",
       "      <td>0.559346</td>\n",
       "      <td>445.64777</td>\n",
       "      <td>0.966470</td>\n",
       "      <td>0.110590</td>\n",
       "      <td>3.787900</td>\n",
       "      <td>1.923547</td>\n",
       "      <td>1.259669</td>\n",
       "      <td>413.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.988054</td>\n",
       "      <td>8.029321</td>\n",
       "      <td>607</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264306</td>\n",
       "      <td>0.955981</td>\n",
       "      <td>0.502363</td>\n",
       "      <td>275.96344</td>\n",
       "      <td>1.258264</td>\n",
       "      <td>0.105571</td>\n",
       "      <td>3.382340</td>\n",
       "      <td>1.178125</td>\n",
       "      <td>1.098981</td>\n",
       "      <td>230.76562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.230799</td>\n",
       "      <td>8.335974</td>\n",
       "      <td>859</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134617</td>\n",
       "      <td>1.981435</td>\n",
       "      <td>0.341682</td>\n",
       "      <td>340.51971</td>\n",
       "      <td>1.019758</td>\n",
       "      <td>0.060972</td>\n",
       "      <td>3.726692</td>\n",
       "      <td>2.132911</td>\n",
       "      <td>0.920216</td>\n",
       "      <td>290.67188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.928959</td>\n",
       "      <td>7.687527</td>\n",
       "      <td>468</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311457</td>\n",
       "      <td>1.755831</td>\n",
       "      <td>0.471999</td>\n",
       "      <td>273.03738</td>\n",
       "      <td>0.969992</td>\n",
       "      <td>0.106094</td>\n",
       "      <td>3.246898</td>\n",
       "      <td>1.387182</td>\n",
       "      <td>1.057581</td>\n",
       "      <td>233.29688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.943185</td>\n",
       "      <td>8.509120</td>\n",
       "      <td>802</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150157</td>\n",
       "      <td>0.926504</td>\n",
       "      <td>0.334964</td>\n",
       "      <td>318.52930</td>\n",
       "      <td>0.962941</td>\n",
       "      <td>0.139121</td>\n",
       "      <td>2.965015</td>\n",
       "      <td>1.595772</td>\n",
       "      <td>1.105917</td>\n",
       "      <td>299.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.007861</td>\n",
       "      <td>8.373358</td>\n",
       "      <td>803</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021925</td>\n",
       "      <td>1.698029</td>\n",
       "      <td>0.284835</td>\n",
       "      <td>333.91647</td>\n",
       "      <td>0.945028</td>\n",
       "      <td>0.180554</td>\n",
       "      <td>2.862705</td>\n",
       "      <td>2.124698</td>\n",
       "      <td>1.216409</td>\n",
       "      <td>304.17188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>20</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>4.063982</td>\n",
       "      <td>9.465725</td>\n",
       "      <td>3235</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206101</td>\n",
       "      <td>2.111671</td>\n",
       "      <td>0.828672</td>\n",
       "      <td>497.45853</td>\n",
       "      <td>1.027950</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>6.411793</td>\n",
       "      <td>2.181164</td>\n",
       "      <td>1.102727</td>\n",
       "      <td>428.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.350606</td>\n",
       "      <td>8.468756</td>\n",
       "      <td>1014</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032591</td>\n",
       "      <td>0.800455</td>\n",
       "      <td>0.762905</td>\n",
       "      <td>330.38092</td>\n",
       "      <td>1.077245</td>\n",
       "      <td>0.085065</td>\n",
       "      <td>3.930042</td>\n",
       "      <td>1.682398</td>\n",
       "      <td>1.146229</td>\n",
       "      <td>288.14062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.304868</td>\n",
       "      <td>8.467934</td>\n",
       "      <td>993</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784656</td>\n",
       "      <td>0.353654</td>\n",
       "      <td>338.37573</td>\n",
       "      <td>0.957881</td>\n",
       "      <td>0.049678</td>\n",
       "      <td>4.300103</td>\n",
       "      <td>1.724484</td>\n",
       "      <td>0.958431</td>\n",
       "      <td>310.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.371190</td>\n",
       "      <td>8.885169</td>\n",
       "      <td>1387</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053199</td>\n",
       "      <td>0.915717</td>\n",
       "      <td>1.118313</td>\n",
       "      <td>361.44711</td>\n",
       "      <td>1.100951</td>\n",
       "      <td>0.083362</td>\n",
       "      <td>4.125259</td>\n",
       "      <td>1.787146</td>\n",
       "      <td>1.191065</td>\n",
       "      <td>315.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.019458</td>\n",
       "      <td>8.661794</td>\n",
       "      <td>979</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012993</td>\n",
       "      <td>1.607528</td>\n",
       "      <td>0.314378</td>\n",
       "      <td>356.62656</td>\n",
       "      <td>0.894414</td>\n",
       "      <td>0.183280</td>\n",
       "      <td>3.012048</td>\n",
       "      <td>2.115294</td>\n",
       "      <td>1.289496</td>\n",
       "      <td>330.32812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.868501</td>\n",
       "      <td>7.499315</td>\n",
       "      <td>390</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101264</td>\n",
       "      <td>1.266159</td>\n",
       "      <td>0.405293</td>\n",
       "      <td>271.49490</td>\n",
       "      <td>0.976947</td>\n",
       "      <td>0.127191</td>\n",
       "      <td>3.072506</td>\n",
       "      <td>1.277227</td>\n",
       "      <td>1.095773</td>\n",
       "      <td>234.14062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.396022</td>\n",
       "      <td>8.618585</td>\n",
       "      <td>1172</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252794</td>\n",
       "      <td>1.927637</td>\n",
       "      <td>1.053422</td>\n",
       "      <td>356.12244</td>\n",
       "      <td>1.005461</td>\n",
       "      <td>0.065422</td>\n",
       "      <td>4.058465</td>\n",
       "      <td>1.645821</td>\n",
       "      <td>1.038064</td>\n",
       "      <td>306.70312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.110328</td>\n",
       "      <td>8.033091</td>\n",
       "      <td>646</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110379</td>\n",
       "      <td>1.257559</td>\n",
       "      <td>0.663095</td>\n",
       "      <td>286.55423</td>\n",
       "      <td>1.102742</td>\n",
       "      <td>0.262205</td>\n",
       "      <td>2.285389</td>\n",
       "      <td>1.738979</td>\n",
       "      <td>1.170255</td>\n",
       "      <td>256.07812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3.435096</td>\n",
       "      <td>9.321297</td>\n",
       "      <td>1910</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235066</td>\n",
       "      <td>1.190890</td>\n",
       "      <td>0.748277</td>\n",
       "      <td>426.22278</td>\n",
       "      <td>0.970890</td>\n",
       "      <td>0.092096</td>\n",
       "      <td>4.019930</td>\n",
       "      <td>1.654345</td>\n",
       "      <td>1.219940</td>\n",
       "      <td>403.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.212369</td>\n",
       "      <td>6.341495</td>\n",
       "      <td>122</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096085</td>\n",
       "      <td>0.710108</td>\n",
       "      <td>0.255627</td>\n",
       "      <td>200.08705</td>\n",
       "      <td>0.857344</td>\n",
       "      <td>0.132533</td>\n",
       "      <td>2.207653</td>\n",
       "      <td>1.483309</td>\n",
       "      <td>0.803699</td>\n",
       "      <td>167.06250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4390</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.836740</td>\n",
       "      <td>8.691143</td>\n",
       "      <td>934</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067507</td>\n",
       "      <td>2.026548</td>\n",
       "      <td>0.437741</td>\n",
       "      <td>340.40399</td>\n",
       "      <td>0.938120</td>\n",
       "      <td>0.257965</td>\n",
       "      <td>2.719839</td>\n",
       "      <td>2.057141</td>\n",
       "      <td>1.381412</td>\n",
       "      <td>318.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>13</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>7</td>\n",
       "      <td>3.476305</td>\n",
       "      <td>8.448178</td>\n",
       "      <td>1096</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139503</td>\n",
       "      <td>1.361360</td>\n",
       "      <td>0.443640</td>\n",
       "      <td>310.53876</td>\n",
       "      <td>1.027772</td>\n",
       "      <td>0.022343</td>\n",
       "      <td>4.221582</td>\n",
       "      <td>1.651008</td>\n",
       "      <td>0.631026</td>\n",
       "      <td>277.59375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4392</th>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.464367</td>\n",
       "      <td>5.966286</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394016</td>\n",
       "      <td>1.021514</td>\n",
       "      <td>0.571486</td>\n",
       "      <td>168.79506</td>\n",
       "      <td>0.947847</td>\n",
       "      <td>0.140899</td>\n",
       "      <td>2.148943</td>\n",
       "      <td>1.122746</td>\n",
       "      <td>0.806638</td>\n",
       "      <td>136.26562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4393</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.879264</td>\n",
       "      <td>8.673676</td>\n",
       "      <td>897</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081234</td>\n",
       "      <td>1.830255</td>\n",
       "      <td>0.389860</td>\n",
       "      <td>322.70428</td>\n",
       "      <td>0.988634</td>\n",
       "      <td>0.056690</td>\n",
       "      <td>3.205171</td>\n",
       "      <td>2.154311</td>\n",
       "      <td>0.763144</td>\n",
       "      <td>300.79688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.238938</td>\n",
       "      <td>8.167924</td>\n",
       "      <td>764</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233432</td>\n",
       "      <td>0.614079</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>293.80347</td>\n",
       "      <td>1.026686</td>\n",
       "      <td>0.089191</td>\n",
       "      <td>3.514620</td>\n",
       "      <td>1.413340</td>\n",
       "      <td>1.049635</td>\n",
       "      <td>253.54688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.996205</td>\n",
       "      <td>8.037785</td>\n",
       "      <td>608</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075851</td>\n",
       "      <td>1.626858</td>\n",
       "      <td>0.236096</td>\n",
       "      <td>314.13821</td>\n",
       "      <td>0.872062</td>\n",
       "      <td>0.149392</td>\n",
       "      <td>3.281516</td>\n",
       "      <td>1.568015</td>\n",
       "      <td>1.268346</td>\n",
       "      <td>281.39062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.240176</td>\n",
       "      <td>9.128287</td>\n",
       "      <td>1517</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137342</td>\n",
       "      <td>2.291194</td>\n",
       "      <td>0.440418</td>\n",
       "      <td>377.04456</td>\n",
       "      <td>1.018824</td>\n",
       "      <td>0.047208</td>\n",
       "      <td>3.593031</td>\n",
       "      <td>2.247911</td>\n",
       "      <td>0.780668</td>\n",
       "      <td>348.89062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.048637</td>\n",
       "      <td>8.206570</td>\n",
       "      <td>701</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305459</td>\n",
       "      <td>1.171489</td>\n",
       "      <td>0.757903</td>\n",
       "      <td>271.34589</td>\n",
       "      <td>1.164466</td>\n",
       "      <td>0.054237</td>\n",
       "      <td>2.984556</td>\n",
       "      <td>1.680681</td>\n",
       "      <td>0.695071</td>\n",
       "      <td>229.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.848982</td>\n",
       "      <td>7.696564</td>\n",
       "      <td>436</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193192</td>\n",
       "      <td>0.984402</td>\n",
       "      <td>0.602598</td>\n",
       "      <td>250.76385</td>\n",
       "      <td>1.083378</td>\n",
       "      <td>0.141232</td>\n",
       "      <td>3.015607</td>\n",
       "      <td>1.471790</td>\n",
       "      <td>1.133291</td>\n",
       "      <td>218.10938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.165411</td>\n",
       "      <td>8.164854</td>\n",
       "      <td>735</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158122</td>\n",
       "      <td>1.188120</td>\n",
       "      <td>0.620566</td>\n",
       "      <td>274.64322</td>\n",
       "      <td>1.095956</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>4.006955</td>\n",
       "      <td>1.370780</td>\n",
       "      <td>0.620302</td>\n",
       "      <td>248.48438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.130244</td>\n",
       "      <td>8.644922</td>\n",
       "      <td>1021</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024293</td>\n",
       "      <td>1.373384</td>\n",
       "      <td>0.469207</td>\n",
       "      <td>351.10757</td>\n",
       "      <td>1.032771</td>\n",
       "      <td>0.075947</td>\n",
       "      <td>3.648960</td>\n",
       "      <td>2.164404</td>\n",
       "      <td>1.005596</td>\n",
       "      <td>322.31250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4401 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      diameter  petitjean  petitjeanSC  radius   VDistEq   VDistMa  \\\n",
       "0            5   0.400000     0.666667       3  2.037476  6.011166   \n",
       "1            9   0.444444     0.800000       5  2.954872  8.805204   \n",
       "2           10   0.500000     1.000000       5  3.083532  8.211762   \n",
       "3            7   0.428571     0.750000       4  2.616827  7.313269   \n",
       "4            7   0.428571     0.750000       4  2.609669  6.833154   \n",
       "5            7   0.428571     0.750000       4  2.575820  7.324709   \n",
       "6            6   0.500000     1.000000       3  2.473536  6.864490   \n",
       "7            6   0.500000     1.000000       3  2.331947  6.311081   \n",
       "8            9   0.444444     0.800000       5  2.964607  7.678609   \n",
       "9            7   0.428571     0.750000       4  2.492033  7.331616   \n",
       "10           5   0.400000     0.666667       3  2.215084  6.621265   \n",
       "11          10   0.500000     1.000000       5  3.103526  8.651907   \n",
       "12          11   0.454545     0.833333       6  3.172353  8.021215   \n",
       "13           7   0.428571     0.750000       4  2.645954  6.838005   \n",
       "14           5   0.400000     0.666667       3  2.116835  6.635355   \n",
       "15           7   0.428571     0.750000       4  2.440839  6.583960   \n",
       "16           4   0.250000     0.333333       3  1.864984  6.930515   \n",
       "17          17   0.470588     0.888889       9  3.897747  9.399097   \n",
       "18           5   0.400000     0.666667       3  2.111014  6.343670   \n",
       "19           9   0.444444     0.800000       5  2.841363  7.057745   \n",
       "20           6   0.333333     0.500000       4  2.342719  6.599185   \n",
       "21           8   0.500000     1.000000       4  2.763862  7.058914   \n",
       "22           5   0.400000     0.666667       3  2.069014  6.341292   \n",
       "23           7   0.428571     0.750000       4  2.451120  6.860934   \n",
       "24           9   0.444444     0.800000       5  3.011497  8.190375   \n",
       "25           8   0.500000     1.000000       4  2.815238  8.074170   \n",
       "26           7   0.428571     0.750000       4  2.582369  6.846662   \n",
       "27           5   0.400000     0.666667       3  2.112197  6.007506   \n",
       "28           9   0.444444     0.800000       5  2.944840  7.274346   \n",
       "29          19   0.473684     0.900000      10  3.917071  8.798213   \n",
       "...        ...        ...          ...     ...       ...       ...   \n",
       "4371        12   0.500000     1.000000       6  3.432720  9.749645   \n",
       "4372         7   0.428571     0.750000       4  2.630577  7.094596   \n",
       "4373        23   0.478261     0.916667      12  4.220913  9.998367   \n",
       "4374        13   0.461538     0.857143       7  3.402777  9.440133   \n",
       "4375         9   0.444444     0.800000       5  2.988054  8.029321   \n",
       "4376        11   0.454545     0.833333       6  3.230799  8.335974   \n",
       "4377         8   0.500000     1.000000       4  2.928959  7.687527   \n",
       "4378        10   0.500000     1.000000       5  2.943185  8.509120   \n",
       "4379         9   0.444444     0.800000       5  3.007861  8.373358   \n",
       "4380        20   0.500000     1.000000      10  4.063982  9.465725   \n",
       "4381        12   0.500000     1.000000       6  3.350606  8.468756   \n",
       "4382        11   0.454545     0.833333       6  3.304868  8.467934   \n",
       "4383        12   0.500000     1.000000       6  3.371190  8.885169   \n",
       "4384         9   0.444444     0.800000       5  3.019458  8.661794   \n",
       "4385         8   0.500000     1.000000       4  2.868501  7.499315   \n",
       "4386        12   0.500000     1.000000       6  3.396022  8.618585   \n",
       "4387        10   0.500000     1.000000       5  3.110328  8.033091   \n",
       "4388        14   0.500000     1.000000       7  3.435096  9.321297   \n",
       "4389         5   0.400000     0.666667       3  2.212369  6.341495   \n",
       "4390         8   0.500000     1.000000       4  2.836740  8.691143   \n",
       "4391        13   0.461538     0.857143       7  3.476305  8.448178   \n",
       "4392         6   0.500000     1.000000       3  2.464367  5.966286   \n",
       "4393         9   0.444444     0.800000       5  2.879264  8.673676   \n",
       "4394        11   0.454545     0.833333       6  3.238938  8.167924   \n",
       "4395         9   0.444444     0.800000       5  2.996205  8.037785   \n",
       "4396        11   0.454545     0.833333       6  3.240176  9.128287   \n",
       "4397        10   0.500000     1.000000       5  3.048637  8.206570   \n",
       "4398         8   0.500000     1.000000       4  2.848982  7.696564   \n",
       "4399        10   0.500000     1.000000       5  3.165411  8.164854   \n",
       "4400        10   0.500000     1.000000       5  3.130244  8.644922   \n",
       "\n",
       "      weinerPath  weinerPol  a_aro  a_count    ...        FASA_P    FCASA.  \\\n",
       "0             82          9      6       15    ...      0.129128  0.326050   \n",
       "1           1046         40     11       31    ...      0.059026  0.847237   \n",
       "2            742         24     12       35    ...      0.046707  0.817581   \n",
       "3            288         20     10       20    ...      0.044688  0.417896   \n",
       "4            203         13      6       22    ...      0.103925  0.595412   \n",
       "5            284         21      6       26    ...      0.088718  0.641968   \n",
       "6            197         15      6       22    ...      0.163559  0.432359   \n",
       "7            120         10      5       18    ...      0.150887  0.674236   \n",
       "8            472         21     12       32    ...      0.038050  0.724254   \n",
       "9            271         22     12       26    ...      0.000000  0.353197   \n",
       "10           144         14      6       19    ...      0.129647  0.568070   \n",
       "11          1016         32     14       40    ...      0.100093  1.137040   \n",
       "12           657         23     11       34    ...      0.083367  0.891294   \n",
       "13           212         13      6       24    ...      0.206543  0.588854   \n",
       "14           138         16      6       25    ...      0.000000  0.295517   \n",
       "15           152         13      6       21    ...      0.168579  0.657007   \n",
       "16           156         23      0       30    ...      0.094981  0.458418   \n",
       "17          2782         38      6       60    ...      0.109526  1.993336   \n",
       "18           111         13      0       24    ...      0.225422  0.621861   \n",
       "19           270         16      6       24    ...      0.155281  0.929101   \n",
       "20           147         13      6       18    ...      0.000000  0.286153   \n",
       "21           261         17      6       20    ...      0.114256  0.454641   \n",
       "22           108         12      6       15    ...      0.242591  0.358955   \n",
       "23           188         16      6       23    ...      0.007464  0.416032   \n",
       "24           688         31      6       41    ...      0.067148  0.753454   \n",
       "25           567         30      6       44    ...      0.004437  0.619382   \n",
       "26           202         15      6       24    ...      0.232878  0.589891   \n",
       "27            86          9      5       19    ...      0.156427  0.662649   \n",
       "28           339         16      6       30    ...      0.194714  0.696591   \n",
       "29          1812         29     12       52    ...      0.007766  0.947079   \n",
       "...          ...        ...    ...      ...    ...           ...       ...   \n",
       "4371        2739         54     24       52    ...      0.059120  1.331589   \n",
       "4372         252         16      6       21    ...      0.307029  0.567474   \n",
       "4373        5174         55     12       74    ...      0.104864  2.405340   \n",
       "4374        2061         57      0       66    ...      0.124887  1.510654   \n",
       "4375         607         28      6       32    ...      0.264306  0.955981   \n",
       "4376         859         27      9       42    ...      0.134617  1.981435   \n",
       "4377         468         19      6       36    ...      0.311457  1.755831   \n",
       "4378         802         44      0       49    ...      0.150157  0.926504   \n",
       "4379         803         26     11       43    ...      0.021925  1.698029   \n",
       "4380        3235         38      6       66    ...      0.206101  2.111671   \n",
       "4381        1014         31      6       41    ...      0.032591  0.800455   \n",
       "4382         993         30     12       40    ...      0.000000  0.784656   \n",
       "4383        1387         34     12       43    ...      0.053199  0.915717   \n",
       "4384         979         37     12       49    ...      0.012993  1.607528   \n",
       "4385         390         18      6       34    ...      0.101264  1.266159   \n",
       "4386        1172         29      6       46    ...      0.252794  1.927637   \n",
       "4387         646         23      6       32    ...      0.110379  1.257559   \n",
       "4388        1910         56      0       67    ...      0.235066  1.190890   \n",
       "4389         122         12      0       27    ...      0.096085  0.710108   \n",
       "4390         934         37     12       46    ...      0.067507  2.026548   \n",
       "4391        1096         29     12       36    ...      0.139503  1.361360   \n",
       "4392         104          7      0       20    ...      0.394016  1.021514   \n",
       "4393         897         43      9       43    ...      0.081234  1.830255   \n",
       "4394         764         28      6       37    ...      0.233432  0.614079   \n",
       "4395         608         25      6       43    ...      0.075851  1.626858   \n",
       "4396        1517         50      9       53    ...      0.137342  2.291194   \n",
       "4397         701         29      0       32    ...      0.305459  1.171489   \n",
       "4398         436         23      9       28    ...      0.193192  0.984402   \n",
       "4399         735         28     11       31    ...      0.158122  1.188120   \n",
       "4400        1021         36     12       41    ...      0.024293  1.373384   \n",
       "\n",
       "      FCASA..1        VSA      dens      glob  std_dim1  std_dim2  std_dim3  \\\n",
       "0     0.224950  138.13699  0.958985  0.031032  1.853123  1.382682  0.326444   \n",
       "1     1.136764  296.30197  1.296254  0.258447  2.394159  1.966377  1.217135   \n",
       "2     0.464419  296.54431  0.946322  0.162139  3.164745  1.552043  1.274330   \n",
       "3     0.574827  187.43799  1.195242  0.018573  2.524826  1.446899  0.344087   \n",
       "4     0.267278  193.42802  0.978233  0.088156  2.550456  1.430062  0.757260   \n",
       "5     0.320366  218.67284  0.974160  0.042736  2.843004  1.581263  0.587728   \n",
       "6     0.357734  188.12050  0.968377  0.019074  2.784485  1.359217  0.384559   \n",
       "7     0.253037  166.62454  1.025273  0.025261  2.633108  1.334763  0.418498   \n",
       "8     0.428256  261.96707  0.913285  0.031045  3.156831  1.637290  0.556221   \n",
       "9     0.269067  211.11229  0.909062  0.035267  2.516839  1.749148  0.472649   \n",
       "10    0.244231  169.23441  0.978263  0.034130  2.232579  1.548814  0.412455   \n",
       "11    0.543016  333.74268  1.014593  0.032526  3.667873  2.044620  0.661501   \n",
       "12    0.501066  290.32471  0.979809  0.079126  3.234525  2.066361  0.909850   \n",
       "13    0.426566  199.01100  0.944719  0.143789  2.386840  1.241180  0.905079   \n",
       "14    0.144826  192.30621  0.831238  0.065099  2.208827  1.801646  0.563573   \n",
       "15    0.241993  178.11615  0.972610  0.043325  2.602670  1.293131  0.541739   \n",
       "16    0.199364  208.65715  0.879704  0.361984  1.945283  1.427529  1.170381   \n",
       "17    0.759835  482.21530  1.011069  0.075144  5.715179  1.642226  1.566671   \n",
       "18    0.261949  173.44606  0.954850  0.382645  1.689691  1.297074  1.045214   \n",
       "19    0.328421  209.91570  1.005831  0.036820  3.373866  1.161317  0.647394   \n",
       "20    0.235847  168.94328  0.919319  0.017801  2.249843  1.410680  0.300171   \n",
       "21    0.434381  192.14456  1.116867  0.009320  2.907551  1.258343  0.280702   \n",
       "22    0.305420  143.44342  1.071982  0.030909  1.855768  1.410430  0.326260   \n",
       "23    0.256968  206.14227  1.002195  0.058401  2.359899  1.745628  0.570301   \n",
       "24    0.416760  296.70996  0.958390  0.120846  3.455375  1.354939  1.201189   \n",
       "25    0.310618  342.15201  0.854597  0.126480  3.275252  1.926175  1.164813   \n",
       "26    0.413521  202.72173  0.956325  0.167625  2.534937  1.143422  1.037853   \n",
       "27    0.208066  162.03949  0.952368  0.089620  2.179934  1.370603  0.652598   \n",
       "28    0.384625  238.42363  0.928152  0.202991  2.515293  1.463880  1.133252   \n",
       "29    0.445880  411.07907  0.897444  0.008328  6.939130  1.223135  0.633240   \n",
       "...        ...        ...       ...       ...       ...       ...       ...   \n",
       "4371  0.789783  444.69284  0.998202  0.241440  3.745843  2.286851  1.840579   \n",
       "4372  0.922751  191.10207  1.074610  0.009548  2.822690  1.355110  0.275819   \n",
       "4373  0.769689  571.06494  0.996555  0.061811  6.141288  2.657702  1.526831   \n",
       "4374  0.559346  445.64777  0.966470  0.110590  3.787900  1.923547  1.259669   \n",
       "4375  0.502363  275.96344  1.258264  0.105571  3.382340  1.178125  1.098981   \n",
       "4376  0.341682  340.51971  1.019758  0.060972  3.726692  2.132911  0.920216   \n",
       "4377  0.471999  273.03738  0.969992  0.106094  3.246898  1.387182  1.057581   \n",
       "4378  0.334964  318.52930  0.962941  0.139121  2.965015  1.595772  1.105917   \n",
       "4379  0.284835  333.91647  0.945028  0.180554  2.862705  2.124698  1.216409   \n",
       "4380  0.828672  497.45853  1.027950  0.029579  6.411793  2.181164  1.102727   \n",
       "4381  0.762905  330.38092  1.077245  0.085065  3.930042  1.682398  1.146229   \n",
       "4382  0.353654  338.37573  0.957881  0.049678  4.300103  1.724484  0.958431   \n",
       "4383  1.118313  361.44711  1.100951  0.083362  4.125259  1.787146  1.191065   \n",
       "4384  0.314378  356.62656  0.894414  0.183280  3.012048  2.115294  1.289496   \n",
       "4385  0.405293  271.49490  0.976947  0.127191  3.072506  1.277227  1.095773   \n",
       "4386  1.053422  356.12244  1.005461  0.065422  4.058465  1.645821  1.038064   \n",
       "4387  0.663095  286.55423  1.102742  0.262205  2.285389  1.738979  1.170255   \n",
       "4388  0.748277  426.22278  0.970890  0.092096  4.019930  1.654345  1.219940   \n",
       "4389  0.255627  200.08705  0.857344  0.132533  2.207653  1.483309  0.803699   \n",
       "4390  0.437741  340.40399  0.938120  0.257965  2.719839  2.057141  1.381412   \n",
       "4391  0.443640  310.53876  1.027772  0.022343  4.221582  1.651008  0.631026   \n",
       "4392  0.571486  168.79506  0.947847  0.140899  2.148943  1.122746  0.806638   \n",
       "4393  0.389860  322.70428  0.988634  0.056690  3.205171  2.154311  0.763144   \n",
       "4394  0.769841  293.80347  1.026686  0.089191  3.514620  1.413340  1.049635   \n",
       "4395  0.236096  314.13821  0.872062  0.149392  3.281516  1.568015  1.268346   \n",
       "4396  0.440418  377.04456  1.018824  0.047208  3.593031  2.247911  0.780668   \n",
       "4397  0.757903  271.34589  1.164466  0.054237  2.984556  1.680681  0.695071   \n",
       "4398  0.602598  250.76385  1.083378  0.141232  3.015607  1.471790  1.133291   \n",
       "4399  0.620566  274.64322  1.095956  0.023965  4.006955  1.370780  0.620302   \n",
       "4400  0.469207  351.10757  1.032771  0.075947  3.648960  2.164404  1.005596   \n",
       "\n",
       "            vol  \n",
       "0     123.18750  \n",
       "1     262.82812  \n",
       "2     266.62500  \n",
       "3     164.95312  \n",
       "4     165.79688  \n",
       "5     193.21875  \n",
       "6     167.48438  \n",
       "7     136.68750  \n",
       "8     232.45312  \n",
       "9     198.28125  \n",
       "10    151.45312  \n",
       "11    294.04688  \n",
       "12    249.32812  \n",
       "13    173.81250  \n",
       "14    175.92188  \n",
       "15    154.40625  \n",
       "16    189.00000  \n",
       "17    405.00000  \n",
       "18    148.92188  \n",
       "19    180.14062  \n",
       "20    153.56250  \n",
       "21    158.62500  \n",
       "22    126.98438  \n",
       "23    182.25000  \n",
       "24    269.57812  \n",
       "25    307.12500  \n",
       "26    171.70312  \n",
       "27    132.46875  \n",
       "28    207.14062  \n",
       "29    361.54688  \n",
       "...         ...  \n",
       "4371  417.23438  \n",
       "4372  165.79688  \n",
       "4373  500.34375  \n",
       "4374  413.43750  \n",
       "4375  230.76562  \n",
       "4376  290.67188  \n",
       "4377  233.29688  \n",
       "4378  299.53125  \n",
       "4379  304.17188  \n",
       "4380  428.62500  \n",
       "4381  288.14062  \n",
       "4382  310.50000  \n",
       "4383  315.56250  \n",
       "4384  330.32812  \n",
       "4385  234.14062  \n",
       "4386  306.70312  \n",
       "4387  256.07812  \n",
       "4388  403.31250  \n",
       "4389  167.06250  \n",
       "4390  318.09375  \n",
       "4391  277.59375  \n",
       "4392  136.26562  \n",
       "4393  300.79688  \n",
       "4394  253.54688  \n",
       "4395  281.39062  \n",
       "4396  348.89062  \n",
       "4397  229.50000  \n",
       "4398  218.10938  \n",
       "4399  248.48438  \n",
       "4400  322.31250  \n",
       "\n",
       "[4401 rows x 202 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df=pd.read_csv('MP_Descriptors.tsv', sep='\\t', index_col=False)\n",
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_df_init=pd.read_csv('MP_Outcome.tsv', sep='\\t')\n",
    "y_df=y_df_init.rename(columns={'x':'outcome'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sort the data according to test and train sets\n",
    "x_train_df=x_df[tt_df['test_train']=='Train']\n",
    "x_test_df=x_df[tt_df['test_train']=='Test']\n",
    "\n",
    "y_train_df=y_df[tt_df['test_train']=='Train']\n",
    "y_test_df=y_df[tt_df['test_train']=='Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4126, 202), (4126, 1), (275, 202), (275, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df.shape, y_train_df.shape, x_test_df.shape, y_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diameter</th>\n",
       "      <th>petitjean</th>\n",
       "      <th>petitjeanSC</th>\n",
       "      <th>radius</th>\n",
       "      <th>VDistEq</th>\n",
       "      <th>VDistMa</th>\n",
       "      <th>weinerPath</th>\n",
       "      <th>weinerPol</th>\n",
       "      <th>a_aro</th>\n",
       "      <th>a_count</th>\n",
       "      <th>...</th>\n",
       "      <th>FASA_P</th>\n",
       "      <th>FCASA.</th>\n",
       "      <th>FCASA..1</th>\n",
       "      <th>VSA</th>\n",
       "      <th>dens</th>\n",
       "      <th>glob</th>\n",
       "      <th>std_dim1</th>\n",
       "      <th>std_dim2</th>\n",
       "      <th>std_dim3</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.037476</td>\n",
       "      <td>6.011166</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129128</td>\n",
       "      <td>0.326050</td>\n",
       "      <td>0.224950</td>\n",
       "      <td>138.13699</td>\n",
       "      <td>0.958985</td>\n",
       "      <td>3.103191e-02</td>\n",
       "      <td>1.853123</td>\n",
       "      <td>1.382682</td>\n",
       "      <td>0.326444</td>\n",
       "      <td>123.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.954872</td>\n",
       "      <td>8.805204</td>\n",
       "      <td>1046</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059026</td>\n",
       "      <td>0.847237</td>\n",
       "      <td>1.136764</td>\n",
       "      <td>296.30197</td>\n",
       "      <td>1.296254</td>\n",
       "      <td>2.584471e-01</td>\n",
       "      <td>2.394159</td>\n",
       "      <td>1.966377</td>\n",
       "      <td>1.217135</td>\n",
       "      <td>262.82812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.083532</td>\n",
       "      <td>8.211762</td>\n",
       "      <td>742</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046707</td>\n",
       "      <td>0.817581</td>\n",
       "      <td>0.464419</td>\n",
       "      <td>296.54431</td>\n",
       "      <td>0.946322</td>\n",
       "      <td>1.621386e-01</td>\n",
       "      <td>3.164745</td>\n",
       "      <td>1.552043</td>\n",
       "      <td>1.274330</td>\n",
       "      <td>266.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.616827</td>\n",
       "      <td>7.313269</td>\n",
       "      <td>288</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044688</td>\n",
       "      <td>0.417896</td>\n",
       "      <td>0.574827</td>\n",
       "      <td>187.43799</td>\n",
       "      <td>1.195242</td>\n",
       "      <td>1.857262e-02</td>\n",
       "      <td>2.524826</td>\n",
       "      <td>1.446899</td>\n",
       "      <td>0.344087</td>\n",
       "      <td>164.95312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.609669</td>\n",
       "      <td>6.833154</td>\n",
       "      <td>203</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103925</td>\n",
       "      <td>0.595412</td>\n",
       "      <td>0.267278</td>\n",
       "      <td>193.42802</td>\n",
       "      <td>0.978233</td>\n",
       "      <td>8.815648e-02</td>\n",
       "      <td>2.550456</td>\n",
       "      <td>1.430062</td>\n",
       "      <td>0.757260</td>\n",
       "      <td>165.79688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.575820</td>\n",
       "      <td>7.324709</td>\n",
       "      <td>284</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088718</td>\n",
       "      <td>0.641968</td>\n",
       "      <td>0.320366</td>\n",
       "      <td>218.67284</td>\n",
       "      <td>0.974160</td>\n",
       "      <td>4.273635e-02</td>\n",
       "      <td>2.843004</td>\n",
       "      <td>1.581263</td>\n",
       "      <td>0.587728</td>\n",
       "      <td>193.21875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.473536</td>\n",
       "      <td>6.864490</td>\n",
       "      <td>197</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163559</td>\n",
       "      <td>0.432359</td>\n",
       "      <td>0.357734</td>\n",
       "      <td>188.12050</td>\n",
       "      <td>0.968377</td>\n",
       "      <td>1.907373e-02</td>\n",
       "      <td>2.784485</td>\n",
       "      <td>1.359217</td>\n",
       "      <td>0.384559</td>\n",
       "      <td>167.48438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.331947</td>\n",
       "      <td>6.311081</td>\n",
       "      <td>120</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150887</td>\n",
       "      <td>0.674236</td>\n",
       "      <td>0.253037</td>\n",
       "      <td>166.62454</td>\n",
       "      <td>1.025273</td>\n",
       "      <td>2.526089e-02</td>\n",
       "      <td>2.633108</td>\n",
       "      <td>1.334763</td>\n",
       "      <td>0.418498</td>\n",
       "      <td>136.68750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.964607</td>\n",
       "      <td>7.678609</td>\n",
       "      <td>472</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.724254</td>\n",
       "      <td>0.428256</td>\n",
       "      <td>261.96707</td>\n",
       "      <td>0.913285</td>\n",
       "      <td>3.104502e-02</td>\n",
       "      <td>3.156831</td>\n",
       "      <td>1.637290</td>\n",
       "      <td>0.556221</td>\n",
       "      <td>232.45312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.492033</td>\n",
       "      <td>7.331616</td>\n",
       "      <td>271</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353197</td>\n",
       "      <td>0.269067</td>\n",
       "      <td>211.11229</td>\n",
       "      <td>0.909062</td>\n",
       "      <td>3.526690e-02</td>\n",
       "      <td>2.516839</td>\n",
       "      <td>1.749148</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>198.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.215084</td>\n",
       "      <td>6.621265</td>\n",
       "      <td>144</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129647</td>\n",
       "      <td>0.568070</td>\n",
       "      <td>0.244231</td>\n",
       "      <td>169.23441</td>\n",
       "      <td>0.978263</td>\n",
       "      <td>3.413020e-02</td>\n",
       "      <td>2.232579</td>\n",
       "      <td>1.548814</td>\n",
       "      <td>0.412455</td>\n",
       "      <td>151.45312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.103526</td>\n",
       "      <td>8.651907</td>\n",
       "      <td>1016</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100093</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>333.74268</td>\n",
       "      <td>1.014593</td>\n",
       "      <td>3.252610e-02</td>\n",
       "      <td>3.667873</td>\n",
       "      <td>2.044620</td>\n",
       "      <td>0.661501</td>\n",
       "      <td>294.04688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.172353</td>\n",
       "      <td>8.021215</td>\n",
       "      <td>657</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083367</td>\n",
       "      <td>0.891294</td>\n",
       "      <td>0.501066</td>\n",
       "      <td>290.32471</td>\n",
       "      <td>0.979809</td>\n",
       "      <td>7.912582e-02</td>\n",
       "      <td>3.234525</td>\n",
       "      <td>2.066361</td>\n",
       "      <td>0.909850</td>\n",
       "      <td>249.32812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.645954</td>\n",
       "      <td>6.838005</td>\n",
       "      <td>212</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206543</td>\n",
       "      <td>0.588854</td>\n",
       "      <td>0.426566</td>\n",
       "      <td>199.01100</td>\n",
       "      <td>0.944719</td>\n",
       "      <td>1.437892e-01</td>\n",
       "      <td>2.386840</td>\n",
       "      <td>1.241180</td>\n",
       "      <td>0.905079</td>\n",
       "      <td>173.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.116835</td>\n",
       "      <td>6.635355</td>\n",
       "      <td>138</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295517</td>\n",
       "      <td>0.144826</td>\n",
       "      <td>192.30621</td>\n",
       "      <td>0.831238</td>\n",
       "      <td>6.509933e-02</td>\n",
       "      <td>2.208827</td>\n",
       "      <td>1.801646</td>\n",
       "      <td>0.563573</td>\n",
       "      <td>175.92188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.440839</td>\n",
       "      <td>6.583960</td>\n",
       "      <td>152</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168579</td>\n",
       "      <td>0.657007</td>\n",
       "      <td>0.241993</td>\n",
       "      <td>178.11615</td>\n",
       "      <td>0.972610</td>\n",
       "      <td>4.332529e-02</td>\n",
       "      <td>2.602670</td>\n",
       "      <td>1.293131</td>\n",
       "      <td>0.541739</td>\n",
       "      <td>154.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>1.864984</td>\n",
       "      <td>6.930515</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094981</td>\n",
       "      <td>0.458418</td>\n",
       "      <td>0.199364</td>\n",
       "      <td>208.65715</td>\n",
       "      <td>0.879704</td>\n",
       "      <td>3.619837e-01</td>\n",
       "      <td>1.945283</td>\n",
       "      <td>1.427529</td>\n",
       "      <td>1.170381</td>\n",
       "      <td>189.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "      <td>3.897747</td>\n",
       "      <td>9.399097</td>\n",
       "      <td>2782</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109526</td>\n",
       "      <td>1.993336</td>\n",
       "      <td>0.759835</td>\n",
       "      <td>482.21530</td>\n",
       "      <td>1.011069</td>\n",
       "      <td>7.514427e-02</td>\n",
       "      <td>5.715179</td>\n",
       "      <td>1.642226</td>\n",
       "      <td>1.566671</td>\n",
       "      <td>405.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.111014</td>\n",
       "      <td>6.343670</td>\n",
       "      <td>111</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225422</td>\n",
       "      <td>0.621861</td>\n",
       "      <td>0.261949</td>\n",
       "      <td>173.44606</td>\n",
       "      <td>0.954850</td>\n",
       "      <td>3.826446e-01</td>\n",
       "      <td>1.689691</td>\n",
       "      <td>1.297074</td>\n",
       "      <td>1.045214</td>\n",
       "      <td>148.92188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.841363</td>\n",
       "      <td>7.057745</td>\n",
       "      <td>270</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155281</td>\n",
       "      <td>0.929101</td>\n",
       "      <td>0.328421</td>\n",
       "      <td>209.91570</td>\n",
       "      <td>1.005831</td>\n",
       "      <td>3.681983e-02</td>\n",
       "      <td>3.373866</td>\n",
       "      <td>1.161317</td>\n",
       "      <td>0.647394</td>\n",
       "      <td>180.14062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.342719</td>\n",
       "      <td>6.599185</td>\n",
       "      <td>147</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286153</td>\n",
       "      <td>0.235847</td>\n",
       "      <td>168.94328</td>\n",
       "      <td>0.919319</td>\n",
       "      <td>1.780057e-02</td>\n",
       "      <td>2.249843</td>\n",
       "      <td>1.410680</td>\n",
       "      <td>0.300171</td>\n",
       "      <td>153.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.763862</td>\n",
       "      <td>7.058914</td>\n",
       "      <td>261</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114256</td>\n",
       "      <td>0.454641</td>\n",
       "      <td>0.434381</td>\n",
       "      <td>192.14456</td>\n",
       "      <td>1.116867</td>\n",
       "      <td>9.320415e-03</td>\n",
       "      <td>2.907551</td>\n",
       "      <td>1.258343</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>158.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.069014</td>\n",
       "      <td>6.341292</td>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>0.358955</td>\n",
       "      <td>0.305420</td>\n",
       "      <td>143.44342</td>\n",
       "      <td>1.071982</td>\n",
       "      <td>3.090872e-02</td>\n",
       "      <td>1.855768</td>\n",
       "      <td>1.410430</td>\n",
       "      <td>0.326260</td>\n",
       "      <td>126.98438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.451120</td>\n",
       "      <td>6.860934</td>\n",
       "      <td>188</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007464</td>\n",
       "      <td>0.416032</td>\n",
       "      <td>0.256968</td>\n",
       "      <td>206.14227</td>\n",
       "      <td>1.002195</td>\n",
       "      <td>5.840104e-02</td>\n",
       "      <td>2.359899</td>\n",
       "      <td>1.745628</td>\n",
       "      <td>0.570301</td>\n",
       "      <td>182.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.011497</td>\n",
       "      <td>8.190375</td>\n",
       "      <td>688</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067148</td>\n",
       "      <td>0.753454</td>\n",
       "      <td>0.416760</td>\n",
       "      <td>296.70996</td>\n",
       "      <td>0.958390</td>\n",
       "      <td>1.208460e-01</td>\n",
       "      <td>3.455375</td>\n",
       "      <td>1.354939</td>\n",
       "      <td>1.201189</td>\n",
       "      <td>269.57812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.815238</td>\n",
       "      <td>8.074170</td>\n",
       "      <td>567</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.619382</td>\n",
       "      <td>0.310618</td>\n",
       "      <td>342.15201</td>\n",
       "      <td>0.854597</td>\n",
       "      <td>1.264804e-01</td>\n",
       "      <td>3.275252</td>\n",
       "      <td>1.926175</td>\n",
       "      <td>1.164813</td>\n",
       "      <td>307.12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.582369</td>\n",
       "      <td>6.846662</td>\n",
       "      <td>202</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232878</td>\n",
       "      <td>0.589891</td>\n",
       "      <td>0.413521</td>\n",
       "      <td>202.72173</td>\n",
       "      <td>0.956325</td>\n",
       "      <td>1.676246e-01</td>\n",
       "      <td>2.534937</td>\n",
       "      <td>1.143422</td>\n",
       "      <td>1.037853</td>\n",
       "      <td>171.70312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.112197</td>\n",
       "      <td>6.007506</td>\n",
       "      <td>86</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156427</td>\n",
       "      <td>0.662649</td>\n",
       "      <td>0.208066</td>\n",
       "      <td>162.03949</td>\n",
       "      <td>0.952368</td>\n",
       "      <td>8.961999e-02</td>\n",
       "      <td>2.179934</td>\n",
       "      <td>1.370603</td>\n",
       "      <td>0.652598</td>\n",
       "      <td>132.46875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.944840</td>\n",
       "      <td>7.274346</td>\n",
       "      <td>339</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194714</td>\n",
       "      <td>0.696591</td>\n",
       "      <td>0.384625</td>\n",
       "      <td>238.42363</td>\n",
       "      <td>0.928152</td>\n",
       "      <td>2.029905e-01</td>\n",
       "      <td>2.515293</td>\n",
       "      <td>1.463880</td>\n",
       "      <td>1.133252</td>\n",
       "      <td>207.14062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>3.917071</td>\n",
       "      <td>8.798213</td>\n",
       "      <td>1812</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007766</td>\n",
       "      <td>0.947079</td>\n",
       "      <td>0.445880</td>\n",
       "      <td>411.07907</td>\n",
       "      <td>0.897444</td>\n",
       "      <td>8.327732e-03</td>\n",
       "      <td>6.939130</td>\n",
       "      <td>1.223135</td>\n",
       "      <td>0.633240</td>\n",
       "      <td>361.54688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>14</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.572571</td>\n",
       "      <td>9.833193</td>\n",
       "      <td>3036</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176233</td>\n",
       "      <td>1.118741</td>\n",
       "      <td>1.305721</td>\n",
       "      <td>551.40271</td>\n",
       "      <td>1.183933</td>\n",
       "      <td>2.545903e-02</td>\n",
       "      <td>5.600791</td>\n",
       "      <td>2.461514</td>\n",
       "      <td>0.893656</td>\n",
       "      <td>500.76562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.095007</td>\n",
       "      <td>8.647535</td>\n",
       "      <td>995</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116173</td>\n",
       "      <td>1.124315</td>\n",
       "      <td>0.776725</td>\n",
       "      <td>316.15253</td>\n",
       "      <td>1.032016</td>\n",
       "      <td>8.584797e-02</td>\n",
       "      <td>3.791671</td>\n",
       "      <td>1.415277</td>\n",
       "      <td>1.110953</td>\n",
       "      <td>285.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3.513943</td>\n",
       "      <td>9.942601</td>\n",
       "      <td>3348</td>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129707</td>\n",
       "      <td>1.790756</td>\n",
       "      <td>0.861190</td>\n",
       "      <td>502.05197</td>\n",
       "      <td>0.982690</td>\n",
       "      <td>8.159853e-02</td>\n",
       "      <td>4.121910</td>\n",
       "      <td>2.853009</td>\n",
       "      <td>1.177442</td>\n",
       "      <td>461.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.147236</td>\n",
       "      <td>8.346904</td>\n",
       "      <td>844</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367192</td>\n",
       "      <td>1.331083</td>\n",
       "      <td>0.859917</td>\n",
       "      <td>262.07700</td>\n",
       "      <td>1.257038</td>\n",
       "      <td>6.147956e-03</td>\n",
       "      <td>3.362585</td>\n",
       "      <td>1.634960</td>\n",
       "      <td>0.263657</td>\n",
       "      <td>233.29688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.250569</td>\n",
       "      <td>8.330078</td>\n",
       "      <td>877</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400316</td>\n",
       "      <td>1.326322</td>\n",
       "      <td>0.815678</td>\n",
       "      <td>266.40536</td>\n",
       "      <td>1.259315</td>\n",
       "      <td>7.130000e-06</td>\n",
       "      <td>3.616703</td>\n",
       "      <td>1.547262</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>232.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4101</th>\n",
       "      <td>21</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11</td>\n",
       "      <td>4.157227</td>\n",
       "      <td>9.637900</td>\n",
       "      <td>3900</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085256</td>\n",
       "      <td>1.130733</td>\n",
       "      <td>1.060359</td>\n",
       "      <td>456.16165</td>\n",
       "      <td>0.971665</td>\n",
       "      <td>6.622763e-03</td>\n",
       "      <td>7.496752</td>\n",
       "      <td>1.327635</td>\n",
       "      <td>0.610088</td>\n",
       "      <td>428.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4102</th>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.331284</td>\n",
       "      <td>8.894869</td>\n",
       "      <td>1371</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188365</td>\n",
       "      <td>0.806092</td>\n",
       "      <td>0.830367</td>\n",
       "      <td>330.96170</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>1.110760e-01</td>\n",
       "      <td>3.432496</td>\n",
       "      <td>1.819937</td>\n",
       "      <td>1.143984</td>\n",
       "      <td>291.51562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4103</th>\n",
       "      <td>23</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>12</td>\n",
       "      <td>4.262002</td>\n",
       "      <td>10.603450</td>\n",
       "      <td>8184</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024747</td>\n",
       "      <td>2.060384</td>\n",
       "      <td>1.467118</td>\n",
       "      <td>601.42657</td>\n",
       "      <td>1.229640</td>\n",
       "      <td>5.250508e-02</td>\n",
       "      <td>6.629199</td>\n",
       "      <td>1.627233</td>\n",
       "      <td>1.519014</td>\n",
       "      <td>532.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104</th>\n",
       "      <td>15</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8</td>\n",
       "      <td>3.740112</td>\n",
       "      <td>10.213993</td>\n",
       "      <td>4577</td>\n",
       "      <td>75</td>\n",
       "      <td>24</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010017</td>\n",
       "      <td>1.345630</td>\n",
       "      <td>1.120728</td>\n",
       "      <td>514.10492</td>\n",
       "      <td>1.090148</td>\n",
       "      <td>6.605612e-02</td>\n",
       "      <td>4.762827</td>\n",
       "      <td>2.309567</td>\n",
       "      <td>1.224113</td>\n",
       "      <td>494.85938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4105</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.770297</td>\n",
       "      <td>7.288950</td>\n",
       "      <td>307</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262173</td>\n",
       "      <td>0.889421</td>\n",
       "      <td>0.429905</td>\n",
       "      <td>208.00719</td>\n",
       "      <td>1.010873</td>\n",
       "      <td>1.431517e-02</td>\n",
       "      <td>3.070164</td>\n",
       "      <td>1.358015</td>\n",
       "      <td>0.367333</td>\n",
       "      <td>188.15625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>27</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>14</td>\n",
       "      <td>4.519580</td>\n",
       "      <td>10.273845</td>\n",
       "      <td>7738</td>\n",
       "      <td>57</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084496</td>\n",
       "      <td>1.428641</td>\n",
       "      <td>1.316478</td>\n",
       "      <td>556.59680</td>\n",
       "      <td>1.034491</td>\n",
       "      <td>1.060336e-02</td>\n",
       "      <td>8.318093</td>\n",
       "      <td>1.720382</td>\n",
       "      <td>0.856536</td>\n",
       "      <td>507.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.317332</td>\n",
       "      <td>9.108146</td>\n",
       "      <td>1571</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218167</td>\n",
       "      <td>0.956963</td>\n",
       "      <td>0.639037</td>\n",
       "      <td>342.98721</td>\n",
       "      <td>1.033582</td>\n",
       "      <td>2.380225e-02</td>\n",
       "      <td>4.104442</td>\n",
       "      <td>1.591869</td>\n",
       "      <td>0.633232</td>\n",
       "      <td>337.07812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3.531124</td>\n",
       "      <td>9.095722</td>\n",
       "      <td>1779</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532105</td>\n",
       "      <td>1.608506</td>\n",
       "      <td>1.386807</td>\n",
       "      <td>320.65588</td>\n",
       "      <td>1.213541</td>\n",
       "      <td>9.902763e-02</td>\n",
       "      <td>4.091184</td>\n",
       "      <td>1.389502</td>\n",
       "      <td>1.287440</td>\n",
       "      <td>291.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.348015</td>\n",
       "      <td>8.309436</td>\n",
       "      <td>910</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421365</td>\n",
       "      <td>1.390010</td>\n",
       "      <td>0.903202</td>\n",
       "      <td>270.12653</td>\n",
       "      <td>1.221869</td>\n",
       "      <td>6.885325e-03</td>\n",
       "      <td>3.821694</td>\n",
       "      <td>1.421622</td>\n",
       "      <td>0.317116</td>\n",
       "      <td>239.20312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110</th>\n",
       "      <td>14</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.572571</td>\n",
       "      <td>9.833193</td>\n",
       "      <td>3036</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180281</td>\n",
       "      <td>1.139172</td>\n",
       "      <td>1.310828</td>\n",
       "      <td>541.62756</td>\n",
       "      <td>1.114891</td>\n",
       "      <td>2.544043e-02</td>\n",
       "      <td>5.543736</td>\n",
       "      <td>2.473597</td>\n",
       "      <td>0.884229</td>\n",
       "      <td>491.90625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4111</th>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.241239</td>\n",
       "      <td>8.893834</td>\n",
       "      <td>1275</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169308</td>\n",
       "      <td>1.227398</td>\n",
       "      <td>0.591225</td>\n",
       "      <td>348.46970</td>\n",
       "      <td>1.019203</td>\n",
       "      <td>1.191540e-02</td>\n",
       "      <td>4.416561</td>\n",
       "      <td>2.079754</td>\n",
       "      <td>0.482101</td>\n",
       "      <td>320.20312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4112</th>\n",
       "      <td>15</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.711226</td>\n",
       "      <td>9.987237</td>\n",
       "      <td>3686</td>\n",
       "      <td>64</td>\n",
       "      <td>29</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266219</td>\n",
       "      <td>1.401460</td>\n",
       "      <td>1.386047</td>\n",
       "      <td>546.06934</td>\n",
       "      <td>1.134397</td>\n",
       "      <td>2.673489e-02</td>\n",
       "      <td>5.672372</td>\n",
       "      <td>2.450294</td>\n",
       "      <td>0.927479</td>\n",
       "      <td>492.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4113</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.122931</td>\n",
       "      <td>9.486019</td>\n",
       "      <td>1884</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030554</td>\n",
       "      <td>0.709709</td>\n",
       "      <td>0.667835</td>\n",
       "      <td>409.82919</td>\n",
       "      <td>1.059405</td>\n",
       "      <td>1.791581e-01</td>\n",
       "      <td>3.307741</td>\n",
       "      <td>2.168465</td>\n",
       "      <td>1.400070</td>\n",
       "      <td>398.67188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>19</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "      <td>3.997597</td>\n",
       "      <td>9.662234</td>\n",
       "      <td>3608</td>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210099</td>\n",
       "      <td>0.932413</td>\n",
       "      <td>1.130682</td>\n",
       "      <td>433.08319</td>\n",
       "      <td>1.083598</td>\n",
       "      <td>6.768618e-03</td>\n",
       "      <td>5.722529</td>\n",
       "      <td>1.606175</td>\n",
       "      <td>0.470802</td>\n",
       "      <td>389.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>21</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11</td>\n",
       "      <td>4.157227</td>\n",
       "      <td>9.637900</td>\n",
       "      <td>3900</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082141</td>\n",
       "      <td>1.187952</td>\n",
       "      <td>1.076048</td>\n",
       "      <td>464.61697</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>5.649854e-03</td>\n",
       "      <td>7.222216</td>\n",
       "      <td>1.378745</td>\n",
       "      <td>0.542862</td>\n",
       "      <td>418.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4116</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.171076</td>\n",
       "      <td>8.013471</td>\n",
       "      <td>660</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366382</td>\n",
       "      <td>0.874268</td>\n",
       "      <td>1.064281</td>\n",
       "      <td>244.04774</td>\n",
       "      <td>1.227272</td>\n",
       "      <td>6.340000e-06</td>\n",
       "      <td>3.454345</td>\n",
       "      <td>1.443414</td>\n",
       "      <td>0.008697</td>\n",
       "      <td>217.26562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4117</th>\n",
       "      <td>8</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.754961</td>\n",
       "      <td>8.386939</td>\n",
       "      <td>672</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582185</td>\n",
       "      <td>0.896220</td>\n",
       "      <td>0.860799</td>\n",
       "      <td>235.88287</td>\n",
       "      <td>1.233438</td>\n",
       "      <td>4.017317e-01</td>\n",
       "      <td>2.048023</td>\n",
       "      <td>1.441196</td>\n",
       "      <td>1.298084</td>\n",
       "      <td>222.32812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4118</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.181245</td>\n",
       "      <td>8.497430</td>\n",
       "      <td>942</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008991</td>\n",
       "      <td>0.679030</td>\n",
       "      <td>0.715456</td>\n",
       "      <td>314.67664</td>\n",
       "      <td>1.239662</td>\n",
       "      <td>4.729595e-02</td>\n",
       "      <td>3.307461</td>\n",
       "      <td>1.906384</td>\n",
       "      <td>0.719295</td>\n",
       "      <td>288.98438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.209038</td>\n",
       "      <td>8.890231</td>\n",
       "      <td>1247</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257858</td>\n",
       "      <td>0.859641</td>\n",
       "      <td>0.909330</td>\n",
       "      <td>298.13803</td>\n",
       "      <td>1.109501</td>\n",
       "      <td>4.643737e-03</td>\n",
       "      <td>3.930724</td>\n",
       "      <td>1.736371</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>286.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>17</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "      <td>3.842909</td>\n",
       "      <td>9.671086</td>\n",
       "      <td>3214</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>1.398831</td>\n",
       "      <td>1.131169</td>\n",
       "      <td>384.93948</td>\n",
       "      <td>1.148611</td>\n",
       "      <td>3.880000e-06</td>\n",
       "      <td>5.520238</td>\n",
       "      <td>1.709398</td>\n",
       "      <td>0.010869</td>\n",
       "      <td>372.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4121</th>\n",
       "      <td>17</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "      <td>3.748159</td>\n",
       "      <td>9.684606</td>\n",
       "      <td>3050</td>\n",
       "      <td>61</td>\n",
       "      <td>18</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238527</td>\n",
       "      <td>1.413845</td>\n",
       "      <td>1.406159</td>\n",
       "      <td>394.95737</td>\n",
       "      <td>1.121818</td>\n",
       "      <td>5.049912e-03</td>\n",
       "      <td>5.409522</td>\n",
       "      <td>1.520780</td>\n",
       "      <td>0.384415</td>\n",
       "      <td>372.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.724753</td>\n",
       "      <td>6.251799</td>\n",
       "      <td>151</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522886</td>\n",
       "      <td>0.836453</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>156.30632</td>\n",
       "      <td>1.254802</td>\n",
       "      <td>5.599208e-02</td>\n",
       "      <td>2.657349</td>\n",
       "      <td>0.901326</td>\n",
       "      <td>0.628799</td>\n",
       "      <td>116.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.221335</td>\n",
       "      <td>9.023081</td>\n",
       "      <td>1395</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240370</td>\n",
       "      <td>1.443687</td>\n",
       "      <td>0.850684</td>\n",
       "      <td>332.70416</td>\n",
       "      <td>1.135980</td>\n",
       "      <td>1.501128e-02</td>\n",
       "      <td>4.017947</td>\n",
       "      <td>2.221267</td>\n",
       "      <td>0.492281</td>\n",
       "      <td>298.68750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4124</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.181245</td>\n",
       "      <td>8.497430</td>\n",
       "      <td>942</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060387</td>\n",
       "      <td>0.690701</td>\n",
       "      <td>0.792299</td>\n",
       "      <td>320.21283</td>\n",
       "      <td>1.340913</td>\n",
       "      <td>6.735969e-02</td>\n",
       "      <td>3.063100</td>\n",
       "      <td>2.290363</td>\n",
       "      <td>0.794989</td>\n",
       "      <td>290.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>3.166868</td>\n",
       "      <td>8.890940</td>\n",
       "      <td>1206</td>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089743</td>\n",
       "      <td>0.602089</td>\n",
       "      <td>0.671911</td>\n",
       "      <td>293.99820</td>\n",
       "      <td>0.997095</td>\n",
       "      <td>1.220000e-08</td>\n",
       "      <td>4.262545</td>\n",
       "      <td>1.627098</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>309.23438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4126 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      diameter  petitjean  petitjeanSC  radius   VDistEq    VDistMa  \\\n",
       "0            5   0.400000     0.666667       3  2.037476   6.011166   \n",
       "1            9   0.444444     0.800000       5  2.954872   8.805204   \n",
       "2           10   0.500000     1.000000       5  3.083532   8.211762   \n",
       "3            7   0.428571     0.750000       4  2.616827   7.313269   \n",
       "4            7   0.428571     0.750000       4  2.609669   6.833154   \n",
       "5            7   0.428571     0.750000       4  2.575820   7.324709   \n",
       "6            6   0.500000     1.000000       3  2.473536   6.864490   \n",
       "7            6   0.500000     1.000000       3  2.331947   6.311081   \n",
       "8            9   0.444444     0.800000       5  2.964607   7.678609   \n",
       "9            7   0.428571     0.750000       4  2.492033   7.331616   \n",
       "10           5   0.400000     0.666667       3  2.215084   6.621265   \n",
       "11          10   0.500000     1.000000       5  3.103526   8.651907   \n",
       "12          11   0.454545     0.833333       6  3.172353   8.021215   \n",
       "13           7   0.428571     0.750000       4  2.645954   6.838005   \n",
       "14           5   0.400000     0.666667       3  2.116835   6.635355   \n",
       "15           7   0.428571     0.750000       4  2.440839   6.583960   \n",
       "16           4   0.250000     0.333333       3  1.864984   6.930515   \n",
       "17          17   0.470588     0.888889       9  3.897747   9.399097   \n",
       "18           5   0.400000     0.666667       3  2.111014   6.343670   \n",
       "19           9   0.444444     0.800000       5  2.841363   7.057745   \n",
       "20           6   0.333333     0.500000       4  2.342719   6.599185   \n",
       "21           8   0.500000     1.000000       4  2.763862   7.058914   \n",
       "22           5   0.400000     0.666667       3  2.069014   6.341292   \n",
       "23           7   0.428571     0.750000       4  2.451120   6.860934   \n",
       "24           9   0.444444     0.800000       5  3.011497   8.190375   \n",
       "25           8   0.500000     1.000000       4  2.815238   8.074170   \n",
       "26           7   0.428571     0.750000       4  2.582369   6.846662   \n",
       "27           5   0.400000     0.666667       3  2.112197   6.007506   \n",
       "28           9   0.444444     0.800000       5  2.944840   7.274346   \n",
       "29          19   0.473684     0.900000      10  3.917071   8.798213   \n",
       "...        ...        ...          ...     ...       ...        ...   \n",
       "4096        14   0.928571    13.000000       1  3.572571   9.833193   \n",
       "4097        10   0.500000     1.000000       5  3.095007   8.647535   \n",
       "4098        14   0.500000     1.000000       7  3.513943   9.942601   \n",
       "4099        10   0.500000     1.000000       5  3.147236   8.346904   \n",
       "4100        11   0.454545     0.833333       6  3.250569   8.330078   \n",
       "4101        21   0.476190     0.909091      11  4.157227   9.637900   \n",
       "4102        12   0.500000     1.000000       6  3.331284   8.894869   \n",
       "4103        23   0.478261     0.916667      12  4.262002  10.603450   \n",
       "4104        15   0.466667     0.875000       8  3.740112  10.213993   \n",
       "4105         8   0.500000     1.000000       4  2.770297   7.288950   \n",
       "4106        27   0.481481     0.928571      14  4.519580  10.273845   \n",
       "4107        11   0.454545     0.833333       6  3.317332   9.108146   \n",
       "4108        14   0.500000     1.000000       7  3.531124   9.095722   \n",
       "4109        12   0.500000     1.000000       6  3.348015   8.309436   \n",
       "4110        14   0.928571    13.000000       1  3.572571   9.833193   \n",
       "4111        12   0.500000     1.000000       6  3.241239   8.893834   \n",
       "4112        15   0.933333    14.000000       1  3.711226   9.987237   \n",
       "4113        10   0.500000     1.000000       5  3.122931   9.486019   \n",
       "4114        19   0.473684     0.900000      10  3.997597   9.662234   \n",
       "4115        21   0.476190     0.909091      11  4.157227   9.637900   \n",
       "4116        11   0.454545     0.833333       6  3.171076   8.013471   \n",
       "4117         8   0.375000     0.600000       5  2.754961   8.386939   \n",
       "4118        11   0.454545     0.833333       6  3.181245   8.497430   \n",
       "4119        11   0.454545     0.833333       6  3.209038   8.890231   \n",
       "4120        17   0.470588     0.888889       9  3.842909   9.671086   \n",
       "4121        17   0.470588     0.888889       9  3.748159   9.684606   \n",
       "4122         7   0.428571     0.750000       4  2.724753   6.251799   \n",
       "4123        11   0.454545     0.833333       6  3.221335   9.023081   \n",
       "4124        11   0.454545     0.833333       6  3.181245   8.497430   \n",
       "4125        11   0.454545     0.833333       6  3.166868   8.890940   \n",
       "\n",
       "      weinerPath  weinerPol  a_aro  a_count    ...        FASA_P    FCASA.  \\\n",
       "0             82          9      6       15    ...      0.129128  0.326050   \n",
       "1           1046         40     11       31    ...      0.059026  0.847237   \n",
       "2            742         24     12       35    ...      0.046707  0.817581   \n",
       "3            288         20     10       20    ...      0.044688  0.417896   \n",
       "4            203         13      6       22    ...      0.103925  0.595412   \n",
       "5            284         21      6       26    ...      0.088718  0.641968   \n",
       "6            197         15      6       22    ...      0.163559  0.432359   \n",
       "7            120         10      5       18    ...      0.150887  0.674236   \n",
       "8            472         21     12       32    ...      0.038050  0.724254   \n",
       "9            271         22     12       26    ...      0.000000  0.353197   \n",
       "10           144         14      6       19    ...      0.129647  0.568070   \n",
       "11          1016         32     14       40    ...      0.100093  1.137040   \n",
       "12           657         23     11       34    ...      0.083367  0.891294   \n",
       "13           212         13      6       24    ...      0.206543  0.588854   \n",
       "14           138         16      6       25    ...      0.000000  0.295517   \n",
       "15           152         13      6       21    ...      0.168579  0.657007   \n",
       "16           156         23      0       30    ...      0.094981  0.458418   \n",
       "17          2782         38      6       60    ...      0.109526  1.993336   \n",
       "18           111         13      0       24    ...      0.225422  0.621861   \n",
       "19           270         16      6       24    ...      0.155281  0.929101   \n",
       "20           147         13      6       18    ...      0.000000  0.286153   \n",
       "21           261         17      6       20    ...      0.114256  0.454641   \n",
       "22           108         12      6       15    ...      0.242591  0.358955   \n",
       "23           188         16      6       23    ...      0.007464  0.416032   \n",
       "24           688         31      6       41    ...      0.067148  0.753454   \n",
       "25           567         30      6       44    ...      0.004437  0.619382   \n",
       "26           202         15      6       24    ...      0.232878  0.589891   \n",
       "27            86          9      5       19    ...      0.156427  0.662649   \n",
       "28           339         16      6       30    ...      0.194714  0.696591   \n",
       "29          1812         29     12       52    ...      0.007766  0.947079   \n",
       "...          ...        ...    ...      ...    ...           ...       ...   \n",
       "4096        3036         60     29       61    ...      0.176233  1.118741   \n",
       "4097         995         40      6       36    ...      0.116173  1.124315   \n",
       "4098        3348         49     18       61    ...      0.129707  1.790756   \n",
       "4099         844         30      6       27    ...      0.367192  1.331083   \n",
       "4100         877         29      6       27    ...      0.400316  1.326322   \n",
       "4101        3900         45     24       52    ...      0.085256  1.130733   \n",
       "4102        1371         36     12       35    ...      0.188365  0.806092   \n",
       "4103        8184         69     12       68    ...      0.024747  2.060384   \n",
       "4104        4577         75     24       62    ...      0.010017  1.345630   \n",
       "4105         307         17      6       24    ...      0.262173  0.889421   \n",
       "4106        7738         57     30       64    ...      0.084496  1.428641   \n",
       "4107        1571         51      0       46    ...      0.218167  0.956963   \n",
       "4108        1779         41      6       36    ...      0.532105  1.608506   \n",
       "4109         910         29      6       28    ...      0.421365  1.390010   \n",
       "4110        3036         60     29       61    ...      0.180281  1.139172   \n",
       "4111        1275         41      0       42    ...      0.169308  1.227398   \n",
       "4112        3686         64     29       63    ...      0.266219  1.401460   \n",
       "4113        1884         56     16       50    ...      0.030554  0.709709   \n",
       "4114        3608         49     18       46    ...      0.210099  0.932413   \n",
       "4115        3900         45     24       54    ...      0.082141  1.187952   \n",
       "4116         660         25      6       25    ...      0.366382  0.874268   \n",
       "4117         672         39      0       30    ...      0.582185  0.896220   \n",
       "4118         942         30      6       31    ...      0.008991  0.679030   \n",
       "4119        1247         44     12       34    ...      0.257858  0.859641   \n",
       "4120        3214         57      0       44    ...      0.385997  1.398831   \n",
       "4121        3050         61     18       42    ...      0.238527  1.413845   \n",
       "4122         151          7      0       16    ...      0.522886  0.836453   \n",
       "4123        1395         45      6       38    ...      0.240370  1.443687   \n",
       "4124         942         30     18       33    ...      0.060387  0.690701   \n",
       "4125        1206         47     20       36    ...      0.089743  0.602089   \n",
       "\n",
       "      FCASA..1        VSA      dens          glob  std_dim1  std_dim2  \\\n",
       "0     0.224950  138.13699  0.958985  3.103191e-02  1.853123  1.382682   \n",
       "1     1.136764  296.30197  1.296254  2.584471e-01  2.394159  1.966377   \n",
       "2     0.464419  296.54431  0.946322  1.621386e-01  3.164745  1.552043   \n",
       "3     0.574827  187.43799  1.195242  1.857262e-02  2.524826  1.446899   \n",
       "4     0.267278  193.42802  0.978233  8.815648e-02  2.550456  1.430062   \n",
       "5     0.320366  218.67284  0.974160  4.273635e-02  2.843004  1.581263   \n",
       "6     0.357734  188.12050  0.968377  1.907373e-02  2.784485  1.359217   \n",
       "7     0.253037  166.62454  1.025273  2.526089e-02  2.633108  1.334763   \n",
       "8     0.428256  261.96707  0.913285  3.104502e-02  3.156831  1.637290   \n",
       "9     0.269067  211.11229  0.909062  3.526690e-02  2.516839  1.749148   \n",
       "10    0.244231  169.23441  0.978263  3.413020e-02  2.232579  1.548814   \n",
       "11    0.543016  333.74268  1.014593  3.252610e-02  3.667873  2.044620   \n",
       "12    0.501066  290.32471  0.979809  7.912582e-02  3.234525  2.066361   \n",
       "13    0.426566  199.01100  0.944719  1.437892e-01  2.386840  1.241180   \n",
       "14    0.144826  192.30621  0.831238  6.509933e-02  2.208827  1.801646   \n",
       "15    0.241993  178.11615  0.972610  4.332529e-02  2.602670  1.293131   \n",
       "16    0.199364  208.65715  0.879704  3.619837e-01  1.945283  1.427529   \n",
       "17    0.759835  482.21530  1.011069  7.514427e-02  5.715179  1.642226   \n",
       "18    0.261949  173.44606  0.954850  3.826446e-01  1.689691  1.297074   \n",
       "19    0.328421  209.91570  1.005831  3.681983e-02  3.373866  1.161317   \n",
       "20    0.235847  168.94328  0.919319  1.780057e-02  2.249843  1.410680   \n",
       "21    0.434381  192.14456  1.116867  9.320415e-03  2.907551  1.258343   \n",
       "22    0.305420  143.44342  1.071982  3.090872e-02  1.855768  1.410430   \n",
       "23    0.256968  206.14227  1.002195  5.840104e-02  2.359899  1.745628   \n",
       "24    0.416760  296.70996  0.958390  1.208460e-01  3.455375  1.354939   \n",
       "25    0.310618  342.15201  0.854597  1.264804e-01  3.275252  1.926175   \n",
       "26    0.413521  202.72173  0.956325  1.676246e-01  2.534937  1.143422   \n",
       "27    0.208066  162.03949  0.952368  8.961999e-02  2.179934  1.370603   \n",
       "28    0.384625  238.42363  0.928152  2.029905e-01  2.515293  1.463880   \n",
       "29    0.445880  411.07907  0.897444  8.327732e-03  6.939130  1.223135   \n",
       "...        ...        ...       ...           ...       ...       ...   \n",
       "4096  1.305721  551.40271  1.183933  2.545903e-02  5.600791  2.461514   \n",
       "4097  0.776725  316.15253  1.032016  8.584797e-02  3.791671  1.415277   \n",
       "4098  0.861190  502.05197  0.982690  8.159853e-02  4.121910  2.853009   \n",
       "4099  0.859917  262.07700  1.257038  6.147956e-03  3.362585  1.634960   \n",
       "4100  0.815678  266.40536  1.259315  7.130000e-06  3.616703  1.547262   \n",
       "4101  1.060359  456.16165  0.971665  6.622763e-03  7.496752  1.327635   \n",
       "4102  0.830367  330.96170  1.088463  1.110760e-01  3.432496  1.819937   \n",
       "4103  1.467118  601.42657  1.229640  5.250508e-02  6.629199  1.627233   \n",
       "4104  1.120728  514.10492  1.090148  6.605612e-02  4.762827  2.309567   \n",
       "4105  0.429905  208.00719  1.010873  1.431517e-02  3.070164  1.358015   \n",
       "4106  1.316478  556.59680  1.034491  1.060336e-02  8.318093  1.720382   \n",
       "4107  0.639037  342.98721  1.033582  2.380225e-02  4.104442  1.591869   \n",
       "4108  1.386807  320.65588  1.213541  9.902763e-02  4.091184  1.389502   \n",
       "4109  0.903202  270.12653  1.221869  6.885325e-03  3.821694  1.421622   \n",
       "4110  1.310828  541.62756  1.114891  2.544043e-02  5.543736  2.473597   \n",
       "4111  0.591225  348.46970  1.019203  1.191540e-02  4.416561  2.079754   \n",
       "4112  1.386047  546.06934  1.134397  2.673489e-02  5.672372  2.450294   \n",
       "4113  0.667835  409.82919  1.059405  1.791581e-01  3.307741  2.168465   \n",
       "4114  1.130682  433.08319  1.083598  6.768618e-03  5.722529  1.606175   \n",
       "4115  1.076048  464.61697  0.999990  5.649854e-03  7.222216  1.378745   \n",
       "4116  1.064281  244.04774  1.227272  6.340000e-06  3.454345  1.443414   \n",
       "4117  0.860799  235.88287  1.233438  4.017317e-01  2.048023  1.441196   \n",
       "4118  0.715456  314.67664  1.239662  4.729595e-02  3.307461  1.906384   \n",
       "4119  0.909330  298.13803  1.109501  4.643737e-03  3.930724  1.736371   \n",
       "4120  1.131169  384.93948  1.148611  3.880000e-06  5.520238  1.709398   \n",
       "4121  1.406159  394.95737  1.121818  5.049912e-03  5.409522  1.520780   \n",
       "4122  0.693000  156.30632  1.254802  5.599208e-02  2.657349  0.901326   \n",
       "4123  0.850684  332.70416  1.135980  1.501128e-02  4.017947  2.221267   \n",
       "4124  0.792299  320.21283  1.340913  6.735969e-02  3.063100  2.290363   \n",
       "4125  0.671911  293.99820  0.997095  1.220000e-08  4.262545  1.627098   \n",
       "\n",
       "      std_dim3        vol  \n",
       "0     0.326444  123.18750  \n",
       "1     1.217135  262.82812  \n",
       "2     1.274330  266.62500  \n",
       "3     0.344087  164.95312  \n",
       "4     0.757260  165.79688  \n",
       "5     0.587728  193.21875  \n",
       "6     0.384559  167.48438  \n",
       "7     0.418498  136.68750  \n",
       "8     0.556221  232.45312  \n",
       "9     0.472649  198.28125  \n",
       "10    0.412455  151.45312  \n",
       "11    0.661501  294.04688  \n",
       "12    0.909850  249.32812  \n",
       "13    0.905079  173.81250  \n",
       "14    0.563573  175.92188  \n",
       "15    0.541739  154.40625  \n",
       "16    1.170381  189.00000  \n",
       "17    1.566671  405.00000  \n",
       "18    1.045214  148.92188  \n",
       "19    0.647394  180.14062  \n",
       "20    0.300171  153.56250  \n",
       "21    0.280702  158.62500  \n",
       "22    0.326260  126.98438  \n",
       "23    0.570301  182.25000  \n",
       "24    1.201189  269.57812  \n",
       "25    1.164813  307.12500  \n",
       "26    1.037853  171.70312  \n",
       "27    0.652598  132.46875  \n",
       "28    1.133252  207.14062  \n",
       "29    0.633240  361.54688  \n",
       "...        ...        ...  \n",
       "4096  0.893656  500.76562  \n",
       "4097  1.110953  285.18750  \n",
       "4098  1.177442  461.53125  \n",
       "4099  0.263657  233.29688  \n",
       "4100  0.009657  232.87500  \n",
       "4101  0.610088  428.62500  \n",
       "4102  1.143984  291.51562  \n",
       "4103  1.519014  532.40625  \n",
       "4104  1.224113  494.85938  \n",
       "4105  0.367333  188.15625  \n",
       "4106  0.856536  507.09375  \n",
       "4107  0.633232  337.07812  \n",
       "4108  1.287440  291.93750  \n",
       "4109  0.317116  239.20312  \n",
       "4110  0.884229  491.90625  \n",
       "4111  0.482101  320.20312  \n",
       "4112  0.927479  492.75000  \n",
       "4113  1.400070  398.67188  \n",
       "4114  0.470802  389.81250  \n",
       "4115  0.542862  418.50000  \n",
       "4116  0.008697  217.26562  \n",
       "4117  1.298084  222.32812  \n",
       "4118  0.719295  288.98438  \n",
       "4119  0.267859  286.87500  \n",
       "4120  0.010869  372.93750  \n",
       "4121  0.384415  372.93750  \n",
       "4122  0.628799  116.43750  \n",
       "4123  0.492281  298.68750  \n",
       "4124  0.794989  290.25000  \n",
       "4125  0.000471  309.23438  \n",
       "\n",
       "[4126 rows x 202 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize the observations\n",
    "y_scaler=StandardScaler()\n",
    "y_scaler.fit(y_train_df)\n",
    "y_train_norm=y_scaler.transform(y_train_df)\n",
    "y_test_norm=y_scaler.transform(y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#minmax standardization\n",
    "y_minmax=MinMaxScaler()\n",
    "y_minmax.fit(y_train_df)\n",
    "y_train_mm=y_minmax.transform(y_train_df)\n",
    "y_test_mm=y_minmax.transform(y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAFyCAYAAAA0x5qKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xm4HVWVsPF3JYQgg8yCyCAqKA4gCSCDA+0AKC22tB8Q\nFVFAwdZWA91gt90i0DQODQh8ojggohBFbfmwGSIgigNDkzAPgmBkDkM0TAkQsr4/qq6eHO5YqXvP\nvsn7e5567qld+9RZZ+c81GLvXbUjM5EkSSrVhF4HIEmSNBiTFUmSVDSTFUmSVDSTFUmSVDSTFUmS\nVDSTFUmSVDSTFUmSVDSTFUmSVLQVeh1ASSJibWBXYA6wsLfRSJI0rqwEvBiYmZmPtHlik5Ul7Qqc\n2esgJEkax94HnNXmCU1WljQH4Hvf+x5bbLFFj0NZfkyfPp0TTjih12EsV2zzsWebjz3bfGzdcsst\nvP/974f6Wtomk5UlLQTYYostmDJlSq9jWW6svvrqtvcYs83Hnm0+9mzznml9GoUTbCVJUtFMViRJ\nUtFMViRJUtFMVtRz06ZN63UIyx3bfOzZ5mPPNl92RGb2OoZiRMQUYNasWbOclCVJ0gjMnj2bqVOn\nAkzNzNltntueFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDST\nFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmSVDSTFUmS\nVDSTFUmSVDSTFUmSVLQikpWIeENEnBsR90bE4ojYYxjv2TkiZkXEwoi4LSL266fO/4mIWyJiQURc\nFxFvH51vIEmSRksRyQqwCnAt8A9ADlU5Il4M/A9wCbAVcCLwzYh4W0edHYGzgG8ArwX+H3BORLyy\n5dglSdIoWqHXAQBk5oXAhQAREcN4y0eBOzPzsHr/dxHxemA6cFFd9gnggsw8vt7/bJ3MfJwqKZIk\nSeNAKT0rI7U9cHFX2Uxgh479HYZRR5IkFW68JivrA3O7yuYCz4+IyUPUWX+UY5MkSS0qYhhIg1u0\nCBYsgCefrP52vn7qqep4G9vixZBZ/e18PdK/Q9XJjllJ2TVDaWn22zzXcPYljQ/nnQerrNLrKLQ0\nxmuy8gCwXlfZesCjmfnUEHUeGOrk06dPZ/XVV1+ibNq0aUybNq1ZtLUFC+Duu+Gee+CRR2DevMG3\nxx+vkpJFi5bqY5kwAVZYYfBt4sSq3oQJEPHc18P9O1SdiOqzOnXPUlqa/TbPNZx9SeWbMF7HEAo2\nY8YMZsyYsUTZ/PnzR+3zIgv738WIWAz8XWaeO0idzwNvz8ytOsrOAtbIzHfU+98HnpeZ7+qo8xvg\nuszsd4JtREwBZs2aNYspU6Y0/g6PPw5XXQXXXw833AA33gh33gkPP7xkvQkTYM01Ya21nrutuSas\nthqsvDI873kD/508GSZNem7i0Z2ESJI0mmbPns3UqVMBpmbm7DbPXUTPSkSsArwM6Pv/1pdExFbA\nvMy8OyKOBTbIzL5nqXwN+FhEfAE4DXgL8B7gHR2nPRH4RUQcApwHTAOmAh8eje+weDHMnAmnnQb/\n8z+wcCGstBK88pXwmtfAO98JG20EG28MG24I664Lz3++iYQkSUMpIlkBtgEupXrGSgLH1eXfAfan\nmhS7UV/lzJwTEbsDJ1DdonwPcEBmXtxR5/KIeC9wTL3dDrwrM29uM/BFi+B734NjjoHf/x623BKO\nPBL+9m9h882r3g1JktRcEZfSzPwlg9yZlJkf6qfsMqqeksHO+2Pgx0sd4ABuvBH22gtuuQXe/W74\nzndghx2c1yBJUpuKSFbGo0sugT33hBe/GP73f2GbbXodkSRJyyZnTDRwwQXw9rdXvSi/+pWJiiRJ\no8melRH6xS+qHpXddoMf/7i6E0eSJI0ee1ZG4KmnqjkqO+4IZ59toiJJ0lgwWRmB886Dhx6CE0+s\nbkuWJEmjz2RlBL7znWp+yqtf3etIJElafpisDNPcuVXPygc/2OtIJElavpisDNNZZ1VPm91nn15H\nIknS8sVkZZi+//3qqbRrr93rSCRJWr6YrAzDI49UD3575zt7HYkkScufRslKRKwQEW+NiIMiYrW6\nbIOIWLXd8Mpw8cWQCbvs0utIJEla/oz4oXARsQlwIbAxMBm4CHgMOLzeP7jNAEvws5/Bq14FL3pR\nryORJGn506Rn5UTgamBNYEFH+U+At7QRVEkyYeZMe1UkSeqVJo/bfwOwY2Y+HUsuLzwHWOb6Hm65\nBe69F3bdtdeRSJK0fGrSszIBmNhP+YZUw0HLlJ/9DCZPhje8odeRSJK0fGqSrPwM+FTHftYTa48E\nzm8lqoJcdlm1uvLKK/c6EkmSlk9NkpVDgZ0i4mZgJeAs/joEdHh7oZVh9uzqEfuSJKk3RjxnJTPv\niYitgL2BrYBVgW8BZ2bmgkHfPM488gj88Y8wZUqvI5EkafnVZIItmbkIOLPellnXXFP9NVmRJKl3\nRjwMFBH/EhEf6qd8/4hYpoaBZs+GVVeFzTbrdSSSJC2/msxZOQi4uZ/ym1jGHgh3zTXw2tdWCxhK\nkqTeaHIZXh94sJ/yh4AXLl04ZZk92yEgSZJ6rUmycjewUz/lOwH3LV045Xj0UbjtNpMVSZJ6rckE\n228AX46IScDP67K3AF8EjmsrsF677rrq79Zb9zYOSZKWd02SlS8BawOnACvWZQuBL2TmsW0F1mvX\nXls9uXaLLXodiSRJy7cmz1lJ4PCIOBrYgmoxw9sz86m2g+ul++6rVlmeNKnXkUiStHxr9JwVgMx8\nHPjfFmMpyrx5sNZavY5CkiSNOFmJiFWAT1PNU3kBXZN0M/Ml7YTWWyYrkiSVoUnPyjeBNwHfBe4H\nstWICvHII7Deer2OQpIkNUlW3g7snpm/aTuYksyb5+RaSZJK0OQ5K38C5rUdSGkcBpIkqQxNkpV/\nB46KiJXbDqYkJiuSJJWhyTDQocBLgbkRMQd4pvNgZo77Z74+/TQ88YTJiiRJJWiSrJzTehSFefTR\n6q/JiiRJvdfkoXBHjkYgJTFZkSSpHE3mrCzz5s+v/pqsSJLUe00eCjcRmA7sBWzMX9cHAiAzx/0l\nvq9nZe21exuHJElq1rNyBHAI8ANgdeB44L+BxcDnWoush/p6VtZcs7dxSJKkZsnK+4APZ+ZxwCJg\nRmYeCBwFbN9mcL0yfz6stpqLGEqSVIImycr6wA3168epelcA/gfYvY2geu3RR52vIklSKZokK/cA\nL6xf3wHsUr/eFniqjaB6bf58kxVJkkrRJFn5CdWKywAnA0dHxO3AGcBpbQXWS/asSJJUjibPWfl0\nx+sfRMRdwA7A7Zn50zaD65X582GTTXodhSRJgmZPsF1CZl4OXN5CLMWwZ0WSpHIMK1mJiD2ACzLz\nmfr1gDLz3FYi6yHnrEiSVI7h9qycQ3UX0IMMvjZQAhOXNqhes2dFkqRyDCtZycwJ/b1eVrnisiRJ\n5RhR4hERkyLikojYbLQCKoWP2pckqQwjSlYy8xlgy1GKpSj2rEiSVIYmQzrfAw5oO5DSmKxIklSG\nJrcurwDsHxFvBWYBT3QezMxD2gis10xWJEkqQ5Nk5dXA7Pr15l3HcunCKccqq/Q6AkmSBM2eYPs3\noxFIaSaO+xuwJUlaNizztyE3ZbIiSVIZGj1uPyK2AfYCNgZW7DyWmXu2EFfPTTCNkySpCCO+JEfE\nPsBvgS2AdwOTgFcBbwbmtxpdD5msSJJUhiaX5H8FpmfmO4GngU8CrwDOBu5qMbaeMlmRJKkMTS7J\nLwXOq18/DaySmQmcAHykrcB6LaLXEUiSJGiWrPwJWK1+fS/VrcwAawArNw0kIj4WEX+IiAURcUVE\nbDuM+jdHxJMRcUtE7Nt1fIWI+GxE/L4+5zURsevw42n6TSRJUpuaJCuXAW+rX/8QODEivgHMAC5p\nEkRE7A0cBxwBbA1cB8yMiHUGqP9R4Bjgs8Argc8BX4mI3TuqHQN8GPgY1fyaU4GfRMRWQ8XjnUCS\nJJWjSbLyceD79etjgOOB9YAf0/wx/NOBUzPzjMy8FTgYeBLYf4D676/r/ygz52TmD4CvA4d31Tkm\nM2fWdb4GnA8cOlQw9qpIklSOJg+Fm9fxejHw+aUJICImAVOB/+w4b0bExcAOA7xtMrCwq2whsF1E\nTMzMZ+s6T3XVWQC8fqiYnFwrSVI5mty6fHFEfDAint9SDOsAE4G5XeVzgfUHeM9M4MCImFLHtA1V\nr86k+nx9dQ6JiJdF5W3AnsALhwrIZEWSpHI0uSzfBBwLPBARP4yId9W9I2PpaOAC4PKIeAb4CXB6\nfWxx/feTwO3ArVQ9LCcBp3UcH5DDQJIklaPJMNAnI2I68FbgvcAZwLMR8SPgzMz85QhP+TDwLNW8\nl07rAQ8MEMNCqp6Vg+p69wMHAY9l5kN1nYeBPSNiRWDtzLw/Ij4P3DlUQE8/PZ099lh9ibJp06Yx\nbdq0EX0xSZKWRTNmzGDGjBlLlM2fP3rPhY3qESlLcYKIlYB3Ap8BXpOZI76XJiKuAK7MzE/W+0H1\ngLmTMvNLwzzHL4C7M3PfAY5PAm4Gvp+Z/z5AnSnArNVWm8Wjj04Z6deQJGm5NXv2bKZOnQowNTNn\nt3nuRmsD9YmI9YF9qO682RK4quGpjgdOj4hZ9TmmUz2z5fT6c44FNsjM/er9zYDtgCuBtYBDqB75\n/4GO2LYDXgRcC2xIdVt0AEMmP85ZkSSpHCNOVuqJtX9PNQS0M9WwypnA3pl5R5MgMvPs+pkqR1EN\n61wL7No3pEM10XajjrdMpLoFeXPgGeBSYMfM7Hzc/0rAfwCbAo9TPXX3/Zn56NDfscm3kCRJo6FJ\nz8pcqqfY/gD4l8y8uo1AMvMU4JQBjn2oa/9WYNBxmsy8jKq3ZcTsWZEkqRxNkpU9gEvqZ6wsk0xW\nJEkqR5O7gS4ajUBKYrIiSVI5vCz3w2RFkqRyeFnuhxNsJUkqh8lKP+xZkSSpHF6W+2HPiiRJ5RjW\nBNuI+MRwT5iZJzUPpwwTR/wMXkmSNFqGezfQ9K79dameMPvnen8N4EngQaoFA8c1e1YkSSrHsIaB\nMnPTvo1qDaBrgS0yc63MXAvYApgN9LvmznjjnBVJksrR5LJ8NPCPmfm7voL69XSqx9uPeyYrkiSV\no8ll+YX0P3w0kWpdn3HPYSBJksrRJFm5BDg1Iv6yNk9ETAW+ClzcVmC95ARbSZLK0SRZ2R94ALg6\nIp6KiKeAq6gWODywzeB6xZ4VSZLK0WRtoIeAd0TE5sAr6uJbM/O2ViPrIeesSJJUjiarLveZAwRw\nR2YuaiecMpisSJJUjhFfliNi5Yj4FtVzVW4CNq7LT46IT7ccX084DCRJUjma9CEcC2wF7Aws7Ci/\nGNi7hZh6zp4VSZLK0WQY6O+AvTPziojIjvKbgJe2E1ZvmaxIklSOJpfldakeq99tFSD7KR93TFYk\nSSpHk8vy1cDuHft9CcqBwOVLHVEBnLMiSVI5mgwD/StwQUS8sn7/J+vXOwJvajO4XrFnRZKkcoz4\nspyZvwZeS5Wo3ADsQjUstENmzmo3vN4wWZEkqRyNnrOSmXcAH245lmKYrEiSVI5GyUpETABeBryA\nrt6ZzLyshbh6ymRFkqRyjDhZiYjtgbOATaieYNspqVZfHtecYCtJUjma9Kx8jb/eEXQ/y8jtyp3s\nWZEkqRxNkpXNgPdk5u/bDqYU9qxIklSOJn0IV1LNV1lmTRz3A1mSJC07mvSsnAwcFxHrU926/Ezn\nwcy8vo3AesmeFUmSytEkWflx/fe0jrKkmmy7TEywdc6KJEnlaJKsbNp6FIUxWZEkqRwjTlYy84+j\nEUhJHAaSJKkcw0pWImIP4ILMfKZ+PaDMPLeVyHrInhVJksox3J6Vc4D1qdYAOmeQes5ZkSRJrRpW\nspKZE/p7vawyWZEkqRxelvthsiJJUjmaLmS4CvAmYGNgxc5jmXlSC3H1lBNsJUkqR5OFDLcGzgdW\nBlYB5gHrAE9SzWkZ98mKPSuSJJWjyWX5BOCnwJrAAmB7qhWYZwH/1F5ovWOyIklSOZpcll8LHJeZ\ni4FngcmZeTdwGPCfbQbXKyYrkiSVo8ll+Rlgcf36Qap5KwDzgY3aCKrXnLMiSVI5mkywvQbYFrgd\n+CVwVESsA+wL3NhibD1jz4okSeVocln+V+D++vVngD8BXwXWBT7SUlw9ZbIiSVI5mqwNdHXH6weB\n3VqNqAAOA0mSVA77EPoxcdwvGCBJ0rJjuAsZXkO17s+QMnPKUkVUAHtWJEkqx0gWMlxuOGdFkqRy\nDHchwyNHO5CS2LMiSVI5Gq0NBBAR2wBb1Ls3Z+asdkLqPXtWJEkqR5O1gTYEZgA7AX+ui9eIiN8C\n+2TmPS3G1xMmK5IklaPJZfmbwCRgi8xcKzPXouphmVAfG/dMViRJKkeTYaA3ATtm5u/6CjLzdxHx\nj8CvWoush0xWJEkqR5PL8t1UPSvdJgL3LV04ZXCCrSRJ5WiSrPwzcHI9wRb4y2TbE4F/aiuwXrJn\nRZKkcjQZBjodWBm4MiIWdZxnEXBaRJzWV7GezzLumKxIklSOJsnKp1qPojAmK5IklaPJQobfGY1A\nSmKyIklSOUZ8WY6IDw5QvkJEHLvUERXACbaSJJWjSR/CSRHxw4hYs68gIl4OXAlMaxpIRHwsIv4Q\nEQsi4oqI2HYY9W+OiCcj4paI2LefOp+KiFvrOndFxPERMXmoWOxZkSSpHE0uy1sDGwI3RMTbIuJj\nwGzgVmCrJkFExN7AccAR9fmvA2ZGxDoD1P8ocAzwWeCVwOeAr0TE7h113gscW5/zFcD+wF71+4aI\np8m3kCRJo6HJnJU7ImIn4MvAhcCzwH6ZOWMp4pgOnJqZZwBExMHA7lQJxhf7qf/+uv6P6v05dU/M\n4cB5ddkOwK8z8wf1/l0R8X1gu6GCmTix8feQJEktazrgsTuwD3A51fpAB0TEBk1OFBGTgKnAJX1l\nmZnAxVQJR38mAwu7yhYC20VEX6rxW2Bq33BSRLwEeAd/TWYGiWkk30CSJI2mJhNsTwV+CHwBeAOw\nJfA01bDQXg1iWIfq6bdzu8rnAusP8J6ZwIERMaWOaRvgAKon664DUPf0HAH8OiKeBm4HLs3MLwwV\nkHNWJEkqR5PL8k7A6zLzuKw8kJnvoJo/ctoQ723L0cAFwOUR8QzwE6qH1QEsBoiInYF/BQ6mmgez\nJ/C3EfFvQ53cZEWSpHI0eSjc1Mx8qrswM78SERc3ON/DVPNe1usqXw94oL83ZOZCqp6Vg+p69wMH\nAY9l5kN1taOA72bmt+v9myJiVeBU4D8GC+jMM6dz1VWrL1E2bdo0pk1rfLOTJEnLjBkzZjBjxpJT\nVefPnz9qn9dkgu1TEbECsDPwUuCszHysnrNyb4PzPRMRs4C3AOcCRETU+ycN8d5nqRdPjIh9gJ92\nHF6ZagmATn29LlHPi+nXfvudwKc/PWWE30SSpOVDf/8DP3v2bKZOnToqnzfiZCUiNqG6C2hjqomu\nFwGPUd2JM5lq2GWkjgdOr5OWq6juDlqZeminftjcBpm5X72/GdVdPVcCawGHAK8CPtBxzp8C0yPi\nurreZlS9LecOlqhU52/wDSRJ0qhoMgx0InA11TNVHuko/wnwjSZBZObZ9TNVjqIa1rkW2LVjSGd9\nYKOOt0wEDgU2B54BLgV2zMy7OuocTdWTcjTwIuAhqp4b56xIkjSONElW3kCVGDwdS3ZBzKFKChrJ\nzFOAUwY49qGu/VuBQcdpMrMvUTl6pLHYsyJJUjma9CFMoOrZ6LYh1XDQuGfPiiRJ5WhyWf4Z8KmO\n/azvsjkSOL+VqHrMZEWSpHI0GQY6lGrdnpuBlYCzqCavPsxSLGRYEpMVSZLK0eTW5XsiYitgb6pJ\ntqsC3wLOzMwFLcfXEyYrkiSVo0nPCpm5CDiz3pY5TrCVJKkc9iH0w54VSZLK4WW5H/asSJJUDpOV\nfkzs78ZsSZLUEyYr/bBnRZKkcjRKViJijYg4MCKOjYi16rIpEdH4CbYlcc6KJEnlaLKQ4ZbAxcB8\n4MVU6wHNA/akWtzwAwO+eZwwWZEkqRxNLsvHA6dn5mbAwo7y84E3thJVjzkMJElSOZokK9sCp/ZT\nfi/V6sjjnhNsJUkqR5Nk5Sng+f2Ubw48tHThlMGeFUmSytEkWTkX+GxETKr3MyI2Br4A/Li1yHrI\nOSuSJJWjyWX5UKr1gB4Engf8Evg98BjwmfZC6x2TFUmSytFkIcP5wNsi4vXAllSJy+zMvLjt4HrF\nYSBJksrR5NbljTLz7sz8NfDrUYip5+xZkSSpHE0uy3Mi4pcR8eGIWLP1iApgsiJJUjmaXJa3Aa4C\nPgvcHxHnRMR7ImJyu6H1jsmKJEnlGPFlOTOvycx/pnpa7dupblf+OjA3Ik5rOb6ecM6KJEnlaNyH\nkJVLM/PDwFuBPwD7tRZZD9mzIklSORpfliNiw4g4LCKupRoWehz4WGuR9ZDJiiRJ5WhyN9BBwHuB\nnYBbgTOBd2XmH1uOrWdMViRJKseIkxXg34AZwCcy87qW4ymCyYokSeVokqxsnJnZeiQFcYKtJEnl\nGFayEhFbAjdm5mLgNTHI1Twzr28ptp6xZ0WSpHIMt2flWmB9qvWArgUS6MxY+vYTmNhmgL1gz4ok\nSeUYbrKyKdXzVPpeL9PsWZEkqRzDSla67vTZBPhtZi7qrBMRKwA7AuP+riCTFUmSytHksnwpsFY/\n5avXx8Y9kxVJksrR5LLcNzel29rAE0sXThlMViRJKsewb12OiP+uXyZwekQ81XF4IrAl8NsWY+sZ\nJ9hKklSOkTxnZX79N4DHgAUdx54GrgC+0VJcPWXPiiRJ5Rh2spKZHwKIiDnAf2XmMjHk0x+TFUmS\nyjHiJ9hm5pGjEUhJTFYkSSpHk8ftExHvAfYCNgZW7DyWmVNaiKunnLMiSVI5RtyHEBGfAL4NzAW2\nBq4CHgFeAlzQanQ9YrIiSVI5mgx4/APwkcz8R6qJtV/MzLcBJ1E9a0WSJKk1TZKVjfnrLcoLgNXq\n198FprURlCRJUp8mycoD/PUJtncB29evN2XJxQ0lSZKWWpNk5efAHvXrbwMnRMRFwA+An7QVmCRJ\nEjS7G+gj1ElOZn4lIh6hWsDwXODUFmOTJElq9JyVxcDijv3vA99vMyhJkqQ+w0pWImLL4Z4wM69v\nHo4kSdKShtuzci3VAoZDTaBNqkUNJUmSWjHcZGXTUY1CkiRpAMNKVjLzj6MdiCRJUn9GPME2Ij4w\n2PHMPKN5OJIkSUtqcuvyiV37k4CVqR69/yRgsiJJklrT5NblNbvLImIz4KvAl9oISpIkqU+TJ9g+\nR2beDnya5/a6SJIkLZVWkpXaImCDFs8nSZLUaILtHt1FwAuBjwO/aSMoSZKkPk0m2J7TtZ/AQ1QL\nHB661BFJkiR1aDLBts2hI0mSpEGZeEiSpKI1mbMSwHuAvwFeQFfCk5l7thOaJElSs56VLwPfpVov\n6HFgftfWSER8LCL+EBELIuKKiNh2GPVvjognI+KWiNi36/ilEbG4n+2nTWOUJEljr8kE232BPTPz\n/LaCiIi9geOAjwBXAdOBmRGxeWY+3E/9jwLHAAcCVwOvA74REfMy87y62ruBFTvetg5wHXB2W3FL\nkqTR16RnZT5wZ8txTAdOzcwzMvNW4GCqR/fvP0D999f1f5SZczLzB8DXgcP7KmTmnzPzwb4N2AV4\nAvhRy7FLkqRR1CRZ+RxwREQ8r40AImISMBW4pK8sMxO4GNhhgLdNBhZ2lS0EtouIiQO8Z39gRmYu\nWLqIJUnSWGqSrJwNrAk8GBE3RMTszq3B+dYBJgJzu8rnAusP8J6ZwIERMQUgIrYBDqBaVHGd7soR\nsR3wKuCbDeKTJEk91GTOyneoekK+R5VQZKsRDc/RwHrA5RExAXgAOB04DFjcT/0DgBsyc9aYRShJ\nklrRJFnZHdg1M3/dUgwPA89SJR+d1qNKQp4jMxdS9awcVNe7HzgIeCwzH+qsGxErA3sD/zbcgKZP\nn87qq6++RNm0adOYNm3acE8hSdIya8aMGcyYMWOJsvnzG98QPKSopoeM4A0RtwJ7Zeb1rQURcQVw\nZWZ+st4P4C7gpMz80jDP8Qvg7szsvoX5g8ApwIsy809DnGMKMGvWrFlMmTJlxN9DkqTl1ezZs5k6\ndSrA1MxsMi1kQE3mrBwKfDEiXtxiHMcDH46ID0TEK4CvAStTDe0QEcdGxHf6KkfEZhHxvoh4WURs\nFxHfp5qT8pl+zn0AcM5QiYokSSpTk2Gg71ElEndExJPAM50HM3OtkZ4wM8+OiHWAo6iGda6lGmrq\nG9JZH9io4y0TqZKmzevPvxTYMTPv6jxvRGwO7Ai8baQxSZKkMjRJVj7VehRAZp5CNVzT37EPde3f\nCgw5TpOZt1ElNpIkaZxqsuryd4auJUmS1I4mCxluPNjx7qEYSZKkpdFkGGgOgz9bxWEXSZLUmibJ\nytZd+5PqskPo/24cSZKkxprMWbmun+KrI+I+4J+B/17qqCRJkmpNnrMykN8B27Z4PkmSpEYTbJ/f\nXQS8kGo15ttbiEmSJOkvmsxZ+TPPnWAbwN3APksdkSRJUocmycqbWTJZWQw8BPw+Mxe1EpUkSVKt\nyQTbX4xCHJIkSf0a8QTbiPiXiPhQP+X7R8Th7YQlSZJUaXI30EHAzf2U3wQcvHThSJIkLalJsrI+\n8GA/5Q9R3RUkSZLUmibJyt3ATv2U7wTct3ThSJIkLanJ3UDfAL4cEZOAn9dlbwG+CBzXVmCSJEnQ\nLFn5ErA2cAqwYl22EPhCZh7bVmCSJEnQ7NblBA6PiKOBLYAFwO2Z+VTbwUmSJDXpWQEgMx8H/rfF\nWCRJkp6jzYUMJUmSWmeyIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimay\nIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmS\nimayIkkOhdqhAAANx0lEQVSSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimay\nIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSimayIkmS\nimayIkmSimayIkmSimayIkmSimayIkmSimayIkmSilZMshIRH4uIP0TEgoi4IiK2HUb9myPiyYi4\nJSL27afO6hHxlYi4LyIWRsStEbHb6H0LNTFjxoxeh7Dcsc3Hnm0+9mzzZUcRyUpE7A0cBxwBbA1c\nB8yMiHUGqP9R4Bjgs8Argc8BX4mI3TvqTAIuBjYG9gQ2Bz4M3DtqX0SN+B+UsWebjz3bfOzZ5suO\nFXodQG06cGpmngEQEQcDuwP7A1/sp/776/o/qvfn1D0xhwPn1WUHAGsA22fms3XZXaMUvyRJGiU9\n71mpe0CmApf0lWVmUvWK7DDA2yYDC7vKFgLbRcTEev+dwOXAKRHxQETcEBH/EhE9/86SJGn4Srhw\nrwNMBOZ2lc8F1h/gPTOBAyNiCkBEbEPVkzKpPh/AS4D/Q/Ud3w4cBRwKfKbN4CVJ0ugqZRhopI4G\n1gMur3tKHgBOBw4DFtd1JlAlPB+pe2quiYgNgX+q39+flQAOPPBAVltttSUO7Lrrruy2m3NzR8P8\n+fOZPXt2r8NYrtjmY882H3u2+ei58MILmTlz5hJljz32WN/Lldr+vKiu471TDwM9Cfx9Zp7bUX46\nsHpmvnuQ906kSlruBw4CPp+Za9THfgE8nZm7dNTfjWpOy+TMXNTP+d4LnNnC15IkaXn1vsw8q80T\n9rxnJTOfiYhZwFuAcwEiIur9k4Z477PAffV79gF+2nH4N8C0rre8HLi/v0SlNhN4HzCH586JkSRJ\nA1sJeDHVtbRVPe9ZAYiIvaiGcQ4GrqK6O+g9wCsy86GIOBbYIDP3q+tvBmwHXAmsBRxCldxMzcy7\n6jobAjcCZwAnU926/C3gy5n5+bH7dpIkaWn0vGcFIDPPrp+pchTVsM61wK6Z+VBdZX1go463TKSa\nLLs58AxwKbBjX6JSn/OeiNgVOIHquS331q/7uxVakiQVqoieFUmSpIGUcOuyJEnSgExWJElS0UxW\naiNdSFHDFxFHRMTiru3mrjpH1QtOPhkRF0XEy3oV73gUEW+IiHMj4t66fffop86gbRwRk+uFPx+O\niMci4kcR8YKx+xbjy1BtHhHf7ud3f35XHdt8mOonkF8VEY9GxNyI+ElEbN5PPX/nLRlOm4/V79xk\nhZEvpKhGbqSaPL1+vb2+70BEHA58HPgI1V1eT1C1/4o9iHO8WoVqYvo/AM+ZiDbMNv4y1Zpcfw+8\nEdgA+PHohj2uDdrmtQtY8nff/TgF23z43kB1Z+frgLdSPbH8ZxHxvL4K/s5bN2Sb10b/d56Zy/0G\nXAGc2LEfwD3AYb2ObVnYqJLA2YMcvw+Y3rH/fGABsFevYx+PG9VTnPcYSRvX+08B7+6o8/L6XNv1\n+juVvg3Q5t8G/nuQ99jmS9fm69Rt9fqOMn/nY9/mY/I7X+57VhoupKiR26zuLr8jIr4XERsBRMSm\nVJl4Z/s/SvUMHdu/BcNs422oHmXQWed3VCuV++/Q3M519/mtEXFKRKzVcWwqtvnSWIOqR2se+Dsf\nI0u0eYdR/50v98kKzRZS1MhcAXwQ2JXqwX+bApdFxCpUbZzY/qNpOG28HtXyFI8OUkcjcwHwAeDN\nVOuWvQk4v35CN1Ttaps3ULfhl4FfZ2bf/Dd/56NogDaHMfqdF/FQOC3bMrPz0cs3RsRVwB+BvYBb\nexOVNLoy8+yO3Zsi4gbgDmBnqgdZqrlTgFcCO/U6kOVIv20+Vr9ze1bgYeBZqoy703pUqzmrZZk5\nH7gNeBlVGwe2/2gaThs/AKwYEc8fpI6WQmb+geq/N313p9jmDUTE/wXeAeycmfd3HPJ3PkoGafPn\nGK3f+XKfrGTmM0DfQorAEgsp/rZXcS3LImJVqh/yffUP+wGWbP/nU80+t/1bMMw2ngUs6qrzcmBj\n4PIxC3YZFtV6ZWtTrRIPtvmI1RfNdwF/kx3Lq4C/89EyWJsPUH90fue9nl1cwkY1HPEk1bjbK4BT\ngUeAdXsd27KwAV+iul1tE2BH4CKq8cq16+OH1e39TuA1wDnA7cCKvY59vGxUt9FuBbyWapb9p+r9\njYbbxlTdvH+g6r6dSrVy+a96/d1K3QZr8/rYF6kulJvU/6G+GrgFmGSbN2rvU4A/Ud1Ou17HtlJH\nHX/nY9jmY/k773ljlLJRPSthDtVtbpcD2/Q6pmVlA2ZQ3Qq+gGoG+FnApl11Pkd12+GTVMuLv6zX\ncY+njWpS22KqIc3O7bThtjEwmeqZCg8DjwE/BF7Q6+9W6jZYmwMrARdS/Z/+QuBO4Kt0/Q+QbT6i\n9u6vrZ8FPtBVz9/5GLX5WP7OXchQkiQVbbmfsyJJkspmsiJJkopmsiJJkopmsiJJkopmsiJJkopm\nsiJJkopmsiJJkopmsiJJkopmsiItJyLi0og4vtdxdIqIr0fEIxHxbERs2et4JJXJZEVST0TEblTr\ncb0DeCFwY28jGp8iYr+I+FOv45BG0wq9DkDS+BURE4DMZut2vAy4PzOvbDms5U0ArpuiZZo9K9IY\nqodiToyIL9TDH/dHxBEdxzeJiMWdQyIRsXpd9sZ6/031/i4RMTsinoyIiyNi3Yh4e0TcHBHzI+LM\niFipK4QVIuLkiPhzRDwUEUd1xbdiRPxXRNwTEY9HxOUR8aaO4/tFxJ8i4p0RcRPV4mUbDfBd3xQR\nV0bEwoi4LyKOrZMbIuLbwEnAxvV3uXOQNtupbrcnImJeRFwQEat3xHtSRMyNiAUR8auI2KYrhhG3\nVf15Jw/RVmtExBl1TE9ExPkR8bJ+2mqX+nMeq2Nfr+s8B9bHF9R/P9pxrO/38O6I+Hn9OddGxPZ9\n349q4cS+38izEfHZ+tg/RMRt9XkfiIizB2pjqXi9XtXRzW152oBLqZZc/3fgpcC+VKuYvqU+vkm9\nv2XHe1anWv30jfV+32q/vwG2B7YCbqvPfQGwJbAT8BDwz12f/ShwPLAZMA14HDigo843gF8BOwKb\nAodQrV770vr4fsBTdZ3t6/Os1M/33KA+90nA5sAewIPAZ+vjqwH/BvwRWBdYe4D2ei3Vat0nA68B\nXg4cDKxVHz8RuBvYBXgF8G3gEWCNFtpq/hBt9f+ohq52rGO7oD73xK62mglsXX+Xm4DvdpzjfVQr\nkr+r/rf/uzqWfTt+D4vr9+1G1Rt1NtXqthOAScAnqH5T6wIvAFYGpgLPAHtRJZNbAR/v9e/fza3p\n1vMA3NyWp62+CP6yq+xK4D/r130Xp6GSlWeBnTvqHF6XbdJR9lXg/K7PvrHrs4/tKwM2ri9w63fV\nuQj4j/r1fvXnvHqI73kMcHNX2UeB+R37nwTuHOI8ZwKXDXBs5ToZ2LujbIX64n/oKLfVZvW/yes6\njq8FPAH8fVdbvbirDe7r2L+9M/667DPAb7p+Dx/sOL5Ffd7NOz5nXtc53l0nMKv0+jfv5tbG5jCQ\nNPau79q/n+r/iEfqho7Xc4EnM/OPXWXd572ia/9yYLOICODVwETgtnrI4rGIeAx4I1UvUJ+nM3Oo\nybCvqM/d6TfAqhGx4RDv7fRa4JIBjr2UKjn5bV9BZi4CrqK6oHdqu622oErsrur47HnA77o++8nM\nnNOx/5d/64hYuf4O3+pq789Q9WoNFP/9VPNUBvvNXETVa/WHeqjqvRHxvEHqS0Vzgq009p7p2k/+\nOn9scf03Oo5PGsZ5cojzDseqwCJgSkccfR7veL1gBOdcWm19VtttNdwJrf19Tt+/7ar13wPpSHpq\nzw5ynr7PHjDezHw8IqYAO1MNkR0JfC4itsnMR4cXulQOe1aksjxU/31hR9nWtHe3x+u69ncAbs/M\nBK6h6llZLzPv7NoeHOHn3FKfu9Prgccy854RnOd64C0DHLuD6iK+U19BRKwAbEs1x2NpDdZWt1D9\nz95f6kTE2lRzaob12XWb3kc1H6i7vTt7fYb6t3+a6t+t+/yLM/PnmflpqjkrLwbePJzYpNLYsyIV\nJDMXRsQVwKcjYg6wHnB0P1Wjn7Lh2Dgi/gv4OtUkzI8D0+vPvj0izgLOiIh/okpeXkB1gbsuMy8Y\nweecAnwyIk4G/i/VsNDngONGGO+xwPUR8RXga1TJyc7A2Zk5LyK+CnwpqueM3A0cBjyP6g6ZPqPR\nVr+PiHOBb0TEwVQ9T5+vYzh3BJ9xBHBiRDwKXAhMBrahmiD85WHGP4dqeO3NwHVUE6LfDLwEuIxq\n7sru9Xl+N4LYpGKYrEhjazg9JPsD3wSuprq4HAb8rMF5+vvsM6gu5ldRDfmckJnf7KjzQaq7dP4L\neBHwMNXcjZ+O6IMy74uIdwBfAq4F5lHdaXTMCM9ze0TsAvwn1UTkBfXfs+oqn6a6CJ9BdYfR1cAu\nmTm/8zQj+cwOw2mrE6naZkXgl8Dumdk9hDOgzPxWRDxB9W/8RaoJujcAX+6s1t9bO85xeUR8DfgB\n1STfI4GLgT2pkqGVqCby7pOZtww3NqkkUfVoSpL6RMSlwDWZeUivY5HknBVJklQ4kxVJei67nKWC\nOAwkSZKKZs+KJEkqmsmKJEkqmsmKJEkqmsmKJEkqmsmKJEkqmsmKJEkqmsmKJEkqmsmKJEkqmsmK\nJEkq2v8H0MqzfFM5/L0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d736f51da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#feature selection\n",
    "pca=PCA().fit(x_train_df)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4126, 30), (275, 30))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca=PCA(n_components=30)#from the literature\n",
    "x_train_pca=pca.fit_transform(x_train_df)\n",
    "x_test_pca=pca.fit_transform(x_test_df)\n",
    "x_train_pca.shape, x_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4126, 30), (275, 30))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize the inputs#IS IT REQUIRED TO STANDARDIZE THE TEST DATA\n",
    "x_scaler=StandardScaler()\n",
    "x_scaler.fit(x_train_pca)\n",
    "x_train_norm=x_scaler.transform(x_train_pca)\n",
    "x_test_norm=x_scaler.transform(x_test_pca)\n",
    "x_train_norm.shape, x_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.79656844,  1.10500481,  0.7621479 , ..., -0.12627875,\n",
       "         0.19964714, -0.01756664],\n",
       "       [-0.27984721, -0.13401114, -0.51342307, ...,  0.9004492 ,\n",
       "        -1.37064293,  0.22871855],\n",
       "       [-0.44399513,  0.31496621,  0.136344  , ..., -0.52441648,\n",
       "         0.26350161, -0.34125211],\n",
       "       ..., \n",
       "       [-0.06254811, -0.2936878 , -0.47412742, ...,  1.17670195,\n",
       "        -0.75664626,  1.01886171],\n",
       "       [ 0.19001601,  0.57383777,  0.35290286, ..., -0.6053861 ,\n",
       "         0.93915108,  0.78001961],\n",
       "       [-0.14497032,  0.0988562 ,  0.47812248, ...,  0.95683985,\n",
       "        -0.8903534 ,  1.07504762]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#minmax standardization\n",
    "x_minmax=MinMaxScaler()\n",
    "x_minmax.fit(x_train_pca)\n",
    "x_train_mm=x_minmax.transform(x_train_pca)\n",
    "x_test_mm=x_minmax.transform(x_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for mm: -0.447275263612\n",
      "mse for mm : 0.0305866767772\n",
      "rmse for mm: 0.17489047080167908\n",
      "mae for mm: 0.1403903644\n"
     ]
    }
   ],
   "source": [
    "#model building-linear regression using MINMAX STANDARDIZATION\n",
    "regr=linear_model.LinearRegression()\n",
    "regr.fit(x_train_mm, y_train_mm)\n",
    "y_predict_mm=regr.predict(x_test_mm)\n",
    "#normalize the y_predict\n",
    "#y_predict_mm=y_minmax.fit_transform(y_predict)\n",
    "#evaluate the model\n",
    "#r2_linear_mm=r2_score(y_predict_mm, y_test_mm)\n",
    "r2_linear_mm=r2_score(y_test_mm, y_predict_mm)\n",
    "print('r2 for mm:',r2_linear_mm)\n",
    "mse_linear_mm=mean_squared_error(y_test_mm, y_predict_mm)\n",
    "print('mse for mm :',mse_linear_mm)\n",
    "rmse_mm=sqrt(mse_linear_mm)\n",
    "print('rmse for mm:', rmse_mm)\n",
    "mae_mm=mean_absolute_error(y_test_mm, y_predict_mm)\n",
    "print('mae for mm:', mae_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_minmax.inverse_transform(y_predict_mm).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: -6.97255932586e-32\n"
     ]
    }
   ],
   "source": [
    "#model building-linear regression WITH CROSS_VAL_SCORE \n",
    "regr=linear_model.LinearRegression()\n",
    "regr.fit(x_train_mm, y_train_mm)\n",
    "y_predict_mm=regr.predict(x_test_mm)\n",
    "#normalize the y_predict\n",
    "#y_predict_mm=y_minmax.fit_transform(y_predict)\n",
    "#evaluate the model\n",
    "kfold=KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "results=cross_val_score(regr, x_test_mm, y_predict_mm, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print('mse:', results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: -0.447275263612\n",
      "mse : 1.04947202107\n",
      "rmse: 1.0244374168619181\n",
      "mae: 0.822349791839\n"
     ]
    }
   ],
   "source": [
    "#model building-linear regression\n",
    "regr=linear_model.LinearRegression()\n",
    "regr.fit(x_train_norm, y_train_norm)\n",
    "y_predict_norm=regr.predict(x_test_norm)\n",
    "#normalize the y_predict\n",
    "#y_predict_norm=y_scaler.fit_transform(y_predict)\n",
    "#evaluate the model\n",
    "#r2_linear=r2_score(y_predict_norm, y_test_norm)\n",
    "r2_linear=r2_score(y_test_norm, y_predict_norm)\n",
    "print('r2:',r2_linear)\n",
    "mse_linear=mean_squared_error(y_test_norm, y_predict_norm)\n",
    "print('mse :',mse_linear)\n",
    "rmse=sqrt(mse_linear)\n",
    "print('rmse:', rmse)\n",
    "mae=mean_absolute_error(y_test_norm, y_predict_norm)\n",
    "print('mae:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for linear in true space: -0.447275263612\n",
      "mse for linear in true space: 4381.91613513\n",
      "rmse for linear in true space: 66.19604319843555\n",
      "mae for linear in true space: 53.1377529255\n"
     ]
    }
   ],
   "source": [
    "y_predict_true=y_scaler.inverse_transform(y_predict_norm)\n",
    "#mse in the true space\n",
    "r2_linear=r2_score(y_test_df, y_predict_true)\n",
    "print('r2 for linear in true space:', r2_linear)\n",
    "mse_linear=mean_squared_error(y_test_df, y_predict_true)\n",
    "print('mse for linear in true space:', mse_linear)\n",
    "rmse_linear=sqrt(mse_linear)\n",
    "print('rmse for linear in true space:', rmse_linear)\n",
    "mae_linear=mean_absolute_error(y_test_df, y_predict_true)\n",
    "print('mae for linear in true space:', mae_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22941181626537951,\n",
       " 0.99999999999999989,\n",
       " 0.62091159502693005,\n",
       " 0.99999999999999956,\n",
       " 0.7251364321996312)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(y_predict_norm), np.var(x_train_norm), np.var(x_test_norm), np.var(y_train_norm), np.var(y_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36718672681892006"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(x_train_norm, y_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for lasso: -0.0862748261999\n",
      "rmse for lasso 57.34906194844204\n",
      "mae for lasso 46.1790544177\n"
     ]
    }
   ],
   "source": [
    "#Lasso\n",
    "regr_lasso=linear_model.Lasso(alpha=0.2)\n",
    "regr_lasso.fit(x_train_norm, y_train_norm)\n",
    "y_predict_lasso=regr_lasso.predict(x_test_norm)\n",
    "#scale back to true space\n",
    "y_predict_lasso_true=y_scaler.inverse_transform(y_predict_lasso)\n",
    "#evaluate\n",
    "mse_lasso=mean_squared_error(y_test_df, y_predict_lasso_true)\n",
    "rmse_lasso=sqrt(mse_lasso)\n",
    "mae_lasso=mean_absolute_error(y_test_df, y_predict_lasso_true)\n",
    "r2_lasso=r2_score(y_test_df, y_predict_lasso_true)\n",
    "print('r2 for lasso:',r2_lasso)\n",
    "print('rmse for lasso', rmse_lasso)\n",
    "print('mae for lasso', mae_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042529454625352403"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_lasso.score(x_train_norm, y_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1082: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for lassoCV: -0.445781491512\n",
      "rmse for lassoCV 66.16187301229786\n",
      "mae for lassoCV 53.1085770381\n"
     ]
    }
   ],
   "source": [
    "#LassoCV\n",
    "model_CV=LassoCV(cv=20).fit(x_train_norm, y_train_norm)\n",
    "y_predict_CV = model_CV.predict(x_test_norm)\n",
    "#scale back to true space\n",
    "y_predict_CV_true=y_scaler.inverse_transform(y_predict_CV)\n",
    "#evaluate\n",
    "mse_lassoCV=mean_squared_error(y_test_df, y_predict_CV_true)\n",
    "rmse_lassoCV=sqrt(mse_lassoCV)\n",
    "r2_lassoCV=r2_score(y_test_df, y_predict_CV_true)\n",
    "mae_lassoCV=mean_absolute_error(y_test_df, y_predict_CV_true)\n",
    "print('r2 for lassoCV:',r2_lassoCV)\n",
    "print('rmse for lassoCV',rmse_lassoCV )\n",
    "print('mae for lassoCV', mae_lassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse for lassCV: 66.19604319843555\n",
      "r2 for lasso larsCV: -0.447275263612\n",
      "mae for larsCV: 53.1377529255\n"
     ]
    }
   ],
   "source": [
    "#LassoLarsCV\n",
    "model_larsCV=LassoLarsCV(cv=20).fit(x_train_norm, y_train_norm)\n",
    "y_predict_larsCV=model_larsCV.predict(x_test_norm)\n",
    "#scale back to true space\n",
    "y_predict_larsCV_true=y_scaler.inverse_transform(y_predict_larsCV)\n",
    "#evaluate\n",
    "r2_larsCV=r2_score(y_test_df, y_predict_larsCV_true)\n",
    "mse_larsCV=mean_squared_error(y_test_df, y_predict_larsCV_true)\n",
    "rmse_larsCV=sqrt(mse_larsCV)\n",
    "mae_larsCV=mean_absolute_error(y_test_df, y_predict_larsCV_true)\n",
    "print('rmse for lassCV:', rmse_larsCV)\n",
    "print('r2 for lasso larsCV:',r2_larsCV)\n",
    "print('mae for larsCV:', mae_larsCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for bayesian in true space: -0.438634383683\n",
      "rmse for bayesian in true space: 65.9981373833629\n",
      "mae for bayesian in true space: 52.9714189195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Bayesian Ridge\n",
    "regr_b=linear_model.BayesianRidge()\n",
    "model_b=regr_b.fit(x_train_norm, y_train_norm)\n",
    "y_predict_b=model_b.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_b_true=y_scaler.inverse_transform(y_predict_b)\n",
    "#evaluate\n",
    "mse_b=mean_squared_error(y_test_df, y_predict_b_true)\n",
    "rmse_b=sqrt(mse_b)\n",
    "mae_b=mean_absolute_error(y_test_df, y_predict_b_true)\n",
    "r2_b=r2_score(y_test_df, y_predict_b_true)\n",
    "print('r2 for bayesian in true space:',r2_b)\n",
    "print('rmse for bayesian in true space:', rmse_b)\n",
    "print('mae for bayesian in true space:', mae_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for sgd in true space: -0.436332063558\n",
      "rmse for sgd in true space: 65.94530614174853\n",
      "mae for sgd in true space: 52.9466099654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#stochastic gradient descent\n",
    "regr_sgd=SGDRegressor(loss='squared_loss', penalty=None, alpha=0.001, epsilon=0.0001)\n",
    "model_sgd=regr_sgd.fit(x_train_norm, y_train_norm)\n",
    "y_predict_sgd=model_sgd.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_sgd_true=y_scaler.inverse_transform(y_predict_sgd)\n",
    "#evaluate\n",
    "mse_sgd=mean_squared_error(y_test_df, y_predict_sgd_true)\n",
    "rmse_sgd=sqrt(mse_sgd)\n",
    "mae_sgd=mean_absolute_error(y_test_df, y_predict_sgd_true)\n",
    "r2_sgd=r2_score(y_test_df, y_predict_sgd_true)\n",
    "print('r2 for sgd in true space:',r2_sgd)\n",
    "print('rmse for sgd in true space:', rmse_sgd)\n",
    "print('mae for sgd in true space:', mae_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for sgd2 in true space: -0.385510758668\n",
      "rmse for sgd2 in true space: 64.76813806598496\n",
      "mae for sgd2 in true space: 52.0917503512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "regr_sgd2=linear_model.SGDRegressor(loss='huber', penalty=None)\n",
    "model_sgd2=regr_sgd2.fit(x_train_norm, y_train_norm)\n",
    "y_predict_sgd2=model_sgd2.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_sgd2_true=y_scaler.inverse_transform(y_predict_sgd2)\n",
    "#evaluate\n",
    "mse_sgd2=mean_squared_error(y_test_df, y_predict_sgd2_true)\n",
    "rmse_sgd2=sqrt(mse_sgd2)\n",
    "mae_sgd2=mean_absolute_error(y_test_df, y_predict_sgd2_true)\n",
    "r2_sgd2=r2_score(y_test_df, y_predict_sgd2_true)\n",
    "print('r2 for sgd2 in true space:',r2_sgd2)\n",
    "print('rmse for sgd2 in true space:', rmse_sgd2)\n",
    "print('mae for sgd2 in true space:', mae_sgd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for sgd3 in true space: -0.473699908531\n",
      "rmse for sgd3 in true space: 66.7976200737096\n",
      "mae for sgd3 in true space: 53.2482900085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "regr_sgd3=linear_model.SGDRegressor(loss='epsilon_insensitive', penalty='none', alpha=0.0001)\n",
    "model_sgd3=regr_sgd3.fit(x_train_norm, y_train_norm)\n",
    "y_predict_sgd3=model_sgd3.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_sgd3_true=y_scaler.inverse_transform(y_predict_sgd3)\n",
    "#evaluate\n",
    "mse_sgd3=mean_squared_error(y_test_df, y_predict_sgd3_true)\n",
    "rmse_sgd3=sqrt(mse_sgd3)\n",
    "mae_sgd3=mean_absolute_error(y_test_df, y_predict_sgd3_true)\n",
    "r2_sgd3=r2_score(y_test_df, y_predict_sgd3_true)\n",
    "print('r2 for sgd3 in true space:',r2_sgd3)\n",
    "print('rmse for sgd3 in true space:', rmse_sgd3)\n",
    "print('mae for sgd3 in true space:', mae_sgd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for tree in true space: -1.39483037138\n",
      "rmse for tree in true space: 85.15180134009232\n",
      "mae for tree in true space: 69.8658181891\n"
     ]
    }
   ],
   "source": [
    "#using tree\n",
    "regr_tree=tree.DecisionTreeRegressor()\n",
    "model_tree=regr_tree.fit(x_train_norm, y_train_norm)\n",
    "y_predict_tree=model_tree.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_tree_true=y_scaler.inverse_transform(y_predict_tree)\n",
    "#evaluate\n",
    "mse_tree=mean_squared_error(y_test_df, y_predict_tree_true)\n",
    "rmse_tree=sqrt(mse_tree)\n",
    "mae_tree=mean_absolute_error(y_test_df, y_predict_tree_true)\n",
    "r2_tree=r2_score(y_test_df, y_predict_tree_true)\n",
    "print('r2 for tree in true space:',r2_tree)\n",
    "print('rmse for tree in true space:', rmse_tree)\n",
    "print('mae for tree in true space:', mae_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for ridge in true space: -0.447248586125\n",
      "rmse for ridge in true space: 66.19543310293533\n",
      "mae for ridge in true space: 53.1368208086\n"
     ]
    }
   ],
   "source": [
    "regr_ridge=linear_model.Ridge(alpha=0.75)\n",
    "model_ridge=regr_ridge.fit(x_train_norm, y_train_norm)\n",
    "y_predict_ridge=model_ridge.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_ridge_true=y_scaler.inverse_transform(y_predict_ridge)\n",
    "#evaluate\n",
    "mse_ridge=mean_squared_error(y_test_df, y_predict_ridge_true)\n",
    "rmse_ridge=sqrt(mse_ridge)\n",
    "mae_ridge=mean_absolute_error(y_test_df, y_predict_ridge_true)\n",
    "r2_ridge=r2_score(y_test_df, y_predict_ridge_true)\n",
    "print('r2 for ridge in true space:',r2_ridge)\n",
    "print('rmse for ridge in true space:', rmse_ridge)\n",
    "print('mae for ridge in true space:', mae_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for kernel ridge in true space: -0.447206403499\n",
      "rmse for kernel ridge in true space: 66.19446840427238\n",
      "mae for kernel ridge in true space: 53.1360154112\n"
     ]
    }
   ],
   "source": [
    "regr_kr=KernelRidge(alpha=1.0)\n",
    "model_kr=regr_kr.fit(x_train_norm, y_train_norm)\n",
    "y_predict_kr=model_kr.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_kr_true=y_scaler.inverse_transform(y_predict_kr)\n",
    "#evaluate\n",
    "mse_kr=mean_squared_error(y_test_df, y_predict_kr_true)\n",
    "rmse_kr=sqrt(mse_kr)\n",
    "mae_kr=mean_absolute_error(y_test_df, y_predict_kr_true)\n",
    "r2_kr=r2_score(y_test_df, y_predict_kr_true)\n",
    "print('r2 for kernel ridge in true space:',r2_kr)\n",
    "print('rmse for kernel ridge in true space:', rmse_kr)\n",
    "print('mae for kernel ridge in true space:', mae_kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for svm.rbf in true space: -2.99899561589\n",
      "rmse for svm.rbf in true space: 110.03527293815259\n",
      "mae for svm.rbf in true space: 87.0748887691\n"
     ]
    }
   ],
   "source": [
    "#support vector machines\n",
    "svr_rbf=SVR(kernel='rbf', C=1e3)\n",
    "model_rbf=svr_rbf.fit(x_train_norm, y_train_norm)\n",
    "y_predict_rbf=model_rbf.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_rbf_true=y_scaler.inverse_transform(y_predict_rbf)\n",
    "#evaluate\n",
    "mse_rbf=mean_squared_error(y_test_df, y_predict_rbf_true)\n",
    "rmse_rbf=sqrt(mse_rbf)\n",
    "mae_rbf=mean_absolute_error(y_test_df, y_predict_rbf_true)\n",
    "r2_rbf=r2_score(y_test_df, y_predict_rbf_true)\n",
    "print('r2 for svm.rbf in true space:',r2_rbf)\n",
    "print('rmse for svm.rbf in true space:', rmse_rbf)\n",
    "print('mae for svm.rbf in true space:', mae_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for svm.sig in true space: -6971469.14973\n",
      "rmse for svm.sig in true space: 145284.28724323265\n",
      "mae for svm.sig in true space: 106407.762572\n"
     ]
    }
   ],
   "source": [
    "svr_sig=SVR(kernel='sigmoid', C=1e3)\n",
    "model_sig=svr_sig.fit(x_train_norm, y_train_norm)\n",
    "y_predict_sig=model_sig.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_sig_true=y_scaler.inverse_transform(y_predict_sig)\n",
    "#evaluate\n",
    "mse_sig=mean_squared_error(y_test_df, y_predict_sig_true)\n",
    "rmse_sig=sqrt(mse_sig)\n",
    "mae_sig=mean_absolute_error(y_test_df, y_predict_sig_true)\n",
    "r2_sig=r2_score(y_test_df, y_predict_sig_true)\n",
    "print('r2 for svm.sig in true space:',r2_sig)\n",
    "print('rmse for svm.sig in true space:', rmse_sig)\n",
    "print('mae for svm.sig in true space:', mae_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for svm.poly in true space: -30.7205371927\n",
      "rmse for svm.poly in true space: 309.90367603388313\n",
      "mae for svm.poly in true space: 188.846526566\n"
     ]
    }
   ],
   "source": [
    "svr_poly=SVR(kernel='poly', C=1e3)\n",
    "model_poly=svr_poly.fit(x_train_norm, y_train_norm)\n",
    "y_predict_poly=model_poly.predict(x_test_norm)\n",
    "#scale back to original space\n",
    "y_predict_poly_true=y_scaler.inverse_transform(y_predict_poly)\n",
    "#evaluate\n",
    "mse_poly=mean_squared_error(y_test_df, y_predict_poly_true)\n",
    "rmse_poly=sqrt(mse_poly)\n",
    "mae_poly=mean_absolute_error(y_test_df, y_predict_poly_true)\n",
    "r2_poly=r2_score(y_test_df, y_predict_poly_true)\n",
    "print('r2 for svm.poly in true space:',r2_poly)\n",
    "print('rmse for svm.poly in true space:', rmse_poly)\n",
    "print('mae for svm.poly in true space:', mae_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  66.19832813,   57.34906102,   66.16415152,   66.19832813,\n",
       "         65.99813738,   65.94530614,   64.76813807,   66.79762007,\n",
       "         85.15180134,   66.1954331 ,   66.1944684 ,  110.03527294])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=np.array([rmse_linear, rmse_lasso, rmse_lassoCV, rmse_larsCV, rmse_b, rmse_sgd, rmse_sgd2, rmse_sgd3, rmse_tree, rmse_ridge, rmse_kr, rmse_rbf])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
       "        12.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D=np.array(np.arange(1, 13.0, 1))\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAFkCAYAAADynzv4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1df01ccfcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(D[0],C[0], color='r',marker='^', alpha=1)\n",
    "plt.scatter(D[1],C[1], color='b',marker='p', alpha=1)\n",
    "plt.scatter(D[2],C[2], color='g',marker='o', alpha=1)\n",
    "plt.scatter(D[3],C[3], color='m',marker='H', alpha=1)\n",
    "plt.scatter(D[4],C[4], color='c',marker='s', alpha=1)\n",
    "plt.scatter(D[5],C[5], color='r',marker='^', alpha=1)\n",
    "plt.scatter(D[6],C[6], color='b',marker='p', alpha=1)\n",
    "plt.scatter(D[7],C[7], color='g',marker='o', alpha=1)\n",
    "plt.scatter(D[8],C[8], color='m',marker='H', alpha=1)\n",
    "plt.scatter(D[9],C[9], color='c',marker='s', alpha=1)\n",
    "plt.scatter(D[10],C[10], color='r',marker='^', alpha=1)\n",
    "plt.scatter(D[11],C[11], color='b',marker='p', alpha=1)\n",
    "#plt.scatter(D[12],C[12], color='g',marker='o', alpha=1)\n",
    "#plt.scatter(D[13],C[13], color='m',marker='H', alpha=1)\n",
    "#plt.scatter(D[14],C[14], color='m',marker='H', alpha=1)\n",
    "n=['linear','lasso','lassoCV','larsCV','bayesian','sgd_squaredloss','sgd_huber','sgd_epsiloninsensitive','tree','Ridge','Kernel_Ridge','svr_rbf']\n",
    "for i, txt in enumerate(n):\n",
    "    plt.annotate(txt, (D[i],C[i]))\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEURAL NETWORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DIFFERENT TOPOLOGIES HAVE BEEN GIVEN TO 2D , 3D AND 2D/3D DESCRIPTORS AND ITERATIONS OF CONJUGATED GRADIENT DESCENT GIVEN\n",
    "def baseline_model():\n",
    "    model=Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(30,)))\n",
    "    model.add(Dense(30, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(24, init='normal', activation='relu'))\n",
    "    model.add(Dense(12, init='normal', activation='relu'))\n",
    "    model.add(Dense(6, init='normal', activation='relu'))\n",
    "    model.add(Dense(3, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, init='normal'))\n",
    "    # compile the model \n",
    "    epochs=100\n",
    "    learning_rate=0.5\n",
    "    momemtum=0.8\n",
    "    decay=learning_rate/epochs\n",
    "    sgd=SGD(lr=learning_rate, momentum=momemtum, decay=decay, nesterov=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(69.822726822725301, 9.4805891399517304)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=7\n",
    "#fit and evaluate the NN#USE OF STRATIFIEDKFOLD ONLY FOR BINARY CLASSIFICATION\n",
    "estimators=[]\n",
    "#estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)))\n",
    "pipeline=Pipeline(estimators)\n",
    "kfold=KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results=cross_val_score(pipeline, x_train_norm, y_train_norm, cv=kfold, scoring='mean_absolute_error')\n",
    "abs(results.mean()*100), results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(63.565813542484676, 7.183294718716013)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test=cross_val_score(pipeline, x_test_norm, y_test_norm, cv=kfold, scoring='mean_absolute_error')\n",
    "abs(results_test.mean()*100), results_test.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**TENSORFLOW MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS=list(x_df)\n",
    "len(COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
