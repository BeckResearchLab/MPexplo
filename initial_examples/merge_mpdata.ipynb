{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_train\n",
       "0      Train\n",
       "1      Train\n",
       "2      Train\n",
       "3      Train\n",
       "4      Train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"MP_Data.tsv\", sep='\\t', index_col=False)\n",
    "tt_df=df.rename(columns={'x':'test_train'})\n",
    "tt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diameter</th>\n",
       "      <th>petitjean</th>\n",
       "      <th>petitjeanSC</th>\n",
       "      <th>radius</th>\n",
       "      <th>VDistEq</th>\n",
       "      <th>VDistMa</th>\n",
       "      <th>weinerPath</th>\n",
       "      <th>weinerPol</th>\n",
       "      <th>a_aro</th>\n",
       "      <th>a_count</th>\n",
       "      <th>...</th>\n",
       "      <th>FASA_P</th>\n",
       "      <th>FCASA.</th>\n",
       "      <th>FCASA..1</th>\n",
       "      <th>VSA</th>\n",
       "      <th>dens</th>\n",
       "      <th>glob</th>\n",
       "      <th>std_dim1</th>\n",
       "      <th>std_dim2</th>\n",
       "      <th>std_dim3</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2.037476</td>\n",
       "      <td>6.011166</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129128</td>\n",
       "      <td>0.326050</td>\n",
       "      <td>0.224950</td>\n",
       "      <td>138.13699</td>\n",
       "      <td>0.958985</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>1.853123</td>\n",
       "      <td>1.382682</td>\n",
       "      <td>0.326444</td>\n",
       "      <td>123.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>2.954872</td>\n",
       "      <td>8.805204</td>\n",
       "      <td>1046</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059026</td>\n",
       "      <td>0.847237</td>\n",
       "      <td>1.136764</td>\n",
       "      <td>296.30197</td>\n",
       "      <td>1.296254</td>\n",
       "      <td>0.258447</td>\n",
       "      <td>2.394159</td>\n",
       "      <td>1.966377</td>\n",
       "      <td>1.217135</td>\n",
       "      <td>262.82812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.083532</td>\n",
       "      <td>8.211762</td>\n",
       "      <td>742</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046707</td>\n",
       "      <td>0.817581</td>\n",
       "      <td>0.464419</td>\n",
       "      <td>296.54431</td>\n",
       "      <td>0.946322</td>\n",
       "      <td>0.162139</td>\n",
       "      <td>3.164745</td>\n",
       "      <td>1.552043</td>\n",
       "      <td>1.274330</td>\n",
       "      <td>266.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.616827</td>\n",
       "      <td>7.313269</td>\n",
       "      <td>288</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044688</td>\n",
       "      <td>0.417896</td>\n",
       "      <td>0.574827</td>\n",
       "      <td>187.43799</td>\n",
       "      <td>1.195242</td>\n",
       "      <td>0.018573</td>\n",
       "      <td>2.524826</td>\n",
       "      <td>1.446899</td>\n",
       "      <td>0.344087</td>\n",
       "      <td>164.95312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.609669</td>\n",
       "      <td>6.833154</td>\n",
       "      <td>203</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103925</td>\n",
       "      <td>0.595412</td>\n",
       "      <td>0.267278</td>\n",
       "      <td>193.42802</td>\n",
       "      <td>0.978233</td>\n",
       "      <td>0.088156</td>\n",
       "      <td>2.550456</td>\n",
       "      <td>1.430062</td>\n",
       "      <td>0.757260</td>\n",
       "      <td>165.79688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diameter  petitjean  petitjeanSC  radius   VDistEq   VDistMa  weinerPath  \\\n",
       "0         5   0.400000     0.666667       3  2.037476  6.011166          82   \n",
       "1         9   0.444444     0.800000       5  2.954872  8.805204        1046   \n",
       "2        10   0.500000     1.000000       5  3.083532  8.211762         742   \n",
       "3         7   0.428571     0.750000       4  2.616827  7.313269         288   \n",
       "4         7   0.428571     0.750000       4  2.609669  6.833154         203   \n",
       "\n",
       "   weinerPol  a_aro  a_count    ...        FASA_P    FCASA.  FCASA..1  \\\n",
       "0          9      6       15    ...      0.129128  0.326050  0.224950   \n",
       "1         40     11       31    ...      0.059026  0.847237  1.136764   \n",
       "2         24     12       35    ...      0.046707  0.817581  0.464419   \n",
       "3         20     10       20    ...      0.044688  0.417896  0.574827   \n",
       "4         13      6       22    ...      0.103925  0.595412  0.267278   \n",
       "\n",
       "         VSA      dens      glob  std_dim1  std_dim2  std_dim3        vol  \n",
       "0  138.13699  0.958985  0.031032  1.853123  1.382682  0.326444  123.18750  \n",
       "1  296.30197  1.296254  0.258447  2.394159  1.966377  1.217135  262.82812  \n",
       "2  296.54431  0.946322  0.162139  3.164745  1.552043  1.274330  266.62500  \n",
       "3  187.43799  1.195242  0.018573  2.524826  1.446899  0.344087  164.95312  \n",
       "4  193.42802  0.978233  0.088156  2.550456  1.430062  0.757260  165.79688  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df=pd.read_csv(\"MP_Descriptors.tsv\", sep='\\t', index_col=False)\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outcome\n",
       "0     14.0\n",
       "1     20.5\n",
       "2     27.5\n",
       "3     30.5\n",
       "4     31.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"MP_Outcome.tsv\", sep='\\t')\n",
    "y_df=df.rename(columns={'x':'outcome'})\n",
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_df = x_df[tt_df['test_train'] == 'Train']\n",
    "x_test_df = x_df[tt_df['test_train'] == 'Test']\n",
    "y_train_df = y_df[tt_df['test_train'] == 'Train']\n",
    "y_test_df = y_df[tt_df['test_train'] == 'Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4126, 202), (275, 202))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def generate_dataset(X_train,X_test, n_features, noise=0.1):\n",
    "x_scaler = StandardScaler()\n",
    "x_train = x_scaler.fit_transform(x_train_df)\n",
    "x_test = x_scaler.transform(x_test_df)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4126, 1), (275, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler= StandardScaler()\n",
    "y_train = y_scaler.fit_transform(y_train_df)\n",
    "y_test = y_scaler.transform(y_test_df)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.72\n",
      "Variance score: 0.01\n",
      "-0.00589535723226\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# mske predictions from model\n",
    "y_predict = regr.predict(x_test)\n",
    "# The coefficients\n",
    "#print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % np.mean((y_predict - y_test) ** 2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(x_test, y_test))\n",
    "R_linear=r2_score(y_predict, y_test)\n",
    "print(R_linear)\n",
    "#r_2=regr.score(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
    "  # normalize=False, positive=False, precompute=False, random_state=None,\n",
    " #  selection='cyclic', tol=0.0001, warm_start=False)\n",
    "#reg.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.85\n",
      "Variance score: 0.01\n",
      "-5.7607431508\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Create linear regression object\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "# Train the model using the training sets\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "# mske predictions from model\n",
    "y_predict = reg.predict(x_test)\n",
    "# The coefficients\n",
    "#print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % np.mean((y_predict - y_test) ** 2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(x_test, y_test))\n",
    "R_Lasso=r2_score(y_predict, y_test)\n",
    "print(R_Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1082: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0991865285005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
    "\n",
    "model = LassoCV(cv=20).fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "Lasso_CV=r2_score(y_predict, y_test)\n",
    "print(Lasso_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.021e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:381: RuntimeWarning: overflow encountered in true_divide\n",
      "  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.080e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.226e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.976e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 213 iterations, alpha=1.439e-04, previous alpha=1.429e-04, with an active set of 92 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.233e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.064e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.502e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.339e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 47 iterations, alpha=5.339e-04, previous alpha=5.303e-04, with an active set of 36 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.062e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.332e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 207 iterations, alpha=3.246e-04, previous alpha=7.451e-05, with an active set of 90 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.496e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.126e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=9.258e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.597e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=4.048e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.971e-04, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 210 iterations, alpha=3.776e-04, previous alpha=7.260e-05, with an active set of 107 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.134e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.893e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.888e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.944e-04, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=2.944e-04, previous alpha=2.944e-04, with an active set of 47 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.266e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.276e-04, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=2.182e-04, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 151 iterations, i.e. alpha=9.541e-05, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=9.059e-05, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=8.841e-05, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 175 iterations, i.e. alpha=6.839e-05, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 176 iterations, alpha=7.049e-05, previous alpha=6.676e-05, with an active set of 105 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.350e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.173e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.975e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 208 iterations, alpha=2.468e-04, previous alpha=7.647e-05, with an active set of 85 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.150e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.443e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=6.272e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.805e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 43 iterations, alpha=5.805e-04, previous alpha=5.801e-04, with an active set of 32 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.141e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.905e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.458e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.335e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.629e-04, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.655e-04, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 136 iterations, alpha=5.904e-05, previous alpha=5.874e-05, with an active set of 89 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.160e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.543e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=7.113e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 67 iterations, alpha=3.885e-04, previous alpha=3.882e-04, with an active set of 46 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.238e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=6.343e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.339e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=3.161e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 62 iterations, alpha=3.177e-04, previous alpha=3.161e-04, with an active set of 45 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.164e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.894e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.871e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.933e-04, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=2.938e-04, previous alpha=2.933e-04, with an active set of 47 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.157e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.893e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 213 iterations, alpha=2.234e-04, previous alpha=7.596e-05, with an active set of 84 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.002e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=6.571e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.430e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 210 iterations, alpha=3.889e-04, previous alpha=9.147e-05, with an active set of 93 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.187e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.450e-04, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.824e-04, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.791e-04, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.769e-04, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=1.094e-04, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 151 iterations, i.e. alpha=9.916e-05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=8.285e-05, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=7.748e-05, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 166 iterations, alpha=7.823e-05, previous alpha=7.227e-05, with an active set of 103 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.197e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.953e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.261e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.119e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.903e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 98 iterations, alpha=1.179e-04, previous alpha=1.176e-04, with an active set of 63 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.086e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=7.101e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.182e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.155e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 209 iterations, alpha=2.428e-04, previous alpha=1.004e-04, with an active set of 104 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.083e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.480e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.479e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=7.394e-05, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 173 iterations, alpha=7.394e-05, previous alpha=7.394e-05, with an active set of 84 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=9.645e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.324e-04, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.188e-04, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 186 iterations, i.e. alpha=4.147e-05, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 208 iterations, alpha=5.524e-05, previous alpha=3.197e-05, with an active set of 109 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.419e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.714e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.454e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.021e-04, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 70 iterations, alpha=1.339e-04, previous alpha=1.320e-04, with an active set of 63 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0115075663367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.085e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=8.990e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.366e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.154e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.510e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LassoLarsCV(cv=20).fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "R_LassoLarsCV=r2_score(y_predict, y_test)\n",
    "print(R_LassoLarsCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = LassoLarsIC(cv=20).fit(x_train, y_train)\n",
    "#y_predict = model.predict(x_test)\n",
    "\n",
    "#print(r2_score(y_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0115075663367\n"
     ]
    }
   ],
   "source": [
    "reg_b =linear_model.BayesianRidge()\n",
    "model1=reg_b.fit(x_train, y_train)\n",
    "reg_b.predict = model1.predict(x_test)\n",
    "\n",
    "R_Bayesian=r2_score(y_predict, y_test)\n",
    "print(R_Bayesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import ARDRegression\n",
    "#reg_ARD=linear_model.ARDRegression()\n",
    "#model2=reg_ARD.fit(x_train, y_train)\n",
    "#reg_ARD.predict=model2.predict(x_test)\n",
    "\n",
    "#print(r2_score(y_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00589536, -5.76074315,  0.09918653, -0.01150757, -0.01150757])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array([R_linear, R_Lasso, Lasso_CV, R_LassoLarsCV, R_Bayesian])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=np.array(np.arange(1,6.0,1))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAFkCAYAAAA37aFpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2289385ec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(B[0],A[0], color='r',marker='^', alpha=1)\n",
    "plt.scatter(B[1],A[1], color='b',marker='p', alpha=1)\n",
    "plt.scatter(B[2],A[2], color='g',marker='o', alpha=1)\n",
    "plt.scatter(B[3],A[3], color='m',marker='H', alpha=1)\n",
    "plt.scatter(B[4],A[4], color='c',marker='s', alpha=1)\n",
    "#plt.show()\n",
    "n=['Linear','Lasso','LassoCV','La_LarsCV','Bayesian_Ridge']\n",
    "#l=['-0.00589536','-5.76074315','0.09918653','-0.01150757','-0.01150757']\n",
    "for i, txt in enumerate(n):\n",
    "    plt.annotate(txt, (B[i],A[i]))\n",
    "plt.ylabel('R square statistic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_train=np.ravel(x_train)\n",
    "#from sklearn.svm import SVR\n",
    "#svr_lin = SVR(kernel='linear', C=1e3)\n",
    "#y_lin = svr_lin.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.286818348546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf=linear_model.SGDRegressor(loss='squared_loss', penalty='none', alpha=0.0001, epsilon=0.0001)\n",
    "model2=clf.fit(x_train, y_train)\n",
    "y_predict1=model2.predict(x_test)\n",
    "R_SGD=r2_score(y_predict1, y_test)\n",
    "print(R_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1882446836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf=linear_model.SGDRegressor(loss='huber', alpha=0.0001)\n",
    "model3=clf.fit(x_train, y_train)\n",
    "y_predict2=model3.predict(x_test)\n",
    "R_SGD_h=r2_score(y_predict2, y_test)\n",
    "print(R_SGD_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.326342946481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf=linear_model.SGDRegressor(loss='epsilon_insensitive', penalty='none', alpha=0.0001)\n",
    "model4=clf.fit(x_train, y_train)\n",
    "y_predict3=model4.predict(x_test)\n",
    "R_SGD_ep=r2_score(y_predict3, y_test)\n",
    "print(R_SGD_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.393680463619\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "regr_1=tree.DecisionTreeRegressor()\n",
    "model5=regr_1.fit(x_train, y_train)\n",
    "y_predict4=model5.predict(x_test)\n",
    "#y_1=regr_1.predict(x_test)\n",
    "R_tree=r2_score(y_predict4, y_test)\n",
    "print(R_tree)\n",
    "#x_train.size, y_train.size, x_test.size, y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_train1=np.array(x_train)\n",
    "#x_train1=np.resize(x_train1, (4126))\n",
    "#x_train1.size\n",
    "\n",
    "#plt.scatter(x_train1, y_train, color='darkorange')\n",
    "#plt.plot(x_test, y_1, color='cornflowerblue')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0513537691532\n"
     ]
    }
   ],
   "source": [
    "reg=linear_model.Ridge(alpha=0.5)\n",
    "model6=reg.fit(x_train, y_train)\n",
    "y_predict5=model6.predict(x_test)\n",
    "R_Ridge=r2_score(y_predict5, y_test)\n",
    "print(R_Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0640647203179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "clf_1=KernelRidge(alpha=1.0)\n",
    "model7=clf_1.fit(x_train, y_train)\n",
    "y_predict6=model7.predict(x_test)\n",
    "R_KernelRidge=r2_score(y_predict6, y_test)\n",
    "print(R_KernelRidge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.09780887561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_lin=SVR(kernel='rbf', C=1e3)\n",
    "model8=svr_lin.fit(x_train, y_train)\n",
    "y_predict7=model8.predict(x_test)\n",
    "R_svr_rbf=r2_score(y_predict7, y_test)\n",
    "print(R_svr_rbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0311309451067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_lin=SVR(kernel='poly', C=1e3)\n",
    "model8=svr_lin.fit(x_train, y_train)\n",
    "y_predict8=model8.predict(x_test)\n",
    "R_svr_poly=r2_score(y_predict8, y_test)\n",
    "print(R_svr_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiva\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.000473169651765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_lin=SVR(kernel='sigmoid', C=1e3)\n",
    "model8=svr_lin.fit(x_train, y_train)\n",
    "y_predict9=model8.predict(x_test)\n",
    "R_svr_sig=r2_score(y_predict9, y_test)\n",
    "print(R_svr_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import Perceptron\n",
    "#perp=Perceptron()\n",
    "#model9=perp.fit(x_train, y_train)\n",
    "#y_predict=model9.predict(x_test)\n",
    "#R_perp=r2_score(y_predict, y_test)\n",
    "#print(R_perp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.86818349e-01,   1.88244684e-01,   3.26342946e-01,\n",
       "        -3.93680464e-01,   5.13537692e-02,   6.40647203e-02,\n",
       "        -1.09780888e+00,   3.11309451e-02,  -4.73169652e-04])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=np.array([R_SGD, R_SGD_h, R_SGD_ep, R_tree, R_Ridge, R_KernelRidge, R_svr_rbf, R_svr_poly, R_svr_sig])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D=np.array(np.arange(1, 10.0, 1))\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAFkCAYAAAAOihAyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xl8Tdf6+PHPOimSyCDmmGMOagjVutRPGmOpoqixaNH2\nIpq6paqKdEIn2n47oK2gplJSiiitGFpF0iaUDIagA6pqiqmS5/fHSU5zMsh0QsLzfr32K2evvfba\nz9m9V56svdbaRkRQSimllCqMLLc6AKWUUkqprGiiopRSSqlCSxMVpZRSShVamqgopZRSqtDSREUp\npZRShZYmKkoppZQqtDRRUUoppVShpYmKUkoppQotTVSUUkopVWhpoqKUUkqpQqtIJirGmFHGmCPG\nmMvGmJ3GmHtyeF5rY8w/xpjIgo5RKaWUUvlX5BIVY8yjwFvAFKAZEAWEGWPKZnOeJxACbCrwIJVS\nSinlEKaovZTQGLMT+FFExqbsG+A48K6IzLzBeUuAOCAZeFhE/G5GvEoppZTKuyLVo2KMKQY0Bzan\nlok109oEtLrBecMAH2BaQceolFJKKce561YHkEtlASfgZLryk0C9zE4wxtQBXgPaiEiytQPmxowx\nZYBOQAJwJR/xKqWUUncaZ6AGECYif+W3saKWqOSKMcYCfA5MEZFDqcU5OLVTynlKKaWUypuBwOL8\nNlLUEpXTQBJQIV15BeBEJvXdgRZAU2PM/6WUWbAObbkGdBSRLZmclwCwaNEifH19HRC2yomgoCDe\neeedWx3GHUXv+c2n9/zm03t+cx04cIBBgwZByu/S/CpSiYqI/GOMiQACgK/ANpg2AHg3k1POA43S\nlY0C/IFHyPomXgHw9fXFz0/H3N4snp6eer9vMr3nN5/e85tP7/kt45ChE0UqUUnxNjA/JWHZBQQB\nrsB8AGPM60AlERmSMtB2f9qTjTGngCsicuCmRq2UUkqpXCtSs34ARGQ58D8gGPgJaAx0EpE/U6pU\nBKreovCUuuNYLBa++uorAI4ePYrFYiE6OvqWXL8o2LRpE+++m1kH8L+mTZumPQBKpShyiQqAiHwg\nIjVExEVEWonInjTHhonIAzc4d5quoaIKyunTp3n66aepXr06zs7OeHt706VLF3744QdbnZ9++ol+\n/fpRqVIlnJ2d8fHxoXv37qxdu9ZWJ/UXfurm4eFBo0aNGD16NAcPHrwVXy1LJ06coEuXLrb9nMys\nK8jrF3Zt27Zl5MiRtv3MEq3nnnuOzZs3pz9VqTtSkUxU1O2pf//+tzqEfOvVqxdRUVEsXLiQ+Ph4\n1qxZQ7t27fjrL+sMvdDQUFq1asWlS5dYsGABMTExbNiwgR49ejB58mTOnz9va8sYw7fffsuJEyeI\njo7m9ddf58CBAzRp0oTvvvvOIfE64p6XL1+eYsWK2fZv9iKS6a9f2A0ZMgRnZ+cb1nF1dcXLy+sm\nRXT7ux3+bbmjiYhu6TbAD5CIiAhRKqfOnj0rxhjZunVrpscTExOlbNmy0rt372zbSkhIEGOMREVF\nZTjm7+8vPj4+kpycnKO4Vq9eLX5+fuLs7Cy1atWSadOmyfXr123HjTHy4YcfSpcuXcTFxUVq1qwp\nK1assB2/du2ajBo1Sry9vcXZ2Vlq1Kgh06dPtzs/NDQ0y7i3bNkiLVu2lBIlSoi3t7c8//zzkpSU\nZDverl07CQwMlPHjx0vp0qWlYsWKMnXq1Bx9t6yu/+WXX4q/v7+4urpKkyZN5IcffrDVP3r0qDz0\n0EPi5eUlJUuWlEaNGsn69ettx/fu3StdunQRNzc3qVChggwePFhOnz6dq3inTJki1apVkxIlSkjl\nypVl7NixtmM1atSQ2bNn2z5bLBYxxogxRnx8fGznN23aVERENm7cKM7OznLu3Dm7awQGBkpAQIBt\nf9u2bXL//feLi4uLVKtWTQIDAyUxMTHH91EpR4mIiBBAAD9xwO9k7VFRykHc3Nxwc3Nj9erVXLt2\nLcPxsLAwzpw5w/jx4/N1nbFjx3L06FEiIiKyrbtt2zaGDBlCUFAQMTExfPzxx4SEhPDaa6/Z1Xvp\npZfo06cP0dHRDBw4kH79+hEbGwvA7NmzWbt2LStWrCAuLo7PP/+cGjVq5CjW33//na5du3LvvfcS\nHR3NRx99xCeffMIrr7xiV2/BggW4ubmxa9cuZs6cSXBwcL4efbz44ouMHz+eqKgo6taty4ABA0hO\nTgbgv//9L9euXWP79u3s27ePGTNm4ObmBsC5c+cICAigefPmREZGEhYWxqlTp+jbt2+O412xYgWz\nZs1i7ty5HDx4kNWrV3P33XdnGufu3bsREUJCQjhx4gS7d+8GrL1pqY/QAgIC8PLyYuXKlbbzkpOT\nWb58eeoUUA4dOkSXLl3o06cP+/btY9myZezYsYMxY8bk+R4qVWg4Itu53Ta0R0Xl0ZdffillypQR\nFxcXad26tbzwwgsSHR0tIiIzZswQi8UiZ8+etdXfvXu3uLm52bavv/5aRG7coxITEyPGGPniiy+y\njad9+/Z2vR8iIosWLZJKlSrZ9o0xMmrUKLs69913n60sMDBQ2rdvn+U1btSj8sILL4ivr69d/Q8+\n+EA8PDxs++3atZO2bdva1WnZsqVMnDgx2++X1fU/++wz2/H9+/eLxWKR2NhYERFp3LixBAcHZ9rW\nK6+8Ip07d7YrO378uBhjJD4+Pkfxvv3221K/fn27Xqu00vaopI8/1dSpU6VZs2a2/Weeecbuv0FY\nWJi4uLjYelmGDx8uTz31lF0b27ZtEycnJ7l69WqmcShVULRHRalCrGfPnvz++++sWbOGLl26EB4e\nTvPmzQkJCcm0fpMmTYiKiiIqKorExESuX7+e7TXEmkznaNBqVFQUwcHBuLu727YRI0Zw8uRJrlz5\nd4mD++67z+68Vq1aceCAdQb/0KFD+emnn6hXrx5jx47lm2++yfa6qWJiYmjVyv41XK1bt+bixYv8\n+uuvtrLGjRvb1fH29ubUqVM5vk56aXswvL29ERFbe4GBgbz88su0adOGqVOnsnfvXlvdqKgovv32\nW7v75evrizGGQ4cO2erdKN4+ffpw6dIlfHx8GDlyJKtXryYpKSnP3wVg4MCBbNmyhRMnrOtaLl68\nmK5du+Lh4WGLe/78+XZxd+7cGYAjR47k69pK3WqaqCiVT3F/xbE+fj3xf8UDULx4cQICApg0aRLb\nt29nyJAhTJ06lbp16yIitkcqAMWKFaNmzZrUrFkzx9fbv38/xhh8fHyyrXvx4kWmTZtmS4aioqLY\nt28fcXFx2Q7oTNWsWTMSEhJ45ZVXuHLlCn379qVPnz45jjcn0g+GNcbYHtXkt73UhC61vSeeeIIj\nR47w2GOPsW/fPlq0aMH//Z914eqLFy/SvXt3oqOj7e5ZfHw8bdu2zVG8VapUIS4ujg8//BBXV1dG\njRpF27Zt85WstGjRgpo1a7J06VKuXLnCqlWrbI99UuN+8skn7eKOjo4mLi6OWrVq5fm6ShUGRXHB\nN6UKhTOXz1ClWhUut7gMqR0S0+DzZZ8zoM8AWz1fX19CQ0Pp2LEjpUuXZsaMGXbjDbKSWY+JiPDu\nu+/i4+NDs2bNsm3Dz8+P2NjYbBOhnTt32v3i27lzp906Hm5ubvTp04c+ffrwyCOP0LlzZ86ePUup\nUqVu2K6vry9ffvmlXdn27dtxd3enSpUq2cafFznpaapcuTIjR45k5MiRvPDCC8ydO5dRo0bh5+fH\nl19+SfXq1bFY8v53XIkSJejatStdu3blv//9L/Xr12fv3r00bdo0Q91ixYrlKIkZOHAgixYtonLl\nyjg5OfHggw/ajvn5+bF///4cJa9KFTXao3IbuxPX9LiZBqwcwOXrl/8tuAQIvPDpC+zdu5eEhAS+\n+OIL3njjDXr06IGrqyvz5s3j66+/plu3bmzcuJEjR46wd+9eZs6ciTEGJycnW3MiwunTpzl58iRH\njhxhzZo1tG/fnj179vDpp5/m6BfySy+9xIIFCwgODmb//v3ExMSwbNkyJk+ebFfviy++4LPPPiM+\nPp4pU6awe/du20DMd955h6VLlxIbG0tcXBzLly/H29s72yQFrANXjx8/zpgxY4iNjSU0NJSpU6cy\nbty4nN3kPEh9NJaVoKAgNm7cSEJCApGRkXz33Xc0aNAAgFGjRnHmzBn69evHnj17OHz4MGFhYTz+\n+OPZtpsqJCSETz/9lF9++YUjR46wcOFCXF1dqV69eqb1a9SowebNmzl58iRnz57Nst2BAwcSGRnJ\nq6++Su/eve16dSZMmMD333/PmDFjiIqK4uDBg4SGhupgWnVb0B6V21ivXr24fv06CxcuxMfHh5Mn\nT7J582a7NT0effRROnbsyIIFC6hduzZXr15lx44dTJ48mbZt29qegRtj2Lx5Mw0aNODSpUvs3buX\nWbNm0aRJE9auXYu/v/+t/Kr5cv36de66K3f/V4j7K46wQ2H2hcWtP47uOsr9be8n6XoSVatW5ckn\nn2TixIkA9OjRg++//54ZM2YwZMgQzpw5g6enJy1atGDZsmV07drV1pwxhg4dOgDYftH5+/szd+7c\nHD8q6tixI2vXriU4OJiZM2dSrFgx6tevz/Dhw+3qTZs2jaVLlzJq1Ci8vb1ZunQp9erVA8Dd3Z2Z\nM2dy8OBBnJycuOeee1i3bp1dnGml3a9UqRLr1q3jueeeo2nTppQuXZoRI0YwadKkLM/PrRtdP7Oy\npKQkRo8eza+//oqHhwddunTh7bffBqxjTXbs2MGECRPo1KkTV69epXr16nTu3NnWRnbxlipViunT\npzNu3DiSkpK4++67Wbt2rW1dlPTnv/XWW4wbN445c+ZQpUoVDh8+nGm7tWrVomXLluzevZvZs2fb\nHbv77rsJDw9n0qRJtG3bFhGhVq1aPProozeMVakiwREjcm+3jdtg1k9hXNMjKipK/P39xd3dXTw8\nPKRFixZ29/izzz6TatWqScmSJaVXr17y1ltvSalSpWzHhw4dKj179rRr85lnnpF27drZ9jds2CBt\n2rSRUqVKSZkyZaRbt25y6NChDN9l2bJl8v/+3/8TFxcXCQkJEZHs16E4deqUdOvWTVxcXKRi1YpC\nL4RSCJ0RpqZsIPRD1sWtExHrmhwPPPCAuLi4SJkyZWTkyJFy8eJFW5vfffedtGzZUkqWLCmlSpWS\nNm3ayLFjx3J0vxwls1knSimVVzrrR+VIYVzTY+DAgVStWpWIiAgiIyN5/vnnbd3XP/74I8OHDycw\nMJCff/4Zf39/XnnllRz9tZ22TmJiIuPGjSMyMpJvv/0WJycnevbsmeGciRMn8swzz3DgwAE6derE\n4cOHs12HYsiQIfz222+Eh4fzYciHsBtIzDym2qVrc+nSJTp16kSZMmWIiIhgxYoVbNq0ydZmUlIS\nPXv2xN/fn3379rFz505Gjhxp+z43ul9KKXXHcES2c7tt3AY9KiKFb00PDw8PWbBgQabHBgwYIN26\ndbMr69evn3h5edn2s+pR8ff3z/Kaf/75pxhj5JdffrH7Lu+9955dvezWoYiNjRVjjN3/JtrMbCOY\ndD0qBmkWZF3/Ys6cOVKmTBm5fPmy7Zx169aJk5OTnDp1Ss6cOSMWiyXLXq8b3a9UDRs2tPtvlrq5\nu7vL4sWLb3huKovFUmh7VD7//PNMv5+bm5s0atToVoenlMqEo3tUdIzK7SguDurWpWfPnnTt2pVt\n27axc+dO1q9fzxtvvMHcuXMzPS11TQ+A2rVrO3xNj2effZYnnniCBQsW0L59e/r06WMba3HgwAF6\n9eplV79Vq1aEhYVl1lSWDh48yEsvvcSPP/7I6dOnSU5OxhjDsWPHbAMmAZo3b253XlRUFHv37mXR\nokUZvtuRI0eIjY2lWLFidjNhvhr9FeWnlec69vfpf63+B1jXEGnSpIndNODWrVuTnJxMbGwsbdq0\nYciQIXTs2JEOHTrQvn17+vbtS8WKFbO9X6nWr1/PP//8k+m9qFChQo7uWX7X+ChIDz/8cIY1XlJp\n75JSdwZ99HO7WbEC6tWz/qRwrekxZcoU9u/fT7du3fj2229p0KABoaGhOb6WxWKxJQ+p0v+S7tat\nG3///Tfz5s1j165d7Nq1CxHJ8PirZMmSdvt5WYfCy8UL9+LuvNj2RdYNWEfc6DgMBrcSbjn+Tp9+\n+ik7d+6kdevWLFu2jHr16rFr1y4g4/1q2LBhhvtVtWpV23+z9Fv671gUlSxZMsvvV7Vq1VsdnlLq\nJtBE5XaSnAwvvmj9PHmydT8dX19fEhMT7db0yAlHrOkB1p6asWPHEhYWRq9evfjss89scf344492\nddNOowYoV64cf/zxh13Zzz//bPt85swZ4uLiePHFF/H396devXq2GU7ZfZe061Ck/4V41113Ub9+\nfa5fv243Fic2NpazZ89SrmQ5utTpQp0ydeza9PX1JSoqisuX/53CvH37dpycnGwzasDakzVhwgR2\n7NhBw4YNWbx4cab3q2fPnrb7pZRSdwpNVG4nK1dCSg/JmZgYApo04fPPPy8Ua3pcuXKFMWPGEB4e\nzrFjx9ixYwe7d++2PY4JDAxkw4YNvPXWWxw8eJD3338/w2OfBx54gD179rBw4UIOHjzI1KlT2bdv\nn+24l5cXZcqUYc6cORw6dIhvv/2WcePGZYgtfa8MZL8ORd26denUqRMjR45k165dREREMGLECFxd\nXbP8zgMHDsTZ2ZkhQ4bwyy+/8N133xEYGMhjjz1GuXLlSEhI4IUXXmDnzp0cO3aMjRs3Eh8fT4MG\nDbK9X0opdcdwxECX222jKA6mTUoSqVdPxGIRAblqjLxQpoy0aNFCvLy8xM3NTXx9fWXKlCly5coV\n22kRERHSt29fqVixohQvXlzKlSsnXbp0sRscm5CQIBaLxba5ublJw4YNZfTo0XZTf2/k2rVr0r9/\nf6levbo4OztLlSpVZOzYsXYvTEs7Pfnhhx+Wt99+224wrYj1ZW3e3t7i5eUl48aNk8DAQLvBtJs3\nb5aGDRuKi4uLNG3aVLZu3Wo3WDT1u2Q2MHjPnj3SqVMn8fDwEHd3d2natKm8/vrrtuMnT56Uhx56\nSFxcXKRGjRqyaNEi8fHxsXvBXPqBqfv27ZOAgABxdXWVsmXLylNPPWWb8nzy5Enp2bOnVK5cWZyd\nncXHx0emTZuW4/ullFKFkaMH0xrJ5K/LO50xxg+IiIiIsBs8Wah98QWkexW9rbx375sfjwOEhIQQ\nFBTEmTNnbnUoSimlcigyMjJ1wkJzEYnMb3v66Od2kJxsHZOS/t0kFkuWY1WUUkqpokATldvBzz9b\nx6akT0iSkyEmxnr8JmjUqJHda+ZTNw8PD5YsWXJTYlBKKXV70XVUbgdNm8LmzZCYyTKpJUtaj98E\njljTI60hQ4YwZMiQ/IallFKqCNNE5XZgscADD9zqKHRdC6WUUg6nj36UUkopVWhpoqKUUkqpQksT\nFaWUUkoVWpqoKJWNadOm5fgVATdisVj46quvHBCRUkrdOTRRUYXO6dOnefrpp6levTrOzs54e3vT\npUsXu3f//PTTT/Tr149KlSrh7OyMj48P3bt3Z+3atbY6R48exWKx2DYPDw8aNWrE6NGjOXjwYK5i\nysnboZVSSjmeJiqq0OnVqxdRUVEsXLiQ+Ph41qxZQ7t27WwvGAwNDaVVq1ZcunSJBQsWEBMTw4YN\nG+jRoweTJ0/m/PnztraMMXz77becOHGC6OhoXn/9dQ4cOECTJk347rvvbtVXdJispoMrpdRtwxHr\n8N/sDRgFHAEuAzuBe25QtyewETgFnAO+Bzpm037Re9fPbeLs2bNijJGtW7dmejwxMVHKli0rvXv3\nzrathIQEMcZk+l4ff39/8fHxkeTk5GzbmTp1qjRr1kwWLlwoNWrUEE9PT+nXr59cvHjRVqdGjRp2\n7/wREWnatKnt3T0iIsYY+fDDD6VLly7i4uIiNWvWlBUrVtidc/z4cenbt6+UKlVKSpcuLQ8//LAk\nJCTYjg8dOlR69Oghr776qlSqVElq1qyZbfxKKXUzOfpdP0WuR8UY8yjwFjAFaAZEAWHGmLJZnNIW\na6LSBWsC8h2wxhjT5CaEq3LJzc0NNzc3Vq9ezbVr1zIcDwsL48yZM4wfPz5f1xk7dixHjx4lIiIi\nR/VT36a8bt06vv76a8LDw5k+fXqur/vSSy/Rp08foqOjGThwIP369SM25Y3X169fp1OnTnh6erJj\nxw6+//573N3d6dy5M9evX7e1sXnzZuLi4ti0aZPdoy6llLodFblEBQgCPhaRBSISAzwFXAIez6yy\niASJyJsiEiEih0RkEhAPPHTzQlY55eTkREhICCEhIZQqVYo2bdowadIk9u7dC0B8fDwAdevWtZ2z\nZ88euyX7161bl+116tevj4iQkJCQo7hEhJCQEHx9fWndujWDBw9m8+bNuf5+ffv2ZdiwYdSuXZvg\n4GBatGjBe++9B8DSpUsREebMmUODBg2oV68en3zyCceOHWPLli22Ntzc3Jg3bx6+vr74+vrmOgal\nlCpKilSiYowpBjQHbL8hRESATUCrHLZhAHdAX8lbiCQnw86d1p89e/bk999/Z82aNXTp0oXw8HCa\nN29OSEhIpuc2adKEqKgooqKiSExMtOt9yIqkvDU8p4Nka9Sogaurq23f29ubU6dO5ejctO677z67\n/VatWnHgwAEAoqOjiY+Pt0u6ypQpw9WrVzl06JDtnLvvvpu77tJFpZVSd4ai9q9dWcAJOJmu/CRQ\nL4dtPAeUBJY7MC6VD7//DgMHwpYt4O8Pn38O3t7FCQgIICAggEmTJjFixAimTp3KO++8g4gQGxtL\ny5YtAShWrBg1a9bM1TX379+PMQYfH58c1S9WrJjdvjGG5DQvgbRYLLbkJ1VuB7pevHiRFi1asHjx\n4gxtlStXzva5ZMmSuWpXKaWKsqKWqOSLMWYAMBnoLiKns6sfFBSEp6enXVn//v3p379/AUV45/n6\naxg8GC5csO5v2wYNG8LChdC167/1fH19CQ0NpWPHjpQuXZoZM2awcuXKbNvPrMdERHj33Xfx8fFx\nyPooYE0k/vjjD9v++fPnOXLkSIZ6O3fuZNCgQXb7fn5+APj5+bF8+XLKlSuHm5ubQ+JSSqmCtGTJ\nEpYsWWJXdu7cOYdeo6glKqeBJCD9q3grACdudKIxph8wB+gtIjmal/rOO+/Yfokoxzt9Gh5KGSmU\n2oFw/foZ/v67D926PU54eGOqVXNn9+7dvPHGG/To0QNXV1fmzZtHv3796NatG4GBgdSpU4eLFy+y\nfv16jDE4OTnZriEinD59mpMnT3Lp0iX27dvHrFmz2LNnD+vWrXPY+igPPPAAISEhdOvWDU9PT6ZM\nmZLp45kvvviC5s2b06ZNGxYtWsTu3bv59NNPARg4cCBvvvkmDz/8MNOmTaNKlSokJCSwatUqJkyY\nQKVKlRwSq1JKOUpmf7xHRkbSvHlzh12jSCUqIvKPMSYCCAC+AtuYkwDg3azOM8b0B+YBj4rIhpsR\nq8pemTJQuTL8+mvaUjfgPooVm0WPHof4559/qFq1Kk8++SQTJ04EoEePHnz//ffMmDGDIUOGcObM\nGTw9PWnRogXLli2ja5quGGMMHTp0AMDV1ZXq1avj7+/P3Llzc/246EYmTpxIQkICDz30EJ6enrz8\n8ssZBuoaY5g2bRpLly5l1KhReHt7s3TpUurXrw+Ai4sLW7duZcKECTzyyCNcuHCBypUrExAQgIeH\nh8NiVUqposSkfxZe2Blj+gLzsc722YV1FlBvoL6I/GmMeR2oJCJDUuoPSKkfCKxK09RlETlPJowx\nfkBERESE9qgUsHHj4N13Ie3417vugrFj4c03b11cSiml8iZNj0pzEYnMb3tFatYPgIgsB/4HBAM/\nAY2BTiLyZ0qVikDVNKeMwDoA9/+A39Nss25WzCprvXvbJylg3e/d+9bEo5RSqnApUo9+UonIB8AH\nWRwblm7f/6YEpfLk3nth5Eg4fvzfsqpVIWVCz03RqFEjjh49mqHcGMPHH3+sg6eVUuoWKpKJirp9\nWCzw8ce3Nob169dnOZW4QoX047aVUkrdTJqoqDte1apVs6+klFLqlihyY1SUUkopdefQREUppZRS\nhZYmKkoppZQqtDRRUUoppVShpYmKUkoppQotTVSUUkopVWhpoqKUUkqpQksTFaWUUkoVWpqoKKWU\nshMeHo7FYuH8+Uzf25on/v7+PPvsszes4+Pjw7vvvuuwa6rbgyYqSil1CwwbNoxevXrZla1YsQIX\nFxfeeeedWxTVv4wxOaqXmtQ4OTlhsVgoX748Xbt2Zd++fXb1Vq1axcsvv1wQoarbnCYqSilVCMyb\nN4/Bgwfz8ccfExQUlKc2kpKSHBxVzhhjiIuL48SJE2zcuJGrV6/SrVs3rqd5NXqpUqUoWbLkLYlP\nFW2aqCilbltHjx7FYrEQHR2dZZ2CeMyRWzNnzmTs2LEsW7aMxx57DAAR4fXXX6dmzZq4urrSrFkz\nVq5caTsnNe4NGzbQokULnJ2d2bFjB9OmTaNZs2YsWrQIHx8fSpUqRf/+/UlMTLSdm13beVGuXDnK\nly9P06ZNCQoK4vjx48TExNiOp3/08+eff/LQQw/h6upKrVq1WLx4cYY2Y2NjadOmDS4uLtx9991s\n2bIFi8XCV199Zavz66+/8uijj+Ll5UWZMmXo0aNHpm9DV0WXJipKqSJr2LBhtscOxYsXp2bNmkyY\nMIGrV68CUK1aNU6cOEGjRo1u2E5OH3MUhOeff55XX32Vr7/+mu7du9vKX3vtNRYtWsScOXPYv38/\nQUFBDB48mG3bttmdP3HiRGbMmMGBAwdo3LgxAIcOHSI0NJR169bx9ddfEx4ezvTp03Pddm6ICADn\nzp3j888/B6B48eJZ1h8yZAi//fYb4eHhrFixgg8++IA///zTdjw5OZmHH34Yd3d3du/ezccff8zz\nzz9v99/q+vXrdOrUCU9PT3bs2MH333+Pu7s7nTt3tuvNUUWciOiWbgP8AImIiBClVOE1dOhQefDB\nB+XUqVPqySpyAAAgAElEQVTy66+/SmhoqHh6esrzzz+f4za2bNkiFotFzp07V4CRZjR06FApUaKE\nWCwW+e677+yOXb16VUqWLCk7d+60Kx8+fLgMHDhQRKxxG2NkzZo1dnWmTp0qbm5ukpiYaCsbP368\ntGrVKldt5/SepMbh7u4ubm5uYowRY4z07NnTrl67du0kKChIRERiY2PFGGP3b2xMTIwYY2T27Nki\nIrJ+/XopXry4nDp1ylZn06ZNYoyR0NBQERFZuHCh+Pr6Zrh3rq6u8s0332QbuyoYERERAgjgJw74\nnaw9KkqpIq1EiRKUK1eOypUr0717dzp06MA333wDZP7oZ926ddSrVw9XV1cCAgJISEjI0ObcuXOp\nVq0abm5u9O3bl1mzZuHl5WVXJzQ0lObNm+Pi4kLt2rUJDg4mOTk5V7E3adKEGjVq8NJLL9k9mjl4\n8CCXLl2iQ4cOuLu727aFCxdy+PBhWz1jDM2bN8/Qbo0aNXB1dbXte3t7c+rUqVy1nRvGGLZv305k\nZCQhISHUq1ePDz/8MMv6MTExFCtWDD8/P1tZvXr1KFWqlG0/Li6OqlWrUq5cOVtZy5Yt7dqJjo4m\nPj7e7nuUKVOGq1evcujQoTx9F1X43HWrA1BKKUfZt28fO3bsoEaNGraytI8Kjh8/ziOPPMKYMWMY\nMWIEe/bsyTBldseOHTz99NO88cYbPPTQQ2zatInJkyfbtbNt2zaGDBnC+++/z/3338/BgwcZOXIk\nxhgmT5584yDj4qBuXQAqV67MihUraNeuHZ07d2bDhg2ULFmSixcvAtakqlKlSnanlyhRwm4/swGq\nxYoVs9s3xtiSqNy0nRs1atTAw8ODOnXqcPLkSfr27Ut4eHie28uJixcv0qJFCxYvXmx79JQqbYJT\nVIWHh+Pv78/Zs2fx8PC41eHcMtqjopQq0tasWYO7uzsuLi40btyYP//8k/Hjx9uOp/0F9uGHH1K7\ndm1mzpxJnTp16N+/P0OHDrVr7/333+fBBx8kKCiI2rVr89RTT9G5c2e7OsHBwUycOJFBgwZRvXp1\nAgICCA4O5qOPPrpxsCtWQL161p8pqlatSnh4OCdOnKBTp04kJibSoEEDSpQowdGjR6lZs6bdVrly\n5bzfLCjQtlONGjWKffv2ERoamunx+vXrc/36dSIiImxlsbGxnD171rZfr149jh8/bjduZdeuXXbt\n+Pn5ER8fT7ly5TJ8F3d3d4d8l1vtVo6fKiw0UVFKFRnxly4ReeGCbfvrn39o0bYtoT/+yK5duxg6\ndCjDhg2jR48emZ4fExPDvffea1fWqlUru/3Y2NgMjxjS70dFRREcHGz3yGHEiBGcPHmSK1euZB58\ncjK8+KL18+TJkCaBqlKlCuHh4Zw6dYpOnTohIvzvf/8jKCiIBQsWcPjwYX766Sfef/99Fi5caDsv\nfS9CTri5uTm87fR1XVxcGDFiBC+99FKm9evWrUunTp0YOXIku3btIiIighEjRtg9rurQoQM1a9bk\nscceY+/evezYsYMXX3wRY4ztl/fAgQMpW7YsDz/8MNu3bychIYEtW7YwduxYfv/999zcllxJTk7O\n071XeaOJilKqSIi/dIm6u3bRPCLCtq356y92/vMPAadO4VyrFp988gk7d+7ks88+K9BYLl68yLRp\n04iKirJt+/btIy4uDmdn58xPWrkSYmOtn2NiIN0U2kqVKhEeHs7p06fp3Lkz48ePZ/LkyUyfPp0G\nDRrQpUsX1q1bh4+Pj+2cvP61/fLLLzu07czqjh49mpiYGFak9B6lrzN//nwqV65Mu3bt6N27N08+\n+STly5e3HbdYLISGhpKYmEjLli0ZOXIkL774IiJiu8cuLi5s3bqVatWq8cgjj9CgQQNGjBjB1atX\nM31UsmLFCho3boyrqytly5alY8eOfPXVV7i4uGSYnj527Fjat29vi9XLy4s1a9bQsGFDnJ2dOX78\n+A3vybBhw+jZsyfBwcGUL18eT09Pnn76abvZSNeuXSMwMJAKFSrg4uLC/fffz549ezJt79KlS3h6\nevLll1/ala9evRo3Nze7MU63HUeMyL3dNnTWj1KFTsT588J339lvnTsL998vfPedRJw/LyIiS5Ys\nEW9vb7ly5YokJCSIMUaioqJEROSFF16Qu+++267d559/3m6GS79+/aR79+52dQYNGiReXl62/dat\nW8vw4cNzHnxSkki9eiIWiwhYf9avby1XObZ9+3axWCxy+PDhXJ/7xx9/SLFixWT27Nly9OhR2bdv\nn3z44Ydy7tw58fb2lk8//dRWNykpSSpWrCifffaZiIjMnz9fihcvLm3atJEffvhB4uLi5PLlyze8\n3tChQ8Xd3V369+8v+/fvl3Xr1kn58uXlxRdftNUJDAyUKlWqSFhYmBw4cECGDh0qpUuXlr///ltE\nMs6+GjlypHTr1s3uOg8//LAMGzYs1/ejIDl61s8tTwoK46aJilKFT04TlevXr0uVKlXkrbfeypCo\nHDt2TJydneW5556T2NhY+fzzz8Xb29vul8GOHTvkrrvukrffflvi4+Plo48+krJly0rp0qVtsYSF\nhUnx4sVl2rRp8ssvv8iBAwdk6dKldr+E7Cxfbv3nNv32xRcFe9OKuFWrVsk333wjCQkJ8s0330jD\nhg2lbdu2eWorMjJSLBaLHDt2LMOxZ555Rtq3b2/bDwsLExcXF9v/JubPny8Wi0X27t2b4+sNHTpU\nypYtK1euXLGVffTRR+Lh4SEiIomJiVK8eHFZunSp7fg///wjlStXljfffFNEMiYqu3btkmLFismJ\nEydEROTUqVNSrFgx2bZtW47juhl0erJSSt2Ak5MTo0ePZubMmSQmJto9cqhatSorV64kNDSUpk2b\nMmfOHF5//XW78//zn//w0Ucf8c4779C0aVM2btxIUFCQ3SOdjh07snbtWr755htatmxJq1atmDVr\nlt1sI5vkZOuYFEu6f24tFmt5Lqc032wPPvig3Vic1M3Dw8NuEbmCcOHCBUaNGoWvry+PP/449957\nL6tXr85VG3F/xbE+fj2uVazT0Rs1akTfvn2ZN2+ebfDuwIED2bJlCydOnABg8eLFdO3a1e7xUfHi\nxbNdODC9Jk2a2M2katWqFRcvXuT48eMcOnSI69ev85///Md2/K677qJly5YcOHAg0/buueceGjRo\nQEhICAALFy6kRo0atGnTJldx5dW0adPsppTfLDo9WSlVdE2YkEXxBCakHEv//psHH3yQBx980K5s\nyJAhdvtPPPEETzzxhG1/xIgR1K5d265Ohw4d6NChQ/Yx/vzzv2NT0kpOto5V+flnuAX/+OfUJ598\nwuXLlzM9Vrp06QK99uDBgxk8eHCezj1z+QwDVg4g7FCYrazTY51Y/sJydobv5L333mPSpEns2rWL\nFi1aULNmTZYuXcpTTz3FqlWrWLBggV17Li4u+foujjJ8+HA++OADxo8fz/z583n88cdv2rWfe+45\nAgMDb9r1UmmiopRS6bz11lt06NCBkiVLsm7dOhYuXHjDBcxuqGlT2LwZMhvsWLKk9Xgh5u3tfatD\nyJMBKwew6fAmu7LU/Q1TNjB58mSqV6/OqlWreOaZZxg4cCCLFi2icuXKODk5ZUhm8yIqKoqrV6/a\nelV++OEH3NzcqFq1KmXKlKFYsWLs2LGDfv36AdZXAuzevTvD2j5pDRo0iAkTJvDee+9x4MAB27uh\nciI5Odlu1lRuubq62s3Muln00Y9Sqkhwd3LK1/Hc2LVrFx07dqRx48bMmTOH9957j2HDhuWtMYsF\nHngAHnoo4/bAAxkfCal8i/srjrBDYSRJmt60XyFpaxJh28LYGr2VlStXcvr0aXx9fQHr45/IyEhe\nffVVevfunWHRvLy4du0aTzzxBAcOHGDdunVMnTqVMWPGANZf+k8//TTPPfccYWFh7N+/n+HDh3P5\n8mW7XhIRYfXq1bbZSrVr18bDw4Nnn32W5ORk3Nzc7K6Z39lKW7Zs4d5778XNzQ0vLy/uv/9+2zmp\nL7xMlZSURGBgIF5eXpQvX55JkyYxdOhQxo0bl+97l1aR7FExxowC/gdUBKKAMSKy+wb12wFvAQ2B\nY8CrIhJyE0JVSjlIHVdX4lq25EK6RzlgTVLqOPAvvWXLljmsLXXzHTqTyfL5JYCjwE7ouKAjPjV8\nePvtt+nUqRMAtWrVomXLluzevZvZs2c7JI6AgADq1KlD27ZtuXbtGgMGDGDKlCm249OnT0dEeOyx\nx7hw4QItWrRg48aNeHp62rUzfPhw3nzzTXr06MGFCxeYN2+e7bUOK1eutCXRycnJLF++3DbuyhjD\npUuXmDlzJp988gllypSxmwKeXlJSEj179uTJJ59k2bJlXL16lV27dtn1wKT9PH36dJYsWUJISAj1\n69dn1qxZrF692vHjWBwxIvdmbsCjwBXgMaA+8DFwBiibRf0awEVgJlAPGAX8A3S4wTV01o9SShVR\nsadjhalkucWdjivwGIYOHZrhxYx5kdlspQULFki5cuUkMDDQobOVzpw5IxaLRbZu3Zrp8alTp0qz\nZs1s+xUrVpS3337btp+UlCTVq1cXf3//O37WTxDwsYgsEJEY4CngEpDViKKngcMiMl5EYkXk/4AV\nKe0opZS6zdQtU5dOtTrhZOwfBzoZJzrV6kSdMnVuUWS516RJE9tspUceeYTXXnuN1157jaeeeorB\ngwc7dLaSl5cXQ4YMoWPHjnTv3p13333X1nZ658+f5+TJk9xzzz22MovFkulLMvOrSCUqxphiQHNg\nc2qZiAiwCWiVxWn3pRxPK+wG9ZVSShVxSx5ZQvua7e3K2tdsz5JHljik/dQp2plN296xY0e+248f\nG88O7x38UPkHpuydwtzOczlz5gyTJk3i8OHDDBgwwG620pUrV1i1ahWDBg2yaye3s5U+/fRTdu7c\nSevWrVm2bBl169bN8I6lm62ojVEpCzgBJ9OVn8T6WCczFbOo72GMKSEiVx0bolJKqVvNy8WLDYM2\nEP9XPAfPHKR26doO7UmJiorK8ljlypVp3bp1ntv++9u/+e3d3+zKyi8vz5fffonnZk+qV6/Ohg0b\nqF+/foHMVmrSpAlNmjRhwoQJ/Oc//2Hx4sUZ3nfl4eFBhQoV2L17t20dl+TkZCIjI+1exeAIRS1R\nUUoppXKsTpk6BfKop2bNmg5vEyD5ejLxo+Otf5InwQEOEEkkLSwtuPDkBS4FX8owW2nq1KkOma2U\nkJDAnDlz6N69O5UqVSImJob4+PgMbxhPNWbMGF577TVq1apF/fr1ee+99zh79qzD3/hc1BKV00AS\nUCFdeQUg8wdp1vLM6p/PrjclKCgow+jr/v37079//xwHrJRSSuVUYnQilw5csu2XpCTRRLMyeSWX\n4i9RfWL1Aput5OrqSkxMDAsWLOCvv/7C29ubMWPGMHLkyEzrT5gwga1bt9K7d28AqlevjqurK3v3\n7s1XHOkZ6xCPosMYsxP4UUTGpuwbrFOO3xWRNzKpPx3oIiJN0pQtBkqJSKZ9ZMYYPyAiIiLiliwX\nrJRS6s6UfD2ZPY33cCnukvXP8lRO4FrPlXui78E4ObbHwlFEBF9fX9q2bcvcuXMBmotIZH7bLVKD\naVO8DYwwxjxmjKkPfAS4AvMBjDGvG2PSrpHyEVDTGDPDGFPPGPNfoHdKO0oppVShYbnLQp3369gn\nKQBJUOf9OoUqSTl27Bjz5s0jPj6evXv38tRTT5GQkEDnzp0dep2i9ugHEVlujCkLBGN9hPMz0ElE\n/kypUhGomqZ+gjGmK/AOEAj8CjwhIulnAimllFK3nNcDXlQOrMyp5adsZeUfLY+Xv1e+23Z3d8cY\nQ/qnKcYY1q9fn6tBwBaLhfnz5/Pcc88hIjRq1IjNmzc7/L1IRe7Rz82gj36UUkrdjg4fPpzlscqV\nK9u97TmvIiMjU9dTccijnyLXo6KUUkqpvCmo2UoFqSiOUVFKKaXUHUITFaWUUkoVWpqoKKWUUqrQ\n0kRFKaWUUoWWJipKKaWUKrQ0UVFKKaVUoaWJilJKKaUKLU1UlFJKKVVoaaKilFJKqUJLExWllFJK\nFVqaqCillFKq0NJERSmllFKFliYqSimllCq0cp2oGGPeNcaMzqR8tDFmlmPCUkoppZTKW4/KI8D2\nTMq/B3rnLxyllFJKqX/lJVEpA1zIpPw8UDZ/4SillFJK/SsvicpBoEsm5V2Aw/kLRymllFLqX3fl\n4Zy3gfeNMeWAb1PKAoBxwDOOCkwppZRSKteJioh8aowpAUwCJqcUJwBPi8gCB8amlFJKqTtcXnpU\nEJEPgQ9TelUui8hFx4allFJKKZXHRCWViPzpqECUUkoppdLLUaJijIkEAkTkb2PMT4BkVVdE/BwV\nnFJKKaXubDntUQkFrqb5nGWiopRSSinlKDlKVERkWprPUwssGqWUUkqpNPKyhP5hY0yZTMpLGWN0\nHRWllFJKOUxeFnyrAThlUl4CqJKvaJRSSiml0sjxrB9jTPc0u52MMefS7DthXfTtiKMCU0oppZTK\nzfTk1Sk/BQhJd+wfrIu+jXNATFkyxngB7wPdgGRgJTBWRBKzqH8X8CrW5f1rAueATcDzIvJHQcaq\nlFJKqfzL8aMfEbGIiAU4BpRP3U/ZSohIPRFZW3ChArAY8MXae9MVaAt8fIP6rkBTYBrQDOgJ1MM6\nc0kppZRShVxeltD3SV9mjCklImcdE1LmjDH1gU5AcxH5KaVsDPC1MeZ/InIik1jPp5yTtp3RwI/G\nmCoi8mtBxqyUUkqp/MnLrJ8JxphH0+x/AZwxxvxmjGni0OjstQL+Tk1SUmzC+ijq3ly0UyrlnAJN\nrJRSSimVf3mZ9fMUcBzAGNMBaA90BtYDbzgutAwqAqfSFohIEnAm5Vi2Ul6mOB1YrO8nUkoppQq/\nvLzrpyIpiQrWQa3LRWSjMSYB+DG3jRljXgcm3KCKYB2Xki8pA2u/SGnvv/ltTymllFIFLy+Jyt9A\nVazJSmfgxZRyQ+brq2TnTeCzbOocBk4A5dMWGmOcgNIpx7KUJkmpCjyQ096UoKAgPD097cr69+9P\n//79c3K6UkopdVtbsmQJS5YssSs7d+5cFrXzxojk7rU9xpjU6cHxWGfS1BCRi8aYfsD4gnopYcpg\n2l+AFmkG03YE1gFVMhtMm1InNUmpCfiLyJkcXMsPiIiIiMDPT9+xqJRSSuVUZGQkzZs3B+vkl8j8\ntpeXMSpBWNcy2Q90SNM74Q18kN+AsiIiMUAYMNcYc48xpjXwHrAkbZJijIkxxjyc8vkurGut+AGD\ngGLGmAopW7GCilUppZRSjpGX6cn/YH1ck778HYdEdGMDsCZJm7Au+LYCGJuuTh0g9XlNZay9PwA/\np/w0WMep+ANbCzJYpZRSSuVPjhKVlOXz14vIP+mW0s9ARL5ySGSZt30Wa8/Ijeo4pfl8lLyNm1FK\nKaVUIZDTHpXV/Ds9ePUN6gmaGCillFLKQXKUqKQsnZ/hs1JKKaVUQcrLyrSPpSyclr68uDHmMceE\npZRSSimVt1k/n/HvYNW03Ml+PRSllFJKqRzLS6KSOmsmvSqAY1d5UUoppdQdLcfTk40xP2FNUATY\nbIy5nuawE+ADbHBseEoppZS6k+VmHZXU2T5NsS68lnYZ+mtAAtbF1ZRSSimlHCLHiYqITANIefng\nMhG5UlBBKaWUUkpB3lamDSmIQJRSSiml0st1opLyxuIgoC9QDSie9riIlHZMaEoppZS60+Vl1s8U\n4FlgGdZpym8DX2J9985Uh0WmlFJKqTteXhKVgcAIEXkLuI717cXDgWDgPkcGp5RSSqk7W14SlYrA\n3pTPF/l38be1QFdHBKWUUkopBXlLVH4FvFM+HwI6pny+B7jqiKCUUkoppSBvicoqICDl83vAy8aY\neGAB8KmjAlNKKaWUysv05OfTfF5mjDkK/AeIF5E1jgxOKaWUUne2vExPbgt8LyLXAURkJ7DTGHOX\nMaatiGx1dJBKKaWUujPl5dHPd0Bma6V4phxTSimllHIIR749uQyQmL9wlFJKKaX+lZu3J3+Z8lGA\n+caYtDN8nIDGwPcOjE0ppZRSd7jcjFE5l/LTABeAy2mOXQN2AnMdFJdSSimlVK7enjwMbG9PflNE\n9DGPUkoppQpUXsaozCTNGBVjTHVjzDPGmI43OEcppZRSKtfykqiEAo8BGGNKAbuAcUCoMeZpB8am\nlLpJ/P39efbZZ291GEoplUFeEhU/YFvK597ACaA61uQl0EFxKaUKmaSkpFsdglLqDpSXRMUV62Ba\nsL7n50sRScY6mLa6owJTSt0cw4YNIzw8nNmzZ2OxWHByciIkJASLxcKGDRto0aIFzs7O7NixA4DQ\n0FCaN2+Oi4sLtWvXJjg4mOTkZFt7586dY/jw4ZQvXx5PT0/at29PdHT0rfp6SqkiLi+JykGghzGm\nKtAJ2JhSXh4476jAlFI3x+zZs2nVqhUjRozg5MmT/PHHH1StWhWAiRMnMmPGDA4cOEDjxo3Ztm0b\nQ4YMISgoiJiYGD7++GNCQkJ49dVXbe317t2bv/76i7CwMCIjI/Hz86N9+/acPXv2Vn1FpVQRlpdE\nJRh4E0gAfhSRH1LKOwI/OSgupdRN4uHhQfHixXF1daVcuXKUL18eJycnAF5++WUCAgLw8fGhVKlS\nBAcHM3HiRAYNGkT16tUJCAggODiYjz76CIDt27ezZ88eli9fTrNmzahVqxYzZ87E09OTFStW3Mqv\nqZQqovLyUsIVxpjtgDcQlebQZqxvVi4wxhgv4H2gG5AMrATG5nSqtDHmI2Ak8IyIvFtggSp1GzDG\n0Lx5c7uyqKgovv/+e1555RVbWVJSEteuXePKlStER0dz4cIFSpe2f8vGlStXOHTo0E2JWyl1e8l1\nogIgIiewDqJNW7bLIRHd2GKgAhAAFAfmAx8Dg7I70RjTE7gX+K0A41OqSIgfG8+p5ads+5fvugzN\nMtYrWbKk3f7FixcJDg6mV69eGeqWKFGCixcvUqlSJcLDwxGxf9NGqVKlHBO8UuqOkqdE5VYwxtTH\nOiamuYj8lFI2BvjaGPO/lOQpq3MrA7NTzl93M+JVqrD6+9u/+e1d+3w9iSQSj2bfMenn50dsbCw1\na9bM8viJEydwcnKiWrVqDolXKXVnKzKJCtAK+Ds1SUmxCevic/diXd8lA2OMARYAM0XkgHVXqTtT\n8vVk4kfHW9/OlWa2cUVTkR3rd3Dk0BE8SnmQnJycoUcE4KWXXuKhhx6iatWq9O7dG4vFQlRUFPv2\n7ePll1+mffv2tGrVih49ejBjxgzq1q3Lb7/9xrp16+jVqxd+fn4378sqpW4LeRlMe6tUBE6lLRCR\nJOBMyrGsPA9cE5H3CzA2pYqExOhELh24ZJekADwqj8JlaNSoEeXLl+fYsWNkltR37NiRtWvX8s03\n39CyZUtatWrFrFmzqFGjhq3OunXraNu2LY8//jj16tVjwIABHDt2jAoVKhTwt1NK3Y5MZn813dQA\njHkdmHCDKgL4Ao8Aj4mIb7rzTwIvicjHmbTdHFgLNEt9NGSMOQK8c6PBtMYYPyCibdu2eHp62h3r\n378//fv3z9F3U6qwSb6ezJ7Ge7gUly5ZcQLXeq7cE30Pxkl7HZVSObNkyRKWLFliV3bu3Dm2bt0K\n1qEakfm9Rp4SFWPMYOApwAdoJSJHjTHPAEdEJNNHMDdoqwxQJptqh4HBWF+GaKtrjHECrgC9M7uu\nMWYs8BZp3k2EtdM7GTgmIpk+aE9NVCIiIrSrWt12/v72b6ICojKUN/m2CV7+XrcgIqXU7SQyMjJ1\nxqBDEpVcj1FJeZ9PMDALmIT1Fz/AWeAZshgrkhUR+Qv4KwfX/QEoZYxplmacSgBggB+zOG0B8E26\nso0p5Z/lJk6lbhdeD3hRObCy3ayf8o+W1yRFKVUo5WUw7RhghIisNsY8n6Z8D9aF4AqEiMQYY8KA\nuSnJUnHgPWBJ2hk/xpgYYIKIhIrI38DfadsxxvwDnBCR+IKKVanCrs7sOtSZXedWh6GUUtnKy2Ba\nHzJfgfYqUDKTckcaAMRgne2zFtgKPJmuTh3Ak6zd2kE5SimllMqxvPSoHAGaAkfTlXcGDuQ7ohsQ\nkbNks7ibiDhlczzzBSCUUkopVejkJVF5G/g/Y4wz1vEhLY0x/YGJwHBHBqeUUkqpO1te3vUzzxhz\nGXgFcMW6rP3vWN+5s9TB8SmllFLqDparRCVlldeqwEoR+dwY4wq4icipbE5VSimllMq13A6mNcBB\nrMkKInJJkxSllFJKFZRcJSoikgzEk/0CbUoppZRS+ZaX6cnPA28YYxo5OhillFJKqbTyMutnAdZB\ntFHGmGvA5bQHRaS0IwJTSimllMpLovKMw6NQSimllMpEXqYnhxREIEoppZRS6eWlR8UmZdG34mnL\nROR8viJSSimllEqR68G0xpiSxpj3jTGngESsL/1LuymllFJKOUReZv3MBB4Ansb6IsLhwBSsq9M+\n5rjQlFJKKXWny8ujn4eAx0RkizHmM2CbiBw0xhwFBgKfOzRCpZRSSt2x8tKjUho4nPL5fMo+wHag\nrSOCUkoppZSCvCUqhwGflM8xQN+Uzw8BZx0RlFJKKaUU5C1R+QxokvJ5OjDKGHMFeAd4w1GBKaWU\nUkrlZR2Vd9J83mSMqQ80Bw6KSLQjg1NKKaXUnS1f66gAiMhR4KgDYlFKKaWUspPrRMUY89KNjotI\ncN7DUUoppZT6V156VHqm2y+GdXDtdeAQoImKUkoppRwiL2NUmqUvM8Z4APOBVQ6ISSmllFIKyNus\nnwxS3u8zBXjZEe0ppZRSSoGDEpUUnimbUkoppZRD5GUwbWD6IsAbGAysd0RQSimllFKQt8G0Qen2\nk4E/gRDg9XxHpJRSSimVIi+DaX2yr6WUUkoplX+OHKOilFJKKeVQeRmjsgqQnNQVkV65jkgppZRS\nKuPJtbcAABPsSURBVEVeelTOAQFAizRlzYEHgPMpx1M3hzLGeBljPjfGnDPG/G2MmWeMKZmD83yN\nMaHGmLPGmIvGmB+NMVUcHZ9SSimlHCsvg2lPAsuBp0QkCcAY4wR8AJwXkeccGF96i4EKWBOl4lgX\nmfsYGJTVCcaYWsA2YC4wGbgANASuFGCc6v+3d+9RdpXlHce/DwmXkJBwE2hJBKIYaMCUGYSm1IoN\nkhZULosSJyA2iLcWm0a8gLXLCEUoRqaABS+xgGLGBRGEIAVMrKhBwMxwWUWyrIIIhYkLkAmGoiTz\n9I99kp7c58ycM2fPzPez1lkr5z3v3vuZnVmzf2e/795bkqQ66E9QORv4s/UhBSAz10XE5cC9QEOC\nSuUpzTOB1sx8sNL2YeA7EfHRzOzeyqL/DHwnMy+oanuiETVKkqT66s/Qz2jgkC20H9LP9fXVdOA3\n60NKxVKK+TJHb2mBiAjgROC/I+LOiFgVEfdFxEkNrFOSJNVJf86oXAt8tTKk8kCl7Wjg/MpnjbIf\n8OvqhsqZnBcqn23JPsA44BPAPwIfB/4KuDkijs3MHzawXkmSNED9CSofBbqB8yjuSAvwLPA54PO1\nriwiLqEIEluTwKG1rrdi/Rmeb2fmlZV/PxIRfwp8kGLuylbNmzePCRM2fipAW1sbbW1t/SxHkqTh\no6Ojg46Ojo3aenrqey1NZPbpSuMtL1w8NXn9Qwn7u469gL220+1xilv0L8jMDX0rk3hfAU7LzFu3\nsO4dgTXA/Mz8bFX7pcAxmfnmrdTUAnR2dnbS0tJS648kSdKI1dXVRWtrKxRzSrsGur7+3EdlDEXA\neTkzV0fEARFxNvDTzLy71vVl5vPA833Y7o+B3SPiiKp5KjMonjV0/1bW/WpE/ASYsslHbwCerLVW\nSZI0uPoz+fVW4CyAiNidYp7KecCtEfGhOta2kcxcCdwFfCUi3hQRxwBXAR3VV/xExMpNJst+DpgV\nEedExOsi4lzg7cC/NapWSZJUH/0JKi38/9yO0yjmqxxAEV42fbJyvc0GVlJc7XM78APgA5v0ORjY\nMLEkM79NMR/l48AjFJdXn5qZP25wrZIkaYD6M5l2V4qbpgEcD9ycmb0RcR9FYGmYzHyRbdzcrdJn\n1BbarqO4OZwkSRpC+nNG5efAyRExieIGbOvnpexDcQt9SZKkuuhPULkQWAD8Eri/agjleODBrS0k\nSZJUq5qHfjJzcUT8iOIeKg9XfbQMuKVehUmSJPVnjgqVq2y6N2l7YCvdJUmS+qWRz+aRJEkaEIOK\nJEkqLYOKJEkqrboGlcrt9SVJkuqiLkElInaOiPOAJ+qxPkmSJKghqFTCyCURsSIi7o2IkyvtcygC\nyj8A7Q2qU5IkjUC1XJ58IcVzdb4LHAPcFBHXAn8CfAS4KTPX1b9ESZI0UtUSVP4aOCszb4uIwyge\n8DcamJaZ2ZDqJEnSiFbLHJWJQCdAZv4X8Dug3ZAiSZIapZagMgr4fdX7tcBv61uOJEnS/6tl6CeA\n6yLid5X3uwBfjIg11Z0y89R6FSdJkka2WoLK9Zu8v6GehUiSJG2qz0ElM+c0shBJkqRNeQt9SZJU\nWgYVSZJUWgYVSZJUWgYVSZJUWgYVSZJUWgYVSZJUWgYVSZJUWgYVSZJUWgYVSZJUWgYVSZJUWkMq\nqETEHhHxjYjoiYjfRMTCiBi7nWXGRsQXIuKpiHg5Ih6NiA8MVs2SJKn/hlRQARYBhwIzgBOBPwe+\ntJ1l2oHjgdnAIZX3X4iItzewTkmSVAdDJqhExCHATOC9mbkiM+8FPgy8KyL228ai04HrM/OHmfmr\nzFwIPAwc1fiqJUnSQAyZoEIROH6TmQ9WtS0FEjh6G8vdC7wzIv4QICLeChwM3NWoQiVJUn2MbnYB\nNdgP+HV1Q2aui4gXKp9tzYeBLwNPR8RaYB3wvsxc3rBKJUlSXTT9jEpEXBIRvdt4rYuINwxgE39P\nccbl7UALcB5wdUT8RT3qlyRJjVOGMyoLgGu30+dxoBvYp7oxIkYBe1Y+20xE7AJcDJycmf9Raf6v\niDgC+CjwvW1tdN68eUyYMGGjtra2Ntra2rZTriRJw19HRwcdHR0btfX09NR1G5GZdV1ho1Qm0z4K\nHLl+nkpEHA/cAUzMzM3CSkTsBvQAf5mZd1e1fxE4MDP/civbagE6Ozs7aWlpqf8PI0nSMNXV1UVr\naytAa2Z2DXR9TR/66avMXEkxAfYrEfGmiDgGuAroqA4pEbEyIk6qLPMScA+wICLeEhEHRsTfAGcB\nNw/6DyFJkmpShqGfWswGvkBxtU8vsBiYu0mfg4Hq8ZpZwCXADRTDRE8CF2TmlxterSRJGpAhFVQy\n80XgzO30GbXJ+18D721kXZIkqTGGzNCPJEkaeQwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSp\ntAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwq\nkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSptAwqkiSp\ntAwqkiSptAwqkiSptAwqkiSptIZUUImIT0bE8ohYExEv1LDchRHxTES8HBHfjYjXN7JOSZJUH0Mq\nqAA7AjcC1/R1gYj4BHAu8H7gKGANcFdE7NSQCiVJUt2MbnYBtcjMzwBExHtqWGwucFFm3l5Z9ixg\nFXAyReiRJEklNdTOqNQkIg4C9gOWrW/LzNXA/cD0ZtUlSZL6ZlgHFYqQkhRnUKqtqnwmSZJKrOlB\nJSIuiYjebbzWRcQbml2nJEkafGWYo7IAuHY7fR7v57q7gQD2ZeOzKvsCD25v4Xnz5jFhwoSN2tra\n2mhra+tnOZIkDR8dHR10dHRs1NbT01PXbURm1nWFg6EymbY9M/fsQ99ngM9lZnvl/XiK0HJWZt60\nlWVagM7Ozk5aWlrqWLkkScNbV1cXra2tAK2Z2TXQ9TV96KcWETEpIqYBBwCjImJa5TW2qs/KiDip\narF/BT4VEe+IiMOBrwFPA7cOavGSJKlmZRj6qcWFwFlV79cntbcCP6j8+2Bgw3hNZl4WEbsCXwJ2\nB34I/FVm/r7x5UqSpIEYUkElM+cAc7bTZ9QW2uYD8xtTlSRJapQhNfQjSZJGFoOKJEkqLYOKJEkq\nLYOKJGlIOeigg7jyyiu32WfVqlW87W1vY9y4cey553bvZKESG1KTaSVJ6ov29nZWrVrFI488wvjx\n45tdjgbAoCJJGnS9vb1EBBHR52VeffVVdtxxxz71/cUvfkFrayuTJ0/ub4kqCYd+JEl9tnjxYt74\nxjey6667svfee3P88cdz2223MWbMGFavXr1R37lz53LccccBcN1117HHHnuwZMkSpk6dyi677MJT\nTz21zW3NmTOHU045hc9+9rPsv//+HHLIIRs+W716NbNnz2bcuHFMnDiRq6++esNnBx10EDfffDPX\nX389o0aN4uyzz67jHtBgM6hIkvqku7ub2bNnc84557By5UruueceTj31VI499lj22GMPvvWtb23o\n29vby4033siZZ54JQETw8ssvc9lll/HVr36VRx99lH322We721y2bBk/+9nPWLp0KbfffvuG9gUL\nFnDEEUfw0EMPcf755zN37lyWLVsGwIoVK5g5cyazZs2iu7ubK664os57QoPJoR9JUp88++yzrFu3\njlNOOYVJkyYBMHXqVABmzZrFokWLmDOnuCfn0qVL6enp4dRTT92w/Nq1a7nmmms47LDD+rzNcePG\nsXDhQkaP3vhwdcwxx/Cxj30MgHPPPZfly5fT3t7OjBkz2Guvvdh5550ZM2YMr3nNawb0M6v5PKMi\nSeqTadOmMWPGDA477DBOP/10Fi5cyIsvvgjAGWecwfe//326u7sBWLRoESeeeOJGE1l32mmnmkIK\nwOGHH75ZSAGYPn36Zu8fe+yxWn8kDQEGFUnSdvX2wgMP7MCdd97NnXfeydSpU7nqqquYMmUKTz75\nJEceeSSTJ0/mm9/8Jq+88gq33HLLhmGf9caMGVPzdseOHbv9ThrWDCqSpG165hmYMQOmT4fjjoMD\nD5zOpz/9aR588EF22mknbrnlFqA4q3LDDTewZMkSRo0axQknnNCwmu67777N3h966KEN256axzkq\nkqSt+s534N3vhpdeAniAe+5ZxpQpx3Pllfswdux9PPfccxsCwhlnnMH8+fO5+OKLOe200/p8KXF/\nLF++nAULFnDSSSdx9913s3jxYu64446GbU/NY1CRJG3Rc8/BO95R/DsTYDy9vT/gpZeuYM6c1Rx8\n8AFcfvnlzJw5E4DXve51HHXUUfzkJz9p6JU2EcF5553HihUrmD9/PhMmTKC9vX3DpdAaXiKL3z5V\niYgWoLOzs5OWlpZmlyNJTZEJr30tPP305p9NnAi/+hXUcL82jRBdXV20trYCtGZm10DX5xwVSdIW\nRcDpp8OmF92MHg2zZhlSNDgMKpKkrTrtNFi7duO2tWuL9oHabbfdGD9+PLvttttGr/Hjx7N8+fKB\nb0DDgnNUJElbdfTR8P73Q/Xd7idNgqOOGvi6H3744a1+tv/++w98AxoWDCqSpK3aYQf40pcas24f\nGKi+cOhHkiSVlkFFkiSVlkFFkiSVlkFFkiSVlkFFkiSVlkFFkiSVlkFFkiSVlkFFkiSVlkFFpdHR\n0dHsEkYc9/ngc58PPvf50DakgkpEfDIilkfEmoh4oQ/9R0fEv0TEIxHx24j4n4i4PiL+YDDqVW38\nYzL43OeDz30++NznQ9uQCirAjsCNwDV97L8r8MfAZ4AjgFOAKcCtDalOkiTV1ZB61k9mfgYgIt7T\nx/6rgZnVbRFxLnB/REzMzKfrX6UkSaqXoXZGpR52BxJ4sdmFSJKkbRtSZ1QGKiJ2Bi4FFmXmb7fR\ndReAxx57bFDqUqGnp4eurq5mlzGiuM8Hn/t88LnPB1fVsXOXeqwvMrMe6+l/ARGXAJ/YRpcEDs3M\nn1Ut8x6gPTP3rGE7o4GbgT8A3rqtoBIRs4Fv9HXdkiRpM2dk5qKBrqQMZ1QWANdup8/jA9lAJaTc\nBEwC/mI7Z1MA7gLOAH4JvDKQbUuSNMLsAhxIcSwdsKYHlcx8Hni+UeuvCimTKc6k/KaPNQ04BUqS\nNELdW68VDanJtBExKSKmAQcAoyJiWuU1tqrPyog4qfLv0cC3gBbgTGDHiNi38tqxGT+DJEnqu6bP\nUalFRFwLnLWFj96amT+o9FkHzMnMr0XEAWw+bBQU8142LCNJksppSAUVSZI0sgypoR9JkjSyGFQk\nSVJpGVS2ICL+LiKeiIj/jYj7IuJNza5puIqICyLigYhYHRGrIuKWiHhDs+saKSLi/IjojYjLm13L\ncBcRfxgRX4+I5yLi5Yh4OCJaml3XcBURO0TERRHxeGV//zwiPtXsuoaTiHhzRNxWeeBvb0S8cwt9\nLoyIZyr/B9+NiNfXuh2DyiYiYhbweeDTFA8yfBi4KyL2bmphw9ebgauAo4HjKB48eXdEjGlqVSNA\nJYC/n+J3XA0UEbsDy4HfUTx/7FDgPGC7t0tQv50PfAD4W+AQ4OPAxyvPe1N9jAUeotjHm014jYhP\nAOdS/J05ClhDcTzdqZaNOJl2ExFxH3B/Zs6tvA/gKeDKzLysqcWNAJVA+GvgzzPzR82uZ7iKiHFA\nJ/Ah4J+ABzPzI82taviKiEuB6Zn5lmbXMlJExBKgOzPfV9W2GHg5M7d09agGICJ6gZMz87aqtmeA\nz2Vme+X9eGAV8J7MvLGv6/aMSpXKvVVagWXr27JIckuB6c2qa4RZ/9DIF5pdyDD3b8CSzPxeswsZ\nId4BrIiIGytDnF0RcU6zixrm7gVmRMTBAJV7cB0D3NHUqkaIiDgI2I+Nj6ergfup8Xja9DvTlsze\nwCiKxFdtFTBl8MsZWSpnr/4V+FFm/rTZ9QxXEfEu4I+BI5tdywgymeLs1eeBiylOg18ZEb/LzK83\ntbLh61JgPLCycn+tHYB/zMxvNresEWM/ii+dWzqe7lfLigwqKpOrgT+i+NajBoiIiRRh8LjMfLXZ\n9YwgOwAPZOY/Vd4/HBGHAR8EDCqNMQuYDbwL+ClFOL8iIp4xHA4tDv1s7DlgHbDvJu37At2DX87I\nERFfAE4Ajs3MZ5tdzzDWCrwG6IqIVyPiVeAtwNyI+H3lrJbq71ngsU3aHgNe24RaRorLgEsz86bM\nfDQzvwG0Axc0ua6RopviTvADPp4aVKpUvmF2AjPWt1X+cM+gjg9Y0sYqIeUkisca/KrZ9QxzS4HD\nKb5dTqu8VgA3ANPS2fWNspzNh4+nAE82oZaRYleKL57VevG4Nygy8wmKQFJ9PB1PcYVnTcdTh342\ndzlwXUR0Ag8A8yh+4a9rZlHDVURcDbQB7wTWRMT69N2Tma80r7LhKTPXUJwG3yAi1gDPZ+am3/hV\nP+3A8oi4ALiR4o/1OcD7trmUBmIJ8KmIeBp4lOLhtPOAhU2tahipPBD49RRnTgAmVyYtv5CZT1EM\nM38qIn4O/BK4CHgauLWm7fgFanMR8bcU19zvS3GN+Iczc0VzqxqeKpe0bemXcE5mfm2w6xmJIuJ7\nwENentxYEXECxQTP1wNPAJ/PzH9vblXDV+UgehFwCrAP8AywCLgoM9c2s7bhIiLeAvwnm/8Nvz4z\nz670mU9xH5XdgR8Cf5eZP69pOwYVSZJUVo7VSZKk0jKoSJKk0jKoSJKk0jKoSJKk0jKoSJKk0jKo\nSJKk0jKoSJKk0jKoSJKk0jKoSJKk0jKoSJKk0jKoSJKk0vo/Dt2cETcCcfcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x228938c8c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(D[0],C[0], color='r',marker='^', alpha=1)\n",
    "plt.scatter(D[1],C[1], color='b',marker='p', alpha=1)\n",
    "plt.scatter(D[2],C[2], color='g',marker='o', alpha=1)\n",
    "plt.scatter(D[3],C[3], color='m',marker='H', alpha=1)\n",
    "plt.scatter(D[4],C[4], color='c',marker='s', alpha=1)\n",
    "plt.scatter(D[5],C[5], color='r',marker='^', alpha=1)\n",
    "plt.scatter(D[6],C[6], color='b',marker='p', alpha=1)\n",
    "plt.scatter(D[7],C[7], color='g',marker='o', alpha=1)\n",
    "plt.scatter(D[8],C[8], color='m',marker='H', alpha=1)\n",
    "#plt.scatter(C[4],D[4], color='c',marker='s', alpha=1)\n",
    "#plt.show()\n",
    "n=['SGD_squaredloss','SGD_huber','SGD_epsilon_insensitive','tree','Ridge','Kernel_Ridge','svr_rbf','svr_poly','svr_sig']\n",
    "#l=['-0.00589536','-5.76074315','0.09918653','-0.01150757','-0.01150757']\n",
    "for i, txt in enumerate(n):\n",
    "    plt.annotate(txt, (D[i],C[i]))\n",
    "plt.ylabel('R square statistic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
